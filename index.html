<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="./img/logos/terminal.png" />

    <title>Gokul Adethya: Personal Website</title>


    <!-- Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/timeline.css" rel="stylesheet">
    <link href="css/freelancer.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet"
        type="text/css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&display=swap" rel="stylesheet">
    <script
    src="https://kit.fontawesome.com/d06c559023.js"
    crossorigin="anonymous"
  ></script>
  <link
    href="https://fonts.googleapis.com/css2?family=Andika&family=Montserrat:wght@300;400;500;600&family=Roboto:wght@300&display=swap"
    rel="stylesheet"
  />
  <link rel="stylesheet" href="style.css" />
    <script src="js/modernizr.js"></script>
    <meta name="google-site-verification" content="of5d9ckLytWKCHg9c9NZ7oTGOIP9pyZIvhsY-E1l-kY" />
    <style>
       
        .button {
            display: inline-flex;
            align-items: center;
            padding: 10px 15px;
            border-radius: 5px;
            text-decoration: none;
            color: white;
            font-weight: bold;
            transition: background-color 0.3s;
            border: none;
            cursor: pointer;
        }
        .google-slides {
            background-color: #5796fb; /* Google Slides blue */
        }
        .notion {
            background-color: #fd6969; /* Notion red */
        }
        .button:hover {
            opacity: 0.9;
        }
        .icon {
            margin-right: 8px; /* Space between icon and text */
        }
    </style>
    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse"
                    data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-about-me-hidden navbar-brand" href="#page-top">GOKUL ADETHYA</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li class="page-scroll">
                            <a href="#timeline">Experience</a>
                        </li>
                    <li class="page-scroll">
                        <a href="#about">Publications</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#projects">Projects</a>
                    </li>

                    <li class="page-scroll">
                        <a href="#presentation">Presentation & Surveys</a>
                    </li>
                    

                    <li>
                        <a href="https://drive.google.com/file/d/1xgukjX0Roh4qaay9HiSJre7WK5HIBV05/view?usp=sharing" target="_blank">CV</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->

        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
        <div class="container">
            <div class="row">

                <div class="col-xs-4">
                    <img class="img-responsive img-circle" src="img/4.jpeg" alt="">
                </div>
                <div class="col-xs-8">
                    <div class="intro-text">
                        <span class="name">Gokul Adethya</span>
                        <br>

                        <span class="about_me"> 
                            <p style="text-align: justify">
                                I will be joining the <b>University of California, San Diego</b> as a Fall 2025 <b>Masters student at the Halıcıoğlu Data Science Institute</b>. I recently completed my <b>B.Tech in Computer Science from NIT Trichy (NITT)</b>.
                                
                                I’ve interned across multiple research labs and institutions. Most recently, I was a Research Intern at <a href="https://cistup.iisc.ac.in/" target="_blank">CiSTUP</a>, <a href="https://iisc.ac.in/" target="_blank">IISc</a>, where I worked on <b>multi-modal fusion</b>, <b>cross-modal alignment in medical datasets (MIMIC)</b>, and test-time adaptation of optical flow models.
                            
                                I previously interned at <a href="https://xulabs.github.io/" target="_blank">Xu Labs</a>, <a href="https://www.cmu.edu/" target="_blank">Carnegie Mellon University</a>, focusing on <b>self-supervised learning for Cryo-ET segmentation</b>. I also worked on <b>surgical task and motion planning</b> during an internship at the <a href="http://www.labren.org/mm/principal-investigator/" target="_blank">National University of Singapore</a>. Additional research internships include empathetic response generation and <b>LLM distillation</b> at <a href="https://www.samsungprism.com/" target="_blank">Samsung PRISM</a>, and <b>legal NLP</b> at NIT Trichy.
                            
                                I led the Machine Learning division at <a href="https://spider.nitt.edu/" target="_blank">Spider R&D</a>, NITT’s student-run research group.
                            </h5>
                            
                            <p style="text-align: justify">
                                <strong>Research Interests:</strong> My research interests are focused on two dimensions: 
                                (1) advancing core AI fields such as <b>Natural Language Processing (NLP)</b>, 
                                <b>Computer Vision (CV)</b>, <b>Multimodal Learning</b>, and complementary topics aimed at addressing real-world settings like 
                                <b>Continual Learning (CL)</b>, <b>Test-time Adaptation (TTA)</b>, <b>Self-supervised Learning</b> (for unlabeled/weakly labelled data), 
                                <b>Robustness</b>, <b>Efficiency</b>, and <b>Explainable AI (XAI)</b>; and 
                                (2) applying these advancements in <b>healthcare</b> and <b>biology</b>.
                                   </h5>
                            
                        
                            <h5>
                                For collaborations, reach out at <b><a href="mailto:gokul3112003.com@gmail.com">gokul3112003.com@gmail.com</a></b>, <b><a href="mailto:gthirumurugan@ucsd.edu">gthirumurugan@ucsd.edu</a></b>
                            </h5>
                        </span>
                        
                        <hr>
                        <span class="skills">
                            <ul class="list-inline">
                                <li>
                                    <a href="https://scholar.google.com/citations?hl=en&user=m8n5yo8AAAAJ"
                                        class="btn-social btn-outline" target="_blank"><i class="ai ai-google-scholar" ></i></a>
                                </li>
                                <li>
                                    <a href="https://www.linkedin.com/in/gokul-adethya/" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-linkedin"></i></a>
                                </li>
                                <li>
                                    <a href="https://github.com/FrozenWolf-Cyber" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-github"></i></a>
                                </li>
                                <li>
                                    <a href="mailto:gokul3112003.com@gmail.comr" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-envelope"></i></a>
                                </li>
                            </ul>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <section id="affilition">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Education</h2>
                    <hr class="light">
                </div>
            </div>
            <ul class="affiliation-list row">
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/ucsd.png" style="width: 196px; height: 110px;"/>
                    </a><br>
                    <b>Master of Science in Data Science<br>University of California San Diego<br><br>
                    (Sep 2025 - Present)</b>
                </li>
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/nitt.png" style="width: 115px; height: 110px;"/>
                    </a><br>
                <b>Bachelor of Technology in Computer Science and Engineering<br>NIT-Trichy<br>
                    (July 2021 - May 2025)</b>
                </li>
            </ul>
        </div>
    </section>

<section class="timeline-class" id="timeline">
    <!-- <header class="main-header"> -->
      <!-- <div class="down-arrow"></div> -->
      
    <div class="container">
      <div class="row">
        <div class="col-lg-12 text-center">
            <br>
            <h2>Experience</h2>
            <hr class="light-white">
        </div>
       </div>

    </header>
    <section id="timeline">
        <ul>

            <h3><b class="larger-bold-text">VISTA @ Indian Institute of Science</b><br><br>Research Intern | Jun 2024 – Jan 2025</h3>
            <li id="iisc">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Multi-modality in Healthcare & Test-time Adaptation in Optical Flow</h2>
                </div>
                <h5>
                  Working on cross-modal alignment and <strong class="green-text">self-supervised learning</strong> for medical tasks (<strong class="green-text">MIMIC</strong>), including mortality prediction. Exploring <strong class="green-text">optical flow estimation</strong> and test-time adaptation using <strong class="green-text">Torchscale</strong> and <strong class="green-text">Fabric</strong> for multi-node training and performance optimization.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">Xu Labs @ Carnegie Mellon University</b><br><br>Research Intern | Dec 2023 – May 2024</h3>
            <li id="cmu">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Few-shot Weak Label Cryo-ET Segmentation</h2>
                </div>
                <h5>
                  First-author (submitted to <strong class="red-text">PLOS Computational Biology</strong>) on SaSi, a <strong class="green-text">self-augmented few-shot learning</strong> method for weakly supervised Cryo-ET segmentation. Worked on consistency loss, <strong class="green-text">SimCLR</strong>, AugMix, and pretraining <strong class="green-text">MAE</strong> for denoising/reconstruction. Adapted the <strong class="green-text">Segment Anything Model</strong>.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">Laboratory of Medical Mechatronics @ National University of Singapore</b><br><br>Research Intern | Feb 2023 – Feb 2024</h3>
            <li id="nus">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Surgical Task & Motion Planning </h2>
                </div>
                <h5>
                  Developed <span class="green-text">MASS</span>, an <span class="green-text">LLM</span> + <span class="green-text">RL</span> framework for interpretable <span class="green-text">robotic motion planning</span> in surgical scenarios using PyBullet and LapGym-SOFA (submitted to <strong class="red-text">RA-L</strong>). Trained <span class="green-text">RL</span> policies like HER with <span class="green-text">imitation learning</span> and integrated <strong class="green-text">Grounding DINO</strong> for enhanced planning.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">Spider R&D Club</b><br><br>Head of ML Research | July 2022 – May 2025</h3>
            <li id="spider">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Camouflage Video Segmentation</h2>
                </div>
                <h5>
                  Proposed SAM-PM for video camouflage object detection, improving SAM with minimal parameters (<strong class="red-text">CVPR 2024</strong>). Leading research in <span class="green-text">LLM-driven Task & Motion Planning</span>, <span class="green-text">Reinforcement Learning</span> with custom robotic arms, and <span class="green-text">Continual Learning</span>.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">Samsung PRISM</b><br><br>ML Research Intern | Aug 2022 – Mar 2023</h3>
            <li id="samsung">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Empathetic Response Generation</h2>
                </div>
                <h5>
                  Worked on <strong class="green-text">Empathetic Response Generation</strong> and emotion/intent classification using Flan-T5, BART, and RoBERTa with <strong class="green-text">Hugging Face</strong> for edge devices. Performed <strong class="green-text">Knowledge Distillation</strong> on T5 achieving a 77% size reduction while maintaining BLEU score.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">NIT Trichy</b><br><br>ML Research Intern | Apr 2022 – Oct 2022</h3>
            <li id="nitt">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Natural Legal Language Processing</h2>
                </div>
                <h5>
                  Worked on <strong class="green-text">Natural Legal Language Processing</strong> using BERT, XLNET, and Hierarchical Transformers on judicial data. Published at <strong class="red-text">EMNLP NLLP 2022</strong> workshop with an 80x speedup for sentence boundary detection using a lightweight CNN model.
                </h5>
              </div>
            </li>
            
        </ul>
    </div>
      </section>
      
      
    <script src="main.js"></script>
</section>



    <section id="news-list" >
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>News</h2>
                    <hr class="light">
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Oct 2024</div>
                    <div class="col-xs-8"><b>Awarded the <span style="color: #E73916">IndiaAI Fellowship</span> and grant, given to only 80 across India, to research <span style="color: #e77b16">Speech & NLP multi-modal with Continual Learning !</b></div>
                </div>

                <div class="row">
                    <div class="col-xs-4 news-date">Sep 2024</div>
                    <div class="col-xs-8"><b>Achieved 12th rank out of 74,000 participants nationwide in the <span style="color: #E73916"> Amazon ML Challenge </span> !</b></div>
                </div>
                <!-- <div class="row">
                    <div class="col-xs-4 news-date">Sep 2024</div>
                    <div class="col-xs-8"><b>One first author paper submitted at <span style="color: #E73916">WACV 2025 Conference</span> (affliated with CMU) !</b></div>
                </div> -->
                <div class="row">
                    <div class="col-xs-4 news-date">Jun 2024</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Indian Institute of Science</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">April 2024</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">CVPR 2024 Workshop </span> on Pixel-level Video Understanding in the Wild Challenge !</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Dec 2023</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Carnegie Mellon University</span> as research intern!</b></div>
                </div>
                <!-- <div class="row">
                    <div class="col-xs-4 news-date">Sep 2023</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">International Journal of Biomedical Engineering and Technology </span> !</b></div>
                </div> -->
                <div class="row">
                    <div class="col-xs-4 news-date">July 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">National University of Singapore</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Feb 2023</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">EMNLP 2022 Workshop </span> on Natural Legal Language Processing !</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Nov 2022</div>
                    <div class="col-xs-8"><b>Top 30 teams in <span style="color: #E73916">NeurIPS 2022 CityLearn Challenge</span> in Multi-agent RL!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Aug 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Samsung PRISM</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">July 2022</div>
                    <div class="col-xs-8"><b>Winner of <span style="color: #E73916">Smart India Hackathon</span> among 160k+ students!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">April 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">NIT-Trichy</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Jan 2022</div>
                    <div class="col-xs-8"><b>Runner up of <span style="color: #E73916">MTX - HackOlympics 2.0 Shaastra</span> hackathon in my first year as a solo participant!</b></div>
                </div>
            </div>
        </div>
    </section>

    <!-- About Section -->
    <section class="success grid-band2" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Publications</h2>
                    <hr class="light">
                </div>
            </div>
            <!-- <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/cvpr.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha, Xingjian Li</div>
                    <div class="pubv"><b>WACV 2025</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="#cmu" target="_blank" style="color:red;" data-toggle="modal"><b>UNDER REVIEW - WanDB</b></a></li>
                        </ul>
                    </div>
                </div>
            </div> -->
            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cl-sam/4.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Continual Learning for Robust Video Segmentation of Robot-Assisted Surgical Tool</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</div>
                    <div class="pubv"><b>B.Tech Thesis</b> </div>
                    <div class="publ">
                        <ul>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li> -->
                            <li><a href="https://frozenwolf-cyber.github.io/cl-sam.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/indiccl/1.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>A Study on Regularization-Based Continual Learning Methods for Indic ASR</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, S Nirmala</div>
                    <div class="pubv"><b>Submitted to ACL Rolling reviews - Funded by IndiaAI fellowship</b> </div>
                    <div class="publ">
                        <ul>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li> -->
                            <li><a href="https://frozenwolf-cyber.github.io/indic-cl.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/cvpr.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SAM-PM: Enhancing Video Camouflaged Object Detection using Spatio-Temporal Attention</strong></div>
                    <div class="puba"> Muhammad Nawfal Meeran, <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha</div>
                    <div class="pubv"><b>CVPR 2024 Workshop</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://github.com/SpiderNitt/SAM-PM" target="_blank" style="color:red;"><b>Code</b></a></li>
                            <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/internal.jpg" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Be My ASSistant: Exploring LLM Empowered Interactive Surgical Assistant for Surgical Sub-Task Automation</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Lalithkumar Seenivasan, Ashwin Krishna Kumar, Mobarakol Islam, Hongliang Ren </div>

                    <div class="pubv"><b> (In progress) RA-L</b> </div>
                    <div class="publ">
                        <ul>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li> -->
                            <li><a href="https://frozenwolf-cyber.github.io/mass.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li> -->
                        </ul>
                    </div>
                </div>
            </div>


            


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cmu/comb.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-Interpreted Deep-Learning Approach for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha, Tianyang Wang, Xingjian Li, Min Xu </div>

                    <div class="pubv"><b>(In progress) PLOS Computational Biology</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://arxiv.org/abs/2505.19948" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://frozenwolf-cyber.github.io/sasi.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                            <li><a href="https://docs.google.com/presentation/d/1-fbPDVgn0zuPuR4-GX3mox6vF11oglS6xcqOVqwfZtw/edit?usp=sharing" target="_blank" style="color:blue;"><i class="fa fa-google icon"></i><b>Slides</b></a></li>
                       
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li> -->
                        </ul>
                    </div>

                 

                

                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/emnlp.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Efficient deep learning-based sentence boundary detection in legal text</strong></div>
                    <div class="puba"> Reshma Sheik, <b>T Gokul</b>, S Nirmala</div>
                    <div class="pubv"><b>EMNLP NLLP 2022 Workshop</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://aclanthology.org/2022.nllp-1.18.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://github.com/NLLP-ML/SBD" target="_blank" style="color:red;"><b>Code</b></a></li>
                            <li><a href="img/assests/emnlp.txt" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Talks</h2>
                    <hr class="light">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 text-center">
                <div class="pubt">Known unknowns: Learning novel concepts using reasoning-by-elimination (UAI Oral Talk)</div>
                <br>
                <iframe src="https://drive.google.com/file/d/1aqM2XWzicxCGEe6BbVHKozkmdvvUd2kp/preview"
                    width="480" height="320" allowfullscreen></iframe>
                    
                </div>

            </div>

        </div>
    </section> -->




    <!-- Portfolio Grid Section -->
    <section id="projects" class="" >
        <div class="">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Projects</h2>

            <h5><strong class="click_custom">CLICK</strong> on the project to know more about it !!!</h5>
                    <hr class="light">
                </div>
            </div>
            <div class="band">
                <div class="item-1" style="background-color: #e8eef7;">

                    <a id="onclick-indic" href="https://frozenwolf-cyber.github.io/indic-cl.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="width: 35vw; height: 20vw; background-image: url(img/projects/indiccl/indic.gif);"></div>
                        <article class="article">
                            <h1>A Study on Regularization-Based Continual Learning Methods for Indic ASR</h1>
                            <h5>
                                Sequentially adapting a Conformer-based ASR model to nine Indian languages using regularization based continual learning strategies, improving multilingual performance without forgetting.    </h5>
                            <span> (Submitted) ACL Rolling reviews - Funded by IndiaAI fellowship</span>
                        </article>
                    </a>

                </div>

                <div class="item-2" style="background-color: #e8eef7;">


                    <a href="https://frozenwolf-cyber.github.io/cl-sam.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="width: 23vw; height: 20vw; background-image: url(img/projects/cl-sam/demo.gif);"></div>
                        <article class="article">
                            <h1>Continual Learning for Robust Video Segmentation of Robot-Assisted Surgical Tool </h1>
                            <h5>
                                A continual learning framework using SAM2 and LoRA for robust, privacy-preserving surgical tool segmentation across evolving, distorted endoscopic video domains.   </h5>
                            <span>Thesis@NITT</span>
                        </article>
                    </a>

                </div>



                <div class="item-3" style="background-color: #e8eef7;">


                    <a id="onclick-mimic" href="#mimic" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="width: 23vw; height: 20vw; background-image: url(img/projects/iisc_mimic.gif);"></div>
                        <article class="article">
                            <h1>Medical Multi-Modal Fusion & Cross Modal Alignment </h1>
                            <h5>
                                This project focuses on multi-modal fusion and cross-modal representation alignment with self0supervised learning by integrating EHR, CXR, Notes, and ECG data.
                            </h5>
                            <span>VISTA@Indian Institute of Science</span>
                        </article>
                    </a>

                </div>



                <div class="item-4">


                    <a href="https://frozenwolf-cyber.github.io/mass.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb"
                            style="width: 25vw; background-image: url(img/projects/nus/15-ezgif.com-video-to-gif-converter.gif);">
                        </div>
                        <article class="article">
                            <h1>MASS (My ASSistant)</h1>
                            <h5>
MASS uses an LLM to assist surgeons in automating surgical robots by sequencing actions from simple instructions. The project develops robotic motion planning with LLMs and reinforcement/imitation learning for surgery.
                            </h5>
                            <span> (Submitted) IEEE Robotics and Automation Letters - Laboratory of Medical Mechatronics@National University of Singapore</span>
                        </article>
                    </a>

                </div>

                <div class="item-5">

                    <a href="https://frozenwolf-cyber.github.io/sasi.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/cmu.gif);"></div>
                        <article class="article">
                            <h1>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</h1>
                            <h5>This project introduces a SaSi deep learning approach for weak label cryo-ET segmentation in few-shot learning, using Consistency Loss, SimCLR, and AugMix to improve data efficiency and outperform existing methods. </h5>
                            </h5>
                            <span>(Submitted) PLOS Computational Biology  Xu Labs@Carnegie Mellon University</span>
                        </article>
                    </a>

                </div>

                <div class="item-6">
        
                    <a id="onclick-optical" href="#optical" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/opticalflow.png);"></div>
                        <article class="article">
                            <h1>Test-time adaptation for Optical Flow</h1>
                            <h5>Test-time adaptation for optical flow method by using augmentation and pesudo predictions.</h5>
                            </h5>
                            <span>VISTA@Indian Institute of Science</span>
                        </article>
                    </a>


                </div>

                <div class="item-7">


                    <a id="onclick-SAM-PM" href="#SAM-PM" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/assests/cvpr.png);"></div>
                        <article class="article">
                            <h1>SAM-PM: Enhancing Video Camouflaged Object Detection</h1>
                            <h5>SAM-PM is a propagation Module to enhance the SAM’s performance in video camouflage object detec-
tion, achieving substantial improvements with minimal parameter addition (&lt 1M) while performing better than
previous SOTA by 37.9% on mIoU, which got accepted for CVPR 2024 workshops</h5>
                            </h5>
                            <span>CVPR PVUW Workshop</span>
                        </article>
                    </a>



                </div>

                <div class="item-8">


                    <a id="onclick-nllp" href="#nllp" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/assests/emnlp.png);"></div>
                        <article class="article">
                            <h1>Natural Legal Language Processing</h1>
                            <h5>
                                Worked on Natural Legal Language Processing by using LLMs like BERT, XLNET, and Hierarchical Transformers on judiciary datasets.
                                Developed a lightweight CNN model for sentence boundary detection that is 80x faster than traditional statistical models.
                            </h5>
                            <span>EMNLP NLLP Workshop</span>
                        </article>
                    </a>
       
                </div>




                <div class="item-9">
        
                    <a id="onclick-OCR" href="#OCR" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/mtx.jpeg);"></div>
                        <article class="article">
                            <h1>Intelligent OCR</h1>
                            <h5>
                                My OCR integrates CRAFT, Faster R-CNN, Tesseract, and a Siamese network to perform sentence classification and key-value pair detection, including bounding boxes and linked information using PyTorch which is hosted in Azure Cloud. 
                            </h5>
                            <span>NITT</span>
                        </article>
                    </a>
                  


                </div>

                <div class="item-10">
        
                    <a id="onclick-i-Pravesh" href="#i-Pravesh" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb"
                            style=" background-image: url(img/projects/sih.png);">
                        </div>
                        <article class="article">
                            <h1>i-Pravesh</h1>
                            <h5>I-Pravesh is a Smart Attendance Android App which uses a combination of edge face detection and recognition (MobileFaceNet + TensorFlow Lite) as the authentication biometric for recording attendance.</h5>
                            <span>NITT</span>
                        </article>
                    </a>

                  


                </div>

                <div class="item-11">
                 
                        
                    <a id="onclick-summarise" href="#summarise" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/summarise.png);"></div>
                        <article class="article">
                            <h1>SummarizeIQ: An Integrated Summarization and Content Analysis Engine</h1>
                            <h5>This project integrates T5 like LLM summarization models with Torch Serve, enhanced by extractive methods like LexRank and TextRank. It also includes keyword generation and implements query-based content filtering using cosine similarity, offering a comprehensive solution for article summarization and analysis. 
                            </h5>
                            <span>NITT</span>
                        </article>
                    </a>

                </div>


                <div class="item-12">
                 
                        
                    <a id="onclick-realestatevr" href="#realestatevr" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/vr.gif);"></div>
                        <article class="article">
                            <h1>Real Estate VR</h1>
                            <h5>This VR platform leverages Unity’s advanced capabilities to create an immersive and dynamic environment.
                                 The application features precise asset coordination and spatial intelligence for real-time building placement through an intuitive XR Device Simulator, enabling seamless navigation and interaction.
                                   
                                 
                            </h5>
                            <span>NITT</span>
                        </article>
                    </a>

                </div>


                <div class="item-13">
                 
                        
                    <a id="onclick-compiler" href="#compiler" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/compiler.gif);"></div>
                        <article class="article">
                            <h1>Yet Another Python Compiler</h1>
                            <h5>
                                
                                A Python Compiler, built with Bison and Flex, processes Python code by tokenizing it, parsing it into a syntax tree, and performing semantic checks. 
                                It generates intermediate code and applies optimizations like constant folding and dead code elimination. 
                                Key features include lexical analysis, syntax and semantic error detection, and code optimization.

                                 
                            </h5>
                            <span>Compiler Design - CSPC62 @ NITT</span>
                        </article>
                    </a>

                </div>


                <div id="onclick-octtree" class="item-14">
                 
                        
                    <a href="#octtree" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/octtree.gif);"></div>
                        <article class="article">
                            <h1>Oct tree based 3D OpenGL Renderer</h1>
                            <h5>
                                
The Octree-Based 3D OpenGL Renderer, created with PyOpenGL, efficiently renders 3D scenes by using an octree data structure to manage and optimize rendering performance.
                                 
                            </h5>
                            <span>Advanced Data Structures and Algorithms - CSPE43 @ NITT</span>
                        </article>
                    </a>

                </div>
                <div class="item-15">
                 
                        
                    <a id="onclick-sart" href="#sart" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/sart.gif);"></div>
                        <article class="article">
                            <h1>Sartorius Cell Instance Segmentation</h1>
                            <h5>Trained R-CNN, U-Net and Detectron2 using Pytorch to detect and delineate
                                distinct objects of interest in biological images depicting neuronal cell types commonly used in studying neurological disorders.
                            </h5>
                                <span>NITT</span>
                        </article>
                    </a>

                </div>

                <div class="item-16">
                 
                        
                    <a id="onclick-others" href="#others" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/others.gif);"></div>
                        <article class="article">
                            <h1>Other Projects</h1>
                            <h5>
                                Projects include: <br>

                                Brain tumor classification <br>
                                Pneumonia (COVID-19) chest X-ray classification <br>
                                Neural style transfer <br>
                                Crack detection with TF Lite <br>
                                Face detection using a Siamese Neural Network model to develop a one-shot neural network with FaceNet and MTCNN as the backbone
                                Unity based C# game called "The Keres" 
                            </h5>
                                <span>NITT</span>
                        </article>
                    </a>

                </div>


          

            </div>
        </div>
    </section>



    <div class="portfolio-modal modal fade" id="cl-sam" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Continual Learning for Robust Video Segmentation of Robot-Assisted Surgical Tool</h2>
                            <br>
                            <div class="row">
                                <div class="col-md-8 col-md-offset-2">
                              
                                  <!-- Optional video block -->
                                  <!-- 
                                  <p style="text-align:center;">
                                    <video id="v0" width="100%" playsinline autoplay muted loop controls>
                                      <source src="img/demo_sequence_compressed.mp4" type="video/mp4">
                                    </video>
                                  </h5>
                                  -->
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/2.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h3>Abstract</h3>
                              
                                  <p class="text-justify">
                                    Robust identification and segmentation of surgical tools in robot-assisted procedures are critical for safeguarding patient safety and driving progress toward fully automated surgeries. However, deep learning models often fail to handle challenging visual conditions of real-world surgical scenes, including distortions like over-bleeding, smoke, and low illumination. Conventional training on limited, high-quality data makes adaptation to diverse surgical conditions difficult, with standard approaches often causing catastrophic forgetting and violating privacy due to the need for sensitive historical data.
                                  </h5>
                                  <p class="text-justify">
                                    We propose a new framework that unifies Domain-Incremental Continual Learning (CL) to achieve robust surgical tool segmentation while preserving data privacy across evolving domains. Our approach utilizes the Segment Anything Model 2 (SAM2) as a baseline, employing parameter-efficient Low-Rank Adaptation (LoRA) to facilitate adaptation within the CL framework. This enables sequential, privacy-preserving learning across domains and mitigates forgetting.
                                  </h5>
                                  <p class="text-justify">
                                    We evaluate our methodology on challenging endoscopic video datasets containing various distortions, measuring performance using standard segmentation metrics (mIoU, DSC) and assessing knowledge retention via forgetting analysis. Our results demonstrate that the K-Means based CL method achieves high performance across several learned domains, presenting a significant advancement towards reliable, adaptable, and privacy-conscious computer vision systems for real-world surgical applications.
                                  </h5>
                              
                                </div>
                              </div>
                              
                              <p style="text-align:center;">
                                <img src="img/projects/cl-sam/7.png" class="img-responsive" style="max-width: 50%; transform: scale(1); transform-origin: center;">
                              </h5>
                              
                              <p style="text-align:center;">
                                <img src="img/projects/cl-sam/3.png" class="img-responsive" style="max-width: 50%; transform: scale(1); transform-origin: center;">
                              </h5>
                              
                              <div class="row">
                                <div class="col-md-8 col-md-offset-2">
                                  <br>
                                  <h3>Methodology</h3>
                              
                                  <h4>Domain Identification and Adaptive LoRA Loading (K-Means + CLIP)</h4>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/4.png" class="img-responsive" style="max-width: 70%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h5>
                                    Instead of adapting a single set of parameters over time, this method trains <strong>individual LoRA adapters</strong> for each domain encountered during the learning phase. During inference, the <strong>domain of the input video</strong> is first identified using <strong>CLIP embeddings and K-Means clustering</strong>, and the corresponding adapter is loaded into the base segmentation model for inference.
                                  </h5>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/8.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h5>Training Phase:</h5>
                                  <ul>
                                    <li>Each domain gets a dedicated LoRA adapter trained independently.</li>
                                    <li>Initialization can either be random or derived from the previous domain's adapter.</li>
                                    <li>The segmentation loss is optimized per domain.</li>
                                    <li>Optional <strong>knowledge distillation</strong> uses the previous domain’s adapter as a teacher.</li>
                                    <li>Each trained adapter is stored separately, preserving domain-specific expertise.</li>
                                  </ul>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/9.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h5>Inference Phase:</h5>
                                  <ul>
                                    <li>Frame-level embeddings are extracted using a <strong>pre-trained CLIP model</strong>.</li>
                                    <li>Embeddings are averaged per video and clustered to compute <strong>domain anchor embeddings</strong>.</li>
                                    <li>A test sequence’s embedding is matched to the closest anchor.</li>
                                    <li>The <strong>corresponding LoRA adapter</strong> is dynamically loaded into the model.</li>
                                  </ul>
                              
                                  <h5>
                                    This approach transforms the challenge of continual learning into a <strong>domain recognition and parameter selection problem</strong>, offering robustness to domain shifts and avoiding typical drawbacks of shared-parameter models.
                                  </h5>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/5.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h3>Observations</h3>
                              
                                  <h4>Continual Learning Strategy Evaluation and Forgetting Analysis</h4>
                              
                                  <h5>
                                    The core challenge in continual learning is <strong>catastrophic forgetting</strong>, where the model degrades on earlier domains after learning new ones. To assess this, performance metrics like <strong>mean Dice Similarity Coefficient (DSC)</strong> and <strong>IoU</strong> were tracked across domains as new ones were added in sequence:
                                  </h5>
                                  <h5><em>Smoke → Blood → Low Brightness → Background Change → Regular</em></h5>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/6.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h5>Key Findings:</h5>
                              
                                  <h6>1. Naive Sequential Fine-tuning</h6>
                                  <ul>
                                    <li>Exhibited <strong>significant forgetting</strong>, especially on early domains.</li>
                                    <li>Example: Performance on Smoke dropped by <strong>-0.27</strong> after Background Change training.</li>
                                    <li>Adapts well to recent domains but often <strong>overfits</strong>, hurting generalization.</li>
                                    <li>Shows minimal forward knowledge transfer to new tasks.</li>
                                  </ul>
                              
                                  <h6>2. Learning without Forgetting (LwF)</h6>
                                  <ul>
                                    <li>Demonstrated <strong>better retention</strong> than the Naive baseline.</li>
                                    <li>Example: Forgetting on Smoke was reduced to <strong>-0.106</strong>.</li>
                                    <li>Uses distillation to maintain old knowledge but may slow new domain adaptation.</li>
                                  </ul>
                              
                                  <h6>3. K-Means + CLIP Adapter Selection (KM)</h6>
                                  <ul>
                                    <li>Showed <strong>the least forgetting</strong>, with scores often near zero or slightly positive.</li>
                                    <li>Example: <strong>+0.021 on Smoke</strong> after Low Brightness training.</li>
                                    <li>Uses <strong>separate adapters</strong> for each domain, preventing knowledge overwriting.</li>
                                    <li>Exhibits better <strong>forward generalization</strong> due to CLIP-guided adapter selection.</li>
                                  </ul>
                              
                                  <h5>
                                    This strategy uses <strong>domain-aware inference</strong> to maintain high performance across tasks and avoids the instability of sequential fine-tuning approaches.
                                  </h5>
                              
                                </div>
                              </div>
                              
                              <br>
                              
                              <h3>WanDB Report</h3>
                              <br>
                              <iframe src="https://wandb.ai/frozenwolf/CL-SAM2/reports/CL-for-Robust-Video-Segmentation-of-Surgical-Tool--VmlldzoxMjkwMTg5OQ" style="border:none;height:1024px;width:100%"></iframe>
                              

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>




    <div class="portfolio-modal modal fade" id="cl-asr" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>A Study on Regularization-Based Continual Learning Methods for Indic ASR</h2>
                            <br>
                            
                            <div class="row">
                                <div class="col-md-8 col-md-offset-2">
                                    <h3>Abstract</h3>
                                    <p class="text-justify">
                                        India's linguistic diversity challenges inclusive Automatic Speech Recognition (ASR) system development. Traditional multilingual models, requiring simultaneous access to all language data, are impractical due to sequential data arrival and privacy constraints.
                                    </h5>
                                    <p class="text-justify">
                                        Continual Learning (CL) enables models to learn new languages sequentially without catastrophically forgetting prior knowledge. This paper investigates CL for ASR on Indian languages using the subset of the <em>indicSUPERB</em> benchmark.
                                    </h5>
                                    <p class="text-justify">
                                        We employ a Conformer-based hybrid RNN-T/CTC model, initially pretrained on Hindi, which is subsequently trained incrementally on eight additional Indian languages, for a sequence of nine languages in total.
                                    </h5>
                                    <p class="text-justify">
                                        We evaluate three prominent regularization and distillation-based CL strategies: Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without Forgetting (LwF), chosen for their suitability in no-replay, privacy-conscious scenarios.
                                    </h5>
                                    <p class="text-justify">
                                        Performance is analyzed using Word Error Rate (WER) for both RNN-T and CTC paths on clean/noisy data, and knowledge retention via Backward Transfer. We explore varying training epochs (1, 2, 5 and 10) per task.
                                    </h5>
                                    <p class="text-justify">
                                        Results, compared against naive fine-tuning, demonstrate CL's efficacy in mitigating forgetting for scalable ASR in diverse Indian languages under realistic constraints.
                                    </h5>
                                </div>
                            </div>
                            
                            <div class="row">
                                <div class="col-md-8 col-md-offset-2">
                                    <br>
                                    <h3>Observations</h3>
                            
                                    <h4>CTC Benchmarking</h4>
                                    <img src="img/projects/indiccl/1.png" class="img-responsive" style="max-width: 100%;">
                                    <p class="text-justify">
                                        As shown in Figure 1, the average WER across tasks reveals a clear ranking among methods. <strong>LwF</strong> achieves the best overall performance, followed by <strong>EWC</strong>, then <strong>MAS</strong>, with naive fine-tuning performing the worst. This ranking is particularly evident in short and medium task horizons. For longer sequences, however, the performance gap between methods narrows considerably. Naive fine-tuning, in particular, produces the highest WER maxima across tasks. When analyzing backward transfer (BWT), <strong>MAS</strong> performs best in short sequences, while <strong>LwF</strong> excels in medium-length tasks. For longer sequences, both <strong>MAS</strong> and <strong>LwF</strong> converge to similar average BWT values, whereas <strong>EWC</strong> and naive fine-tuning fall behind.
                                    </h5>
                            
                                    <h4>RNN-T Benchmarking</h4>
                                    <img src="img/projects/indiccl/9.png" class="img-responsive" style="max-width: 100%;">
                                    <p class="text-justify">
                                        Figure 9 shows that RNN-T consistently outperforms CTC in WER across all continual learning strategies. Among these, <strong>EWC</strong> achieves the lowest WER across task lengths, demonstrating strong performance retention on the current task. However, this benefit comes at a cost: <strong>EWC</strong> exhibits the worst BWT of all methods, even lower than that of naive fine-tuning, indicating substantial forgetting. <strong>MAS</strong> shows some improvement in BWT for medium-length sequences, but for longer horizons, BWT scores deteriorate across all methods except <strong>EWC</strong>, eventually becoming nearly indistinguishable.
                                    </h5>
                            
                                    <h4>General Comparison of CL Methods under Noisy Settings</h4>
                                    <img src="img/projects/indiccl/2.png" class="img-responsive" style="max-width: 100%;">
                                    <p class="text-justify">
                                        In noisy conditions (Figure 2), both <strong>LwF</strong> and <strong>MAS</strong> outperform <strong>EWC</strong> and the naive baseline in BWT, suggesting better retention of prior knowledge. Interestingly, noise appears to improve backward transfer, likely due to regularization effects. However, this improvement comes with a trade-off: WER increases, and models perform better on clean audio in absolute terms. This contrast indicates that noise can enhance stability, by reducing forgetting, while simultaneously impairing plasticity, by diminishing learning precision, which is reflected in the higher WER.
                                    </h5>
                            
                                    <img src="img/projects/indiccl/4.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/3.png" class="img-responsive" style="max-width: 100%;">
                            
                                    <h4>WER Performance Analysis</h4>
                                    <p class="text-justify">
                                        Figures 3 and 4 present WER trends over increasing task lengths. Evaluations are averaged over the last two and current tasks, categorized as short (1–3), medium (1–6), and long (1–9). In general, models perform better with clean data. Among the methods, <strong>LwF</strong> consistently maintains WER below 1.0, with high stability indicated by narrow shaded variance regions.
                                    </h5>
                                    <p class="text-justify">
                                        Interestingly, the upper bounds of noisy WER for <strong>LwF</strong> are comparable to the maxima seen under clean conditions. This can be attributed to its distillation-based loss, which prevents overfitting to noisy inputs by anchoring the model to previous predictions. <strong>MAS</strong> follows a similar pattern, though with slightly lower stability. <strong>EWC</strong> occasionally achieves better minimum WERs, particularly for short tasks, but continues to show poor BWT. The naive method performs surprisingly well in short sequences but fails to retain knowledge over longer horizons. Overall, <strong>LwF</strong> demonstrates the effectiveness of knowledge distillation in maintaining a balance between acquiring new knowledge and retaining previous learning. For longer sequences, average WER tends to decline, possibly due to simpler language characteristics in later tasks.
                                    </h5>
                            
                                    <h4>EWC Ablation Studies</h4>
                                    <img src="img/projects/indiccl/8.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/14.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/11.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/5.png" class="img-responsive" style="max-width: 100%;">
                                    <p class="text-justify">
                                        In Figure 5, we examine the impact of different regularization strengths in <strong>EWC</strong> by testing λ<sub>EWC</sub> ∈ {5, 10}. While both values yield similar outcomes, λ<sub>EWC</sub> = 10 leads to slightly better WER in medium and long tasks, though the benefit is minimal in short tasks. BWT trends (Figure 8) for both values remain close to those of the naive baseline, suggesting limited ability to retain performance on earlier tasks. Additionally, results from epoch-wise ablation (Figure 11) show that increasing training epochs reduces WER, with the best results achieved at epoch 10. However, BWT steadily declines with more epochs (Figure 14), confirming the stability-plasticity trade-off: improved learning on new tasks often leads to increased forgetting of previous ones.
                                    </h5>
                            
                                    <img src="img/projects/indiccl/6.png" class="img-responsive" style="max-width: 100%;">
                            
                                    <h4>LwF Ablation Studies</h4>
                                    <p class="text-justify">
                                        As shown in Figure 6, adjusting the distillation weight α<sub>KD</sub> significantly impacts <strong>LwF</strong>’s performance. A higher value of 0.5 severely limits the model’s ability to learn new tasks, resulting in WERs close to 1.0 across all horizons—worse than naive fine-tuning for short sequences. In contrast, α<sub>KD</sub> = 0.1 strikes a better balance, achieving WER comparable to or better than naive fine-tuning while maintaining much stronger BWT. As shown in Figure 8, the 0.5 configuration yields the highest BWT, primarily because the model barely updates and effectively freezes previous knowledge. The 0.1 setting enables more meaningful learning while controlling forgetting.
                                    </h5>
                                    <p class="text-justify">
                                        Epoch-wise trends (Figures 10 and 14) are consistent with those observed in <strong>EWC</strong>. Increasing the epochs improves WER but worsens BWT.
                                    </h5>
                            
                                    <img src="img/projects/indiccl/7.png" class="img-responsive" style="max-width: 100%;">
                            
                                    <h4>MAS Ablation Studies</h4>
                                    <p class="text-justify">
                                        In Figure 7, we compare <strong>MAS</strong> with regularization weights α<sub>ctx</sub> of 0.3 and 1.0. The stronger setting of 1.0 consistently achieves better WER and shows more stable variance across tasks. Its shaded performance region closely overlaps with that of naive fine-tuning, though with lower dispersion. When examining BWT (Figure 8), the 0.3 configuration performs better, matching <strong>LwF</strong> in retaining knowledge.
                                    </h5>
                                    <p class="text-justify">
                                        As with the other methods, <strong>MAS</strong> exhibits the stability-plasticity trade-off: increasing epochs (Figure 12) lowers WER but leads to worsening BWT (Figure 14). This consistent trend across methods emphasizes the fundamental challenge in continual learning of effectively balancing the acquisition of new information with the retention of existing knowledge.
                                    </h5>
                            
                                    <img src="img/projects/indiccl/12.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/13.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/10.png" class="img-responsive" style="max-width: 100%;">
                                </div>
                            </div>
                            
                            <br>
                            <h3>WanDB Report</h3>
                            <br>
                            <iframe src="https://wandb.ai/frozenwolf/CL-ASR/reports/Regularization-Based-CL-Methods-for-Indic-ASR--VmlldzoxMjg5OTAwNw"
                                style="border:none;height:1024px;width:100%"></iframe>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>






    <div class="portfolio-modal modal fade" id="mimic" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Medical Multi-Modal Fusion & Cross Modal Alignment</h2>
                            <br>
                            <h5>
                                <img src="img/projects/iisc_mimic.gif" class="img-responsive img-centered" alt="">
                            
                                <strong>Dataset Analysis and Performance Report</strong><br><br>

                                This project focuses on analyzing and improving the performance of models using a multimodal dataset comprising Electronic Health Records (EHR) data, including demographic information, ICU vitals, Chest X-rays (CXR), ECG signals, and clinical notes. The primary tasks include predicting readmission, mortality, and length of stay (LOS) for patients based on data available within 24 to 48 hours after admission.<br><br>
                                
                                <strong>Modalities and Tasks</strong><br>
                                <ul>
                                    <li><strong>EHR Demographics:</strong> Includes categorical data like race and age, with age discretized into groups due to deidentification.</li>
                                    <li><strong>ICU Vitals:</strong> A selection of 39 vital signs, time series data, and categorical events like procedures.</li>
                                    <li><strong>CXR:</strong> Time series of chest X-ray images.</li>
                                    <li><strong>ECG:</strong> 12-lead ECG signals represented as time series.</li>
                                    <li><strong>Clinical Notes:</strong> Series of notes, such as discharge and radiology notes.</li>
                                </ul>
                                The tasks involve:
                                <ol>
                                    <li><strong>Readmission Prediction:</strong> Whether a patient will be readmitted within a month after discharge.</li>
                                    <li><strong>Mortality Prediction:</strong> Predicting if the patient will survive or die.</li>
                                    <li><strong>Length of Stay Prediction:</strong> Whether the stay will exceed 3 or 7 days.</li>
                                </ol><br>
                                
                                <strong>Dataset and Class Imbalance</strong><br>
                                <ul>
                                    <li>The dataset shows heavy class imbalance across all tasks and modalities. This imbalance persists even when considering the intersection of multiple modalities, leading to a significant portion of the data being unused.</li>
                                    <li>The dataset was split into training, validation, and test sets with an approximately 80%-10%-10% split, maintaining the proportions of each modality combination and balancing the classes.</li>
                                </ul><br>
                                
                                <strong>Experimentation Setup</strong><br>
                                <ul>
                                    <li><strong>Data Pipeline:</strong> A heavily modified HADM pipeline handles data loading and interlinked modality fusion. PyTorch Lightning with Distributed Data Parallel (DDP) is used for training, with configurations managed via YAML files.</li>
                                    <li><strong>Metrics:</strong> Evaluation metrics include binary classification metrics monitored across train, validation, and test sets. Performance is also assessed at the modality and modality combination levels.</li>
                                    <li><strong>Model Structure:</strong> The model pipeline includes a modality encoder, a time series encoder, and a binary classification head. Class imbalance is addressed through oversampling and undersampling strategies, depending on the skewness of the class distribution.</li>
                                </ul><br>
                                
                                <strong>Observations from Literature</strong><br>
                                <ul>
                                    <li>Cross-modal alignment is rarely performed or analyzed extensively in current research, and ECG data is often excluded or underutilized in multimodal tasks. Reported F1 scores are typically below 0.7, highlighting the challenge in improving model performance.</li>
                                </ul><br>
                                
                                <strong>Experimentation Results</strong><br>
                                <ul>
                                    <li><strong>Validation Embeddings:</strong> Representations of different training strategies and modality combinations are visualized to observe variation in embeddings.</li>
                                    <li><strong>Task Metrics:</strong> Metrics for LOS_3, LOS_7, and mortality tasks are carefully monitored.</li>
                                </ul><br>
                                
                      
                                
                                <div class="publ_">
                                    <ul style="list-style-type: none; padding: 0;">
                                        <li style="margin-bottom: 10px;">
                                            <a href="https://docs.google.com/presentation/d/1CU_Gzy2BEQ5bYzLrQQq5W4BrMf7QgzsWftuQOCUiqvE/edit?usp=sharing" target="_blank" class="button google-slides">
                                                <i class="fa fa-google icon"></i>
                                                <b>Slides</b>
                                            </a>
                                        </li>
                                         <li style="margin-bottom: 10px;">
                                             <a href="https://oceanic-antler-eb0.notion.site/1c6ae01d802f44d4aa1c4bafe6f3d4a2?v=0198cff002b34f94ac6bce016294b0ad"  target="_blank" class="button notion">
                                                 <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                                 <b>Notion</b>
                                             </a>
                                         </li>
                                         <li>
                                             <a href="https://oceanic-antler-eb0.notion.site/SOTA-models-963b2664d0894f6bb0f09d6a12242704?pvs=4" target="_blank" class="button notion">
                                                 <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                                 <b>Notion (SOTA)</b>
                                             </a>
                                         </li>
                                            
                                         <li style="margin-bottom: 10px;">
                                            <a href="https://drive.google.com/file/d/17V-IovP9YHo6Lb0jlTgjNYpH9m7nXoAm/view?usp=sharing" target="_blank" class="button google-slides">
                                                <i class="fa fa-google icon"></i>
                                                <b>Report</b>
                                            </a>
                                        </li>
                                         
                                    </div>

                            </h5>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://api.wandb.ai/links/tirthajitb-indian-institute-of-science/s7ncw5hy" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="nus" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Surgical Motion Planning</h2>
                            <br>
                            <h5>
                                <img src="img/projects/nus/surgicalcommand2motion-Page-1.drawio.png" class="img-responsive img-centered" alt="">
                            
                                <h5><strong>Project Summary:</strong></h5>

                                <h5>The project focuses on the development of an embodied robotic agent designed to execute high-level natural language surgical instructions. This system leverages a combination of Reinforcement Learning (RL), Imitation Learning, and a pre-trained Large Language Model (LLM) to function as a "planner." The agent can perform short-horizon skills across multiple surgical scenarios using a library of pre-trained policies. These policies are selected based on the scenario, with skills that are common across scenarios mapped under common primitives with textual labels.</h5>
                                
                                <h5><strong>Methodology:</strong></h5>
                                
                                <h5><strong>1. Planning and Filtering:</strong> The LLM is responsible for planning by determining the sequence of actions required to execute a given surgical instruction. A filtering process similar to SayCAN is used, which ranks skills based on both logical reasoning and geometric feasibility. The LLM generates possible action permutations, which are then filtered to a manageable number using a Chain-of-Thought (CoT) process, reducing the likelihood of errors and improving interpretability.</h5>
                                
                                <h5><strong>2. Action Scoring:</strong> Each action is scored based on two criteria: the LLM's planning score and the RL policy’s affordance score, which reflects the geometric feasibility of the action. The best action is selected based on these combined scores, with a fallback to the highest LLM score if all actions are deemed infeasible.</h5>
                                
                                <h5><strong>3. Execution:</strong> Once an action is selected, a human-in-the-loop is given the option to approve or modify the action before execution. The agent updates the scene and action status after each step, which informs subsequent actions.</h5>
                                
                                <h5><strong>4. Experimental Setups:</strong> The system is tested in two main environments: Surrol and Lapgym, each with specific tasks like retracting tissue, reaching a tumor, and picking and placing gauze. The experiments explore various strategies, including closed-loop, open-loop, and inner monologue setups, with scene and action updates. The DINO model is utilized for detecting objects in the surgical environment, such as gauze and blood, by generating bounding boxes around these targets based on textual inputs provided by the LLM. This integration allows for accurate, real-time detection of key elements, which is critical for the success of surgical tasks.</h5>
                                
                                <h5><strong>Key Features:</strong></h5>
                                
                                <h5><strong>Human-in-the-loop Feedback:</strong> Ensures control over the actions performed, allowing for adjustments based on the requirements.</h5>
                                
                                <h5><strong>Chain-of-Thought (CoT):</strong> Improves planning efficiency and makes the process more interpretable for surgical tasks.</h5>
                                
                                <h5><strong>Scene Description Generation:</strong> Simplifies the scene context for the LLM, making the planning process more effective and relatable to real-world applications.</h5>
                                
                                <h5>The project demonstrates the feasibility of integrating LLMs with RL, Imitation Learning, and DINO for complex surgical tasks, providing a robust foundation for further development in autonomous surgical systems.</h5>
                                

                            
                            </h5>
                            <br>
                            <h3> Demo </h3>
                            <img src="img/projects/nus/15-ezgif.com-video-to-gif-converter.gif" class="img-responsive img-centered" alt="">
                            <img src="img/projects/nus/ezgif-2-584f48650a.gif" class="img-responsive img-centered" alt="">
                            

                            <br>
                         
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="cmu" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</h2>
                            <br>
                            <h5>
                                <img src="img/projects/cmu.png" class="img-responsive img-centered" alt="">
                            
                                <h5><strong>Description:</strong></h5>
                                <h5>This project tackles the critical task of particle localization and classification in cryo-electron tomography (cryo-ET), focusing on real-world challenges such as limited and weakly annotated data. The study is conducted on both private datasets and the SHREC 2021 simulated dataset, aiming to establish robust baselines and explore advanced techniques to improve model performance. The goal is to enhance the accuracy and robustness of particle detection and classification, making it more effective for structural biology applications.</h5>
                                
                                <h5><strong>Methodology:</strong></h5>
                                
                                <h5><strong>1. Supervised Learning:</strong> The approach begins by generating <strong>pseudo-strong labels</strong> from weak labels, converting them into segmentation masks by drawing spheres of minimum radius around the particle centroids. These masks serve as ground truth for training. The tomograms are divided into smaller subtomograms, which are fed into the model to generate voxel-level probabilities for each particle type. Key techniques include:</h5>
                                <ul>
                                    <li><strong>Balanced Sampling and Window Method:</strong> Ensuring that training data is evenly sampled and that particle centroids are accurately localized within smaller windows.</li>
                                    <li><strong>AugMix Modification:</strong> A tailored version of AugMix is applied to improve data augmentation, focusing on enhancing the diversity and robustness of the training samples.</li>
                                </ul>
                                
                                <h5><strong>2. Self-Supervised Learning:</strong> To further improve the model's performance, <strong>SimCLR</strong> is employed. This technique leverages contrastive learning to learn representations that are invariant to data augmentation, improving the model’s ability to generalize from few-shot data.</h5>
                                
                                <h5><strong>3. Post-Processing:</strong> After initial predictions, advanced post-processing techniques are applied to refine the results:</h5>
                                <ul>
                                    <li><strong>Connected Components Analysis:</strong> Used to identify and isolate individual particles from the segmentation masks, improving the accuracy of particle localization.</li>
                                    <li><strong>MeanShift Clustering:</strong> Applied to group detected particles based on their spatial coordinates, helping to better classify and localize particles within the tomogram.</li>
                                </ul>
                       
                            </h5>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://wandb.ai/frozenwolf/semi-self%20supervised%20fewshot/reports/Enhancing-Few-Shot-Detection-with-Weak-Labels-A-Self-Supervised-Learning-and-Augmix-Approach-in-Cryo-ET--Vmlldzo5MTE3MDk4" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="SAM-PM" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SAM-PM: Enhancing Video Camouflaged Object Detection</h2>
                            <br>
                            <h5>
                                <img src="img/assests/cvpr.png" class="img-responsive img-centered" alt="">
                            
                                <h5>
                                    <strong>Methodology</strong><br>
                                    The proposed <strong>SAM-PM</strong> framework adapts the Segment Anything Model (SAM) for the <strong>Video Camouflaged Object Detection (VCOD)</strong> task. SAM-PM consists of two main components: the <strong>Temporal Fusion Mask Module (TFMM)</strong> and the <strong>Memory Prior Affinity Module (MPAM)</strong>.
                                    <ol>
                                    <li><strong>Temporal Fusion Mask Module (TFMM)</strong>: This module enhances temporal information by integrating mask embeddings from multiple frames. It uses a <strong>spatio-temporal cross-attention</strong> mechanism to create temporally enriched mask embeddings.
                                    </li>
                                    <li><strong>Memory Prior Affinity Module (MPAM)</strong>: MPAM utilizes the temporally infused mask embeddings from TFMM along with image embeddings from the current and previous frames. It applies <strong>affinity</strong> to strengthen temporal consistency in mask predictions.
                                </li>
                                    SAM-PM operates in a <strong>semi-supervised</strong> manner, where only the first frame's ground truth mask is used for training. The framework keeps SAM's weights frozen and trains only the SAM-PM components, ensuring <strong>parameter efficiency</strong> with less than 1 million parameters.
                                </ol>
                                    During training and inference, SAM-PM updates its memory with new frames and their predicted masks while discarding outdated data to maintain temporal coherence.
                                    </h5>
                                    
                            </h5>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://api.wandb.ai/links/spider-r-d/7izxfp8j" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SAM-PM')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                             
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="optical" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Test-time adaptation for Optical Flow </h2>
                            <br>
                            <h5>
                                <img src="img/projects/opticalflow.png" class="img-responsive img-centered" alt="">
                                <img src="img/projects/opticalflowex.png" class="img-responsive img-centered" alt="">
                            
                                <h5>
                                    We proposed a test-time adaptation method for optical flow estimation using RAFT-like optical flow models to assesses and improve EPE scores on the Sintel and KITTI 2015 datasets. The implementation is built with Lightning Fabric, allowing for efficient management of training processes. Key features include advanced offloading, checkpointing, and multi-node Distributed Data Parallel (DDP) strategies to optimize batch sizes and assess their impact on model performance. This approach is designed to improve the precision of optical flow predictions and adapt to various real-world conditions.
                                    
                                    <br>
                                    <h5><strong>Tech Stack</strong><br>
                                    <ul>
                                        <li><strong>Framework:</strong> Torch Scale</li>
                                    <li><strong>Framework:</strong> PyTorch Lightning Fabric</li>
                               
                                </h5>  
                            </h5>
                            <br>
                            <h3> WanDB Report </h3>
                            Currently in progress and kept private.
                            
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="nllp" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Natural Legal Language Processing</h2>
                            <br>
                            <h5>
                                <img src="img/assests/emnlp.png" class="img-responsive img-centered" alt="">
                                
                                <h5>
                                    Abstract
A key component of the Natural Language Processing (NLP) pipeline is Sentence Boundary Detection (SBD). Erroneous SBD could affect other processing steps and reduce performance. A few criteria based on punctuation and capitalization are necessary to identify sentence borders in well-defined corpora. However, due to several grammatical ambiguities, the complex structure of legal data poses difficulties for SBD. In this paper, we have trained a neural network framework for identifying the end of the sentence in legal text. We used several state-of-the-art deep learning models, analyzed their performance, and identified that Convolutional Neural Network(CNN) outperformed other deep learning frameworks. We compared the results with rule-based, statistical, and transformer-based frameworks. The best neural network model outscored the popular rule-based framework with an improvement of 8% in the F1 score. Although domain-specific statistical models have slightly improved performance, the trained CNN is 80 times faster in run-time and doesn’t require much feature engineering. Furthermore, after extensive pretraining, the transformer models fall short in overall performance compared to the best deep learning model.
                                </h5>  
                            </h5>
                            <br>
                         
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/NLLP-ML/SBD')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>



    
    <div class="portfolio-modal modal fade" id="OCR" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Intelligent OCR </h2>
                            <img src="img/projects/ocr.gif" class="img-responsive img-centered" alt="">
                            <img src="img/projects/ocr.png" class="img-responsive img-centered" alt="">
                             
                            <h5>
                                This project involves the development of an advanced Optical Character Recognition (OCR) software, leveraging a combination of cutting-edge neural network models: CRAFT, Faster R-CNN, Tesseract, and a Siamese network model. The software is hosted on Azure Cloud, utilizing a virtual machine (VM) to run the server and process documents. The entire system is designed to convert scanned documents into editable text, extract bounding boxes for words and sentences, classify sentences into categories, and provide a comprehensive annotation interface for user modifications.
                                </h5>
                                <h5>
                                <b>Key Features:</b>
                                <ul>
                                <li><b>Model Integration:</b> The OCR software integrates multiple state-of-the-art models trained using PyTorch on the FUND dataset. This setup ensures high accuracy and reliability in text extraction and classification.</li>
                                <li><b>Document Processing:</b> Users can upload scanned documents in various formats (.png, .jpg, .jpeg, and .pdf) to the frontend website. For PDFs, only the first page is processed. The software extracts editable text, provides bounding boxes for each word and sentence, and classifies sentences into categories such as 'other', 'question', 'answer', and 'header'.</li>
                                <li><b>Annotation Interface:</b> The frontend features a user-friendly annotation interface built with annotorious.js. Users can modify model predictions or annotate documents from scratch. Annotations can be saved and downloaded in a simple .txt format.</li>
                                <li><b>Offline Processing:</b> The software supports offline batch processing, allowing users to process multiple images at once. Users can choose between output formats like MTX or FUND dataset formats.</li>
                                <li><b>Server Details:</b> The models run on an Azure VM with Linux (Ubuntu 18.04), specifically a Standard B2s instance (2 vCPUs, 4 GiB memory). Due to server connection constraints, there might be occasional interruptions in access, but comprehensive setup instructions are provided for replicating the environment.</li>
                                <li><b>Training and Metrics:</b> Model training is conducted using PyTorch, with detailed explanations of training steps and performance metrics. All trained models and predictions on public test datasets are available for download.</li>
                                </ul>
                                </h5>
                                
                            <br>
                            <img src="img/projects/ocr2.png" class="img-responsive img-centered" alt="">
                            <img src="img/projects/ocr_demo.png" class="img-responsive img-centered" alt="">
                               
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/OCR')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                        <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://youtu.be/8Cci5rw-KyY')"><i
                                            class="fa fa-fw fa-youtube"></i> Demo</button>
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="i-Pravesh" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>i-Pravesh </h2>
   
                        <img src="img/projects/sih.png" class="img-responsive img-centered" alt="">
                        <img src="img/projects/sih_.png" class="img-responsive img-centered" alt="">
                            <h5>
                                <h5>I-Pravesh is a sophisticated Smart Attendance Android application designed to simplify and enhance the attendance recording process. It uses advanced face detection and recognition technologies, specifically MobileFaceNet combined with TensorFlow Lite, for accurate biometric authentication. The app not only performs fast and reliable face recognition but also ensures that the user is within a designated location through geofencing. It maintains face embeddings and encrypts credentials for secure data handling. Additionally, I-Pravesh features a comprehensive dashboard for both users and administrators, providing a user-friendly interface for managing and monitoring attendance records efficiently.</h5>
                            <br>
                        
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SIH')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>

                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="summarise" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SummarizeIQ: An Integrated Summarization and Content Analysis Engine </h2>
   
                        <img src="img/projects/summarise.png" class="img-responsive img-centered" alt="">
                              
                            <h5>
                                <h5>
                                    SummarizeIQ combines T5 summarization with Torch Serve, incorporating advanced extractive methods like LexRank and TextRank for enhanced text processing. The system features keyword generation and employs query-based content filtering using cosine similarity to deliver precise and relevant summaries. It also boasts a user-friendly website interface, allowing seamless interaction with the summarization tools. Additionally, SummarizeIQ supports multi-threaded web scraping, enabling efficient and concurrent data extraction from various sources for comprehensive article analysis and summarization.
                                </h5>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SummarizeIQ')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>

                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="realestatevr" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Real Estate VR </h2>
   
                        <img src="img/projects/vr.gif" class="img-responsive img-centered" alt="">
                       
                            <h5>
                                <h5>
                                    <h5><strong>Problem Statement</strong><br>
                                        Traditional real estate methods rely on physical visits and static images, limiting immersion and convenience. To enhance the real estate experience, we aim to integrate virtual reality (VR) technology, creating a user-friendly, cross-platform VR application. This solution will offer immersive property exploration and real-time updates, modernizing how users view and interact with properties.</h5>
                                        
                                        <h5><strong>Design</strong><br>
                                        1. <strong>Asset Coordination and Environment Generation:</strong> Utilizing Unity’s assets and spatial mapping, we create a realistic, adaptable virtual environment for immersive exploration.<br>
                                        2. <strong>Advanced User Interaction with XR Device Simulator:</strong> An intuitive interface with Unity’s XR Device Simulator allows seamless navigation and interaction through a 'digital wand,' enhancing user experience with C# scripting.<br>
                                        3. <strong>Dynamic Building Placement with Spatial Intelligence:</strong> Real-time building placement is facilitated by Unity’s spatial analysis, offering precise options based on spatial context.<br>
                                        4. <strong>Futuristic Navigation Paradigms:</strong> Advanced navigation includes teleportation and aerial views, allowing instant spatial traversal and comprehensive property inspection.<br>
                                        5. <strong>Precision-Engineered C# Scripting:</strong> C# scripts ensure smooth user controls, dynamic building placement, and effective interaction management.<br>
                                        Our VR platform allows users to add plots, select and place buildings, and view properties with detailed information. It supports flythrough and teleportation, with eagle-eye views for broader city and plot planning.</h5>
                                        
                                        <h5><strong>Key Features</strong><br>
                                        - Add and visualize new plots<br>
                                        - Place houses with size and price details<br>
                                        - Immersive VR interface<br>
                                        - Flythrough and teleportation travel<br>
                                        - Multi-plot monitoring with eagle-eye views</h5>
                                        
                                        <h5><strong>Tools Used</strong><br>
                                        - Unity<br>
                                        - C#<br>
                                        <img src="img/projects/vr.png" class="img-responsive img-centered" alt="">
                               
                                </h5>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/Real-Estate-VR')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                        <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://youtu.be/galwI6_rIiw')"><i
                                            class="fa fa-fw fa-youtube"></i> Demo</button>
    
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="compiler" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Yet Another Python Compiler</h2>
   
                        <img src="img/projects/compiler.gif" class="img-responsive img-centered" alt="">
                       
                            <h5>
                                A Python Compiler, built with Bison and Flex, processes Python code by tokenizing it, parsing it into a syntax tree, and performing semantic checks. 
                                It generates intermediate code and applies optimizations like constant folding and dead code elimination. 
                                Key features include lexical analysis, syntax and semantic error detection, and code optimization.

                                <h5><strong>Overview</strong><br>
                                    The project focuses on developing a compiler for a custom programming language, implementing various stages of compilation including lexical analysis, parsing, semantic analysis, intermediate code generation, and code optimization.</h5>
                                    
                                    <h5><strong>Tech Stack</strong><br>
                                    <strong>Programming Language:</strong> Python<br>
                                    <strong>Tools:</strong> YACC (Yet Another Compiler Compiler), Lex (for lexical analysis)<br>
                                    <strong>Data Structures:</strong> Symbol tables, parse trees, abstract syntax trees (ASTs)</h5>
                                    
                                    <h5><strong>Features and Contributions</strong></h5>
                                    
                                    <h5><strong>Lexical Analysis:</strong><br>
                                    Token identification and lexical error detection.<br>
                                    Developed token identification and error detection mechanisms.<br>
                                    Created a symbol table to manage identifiers and constants.</h5>
                                    
                                    <h5><strong>Parser:</strong><br>
                                    Syntax declaration, indentation and syntactic error detection.<br>
                                    Implemented syntax rules using YACC for expressions and control flow.<br>
                                    Generated parse trees and abstract syntax trees (ASTs).</h5>
                                    
                                    <h5><strong>Semantic Analysis:</strong><br>
                                    SDD + SDT, Annotated Parse Tree, and Semantic Error detection.<br>
                                    Designed syntax-directed translations to ensure semantic correctness.<br>
                                    Detected semantic errors such as undeclared variables and misuse of reserved identifiers.</h5>
                                    
                                    <h5><strong>Intermediate Code Generation (ICG):</strong><br>
                                    Generated three-address code and quadruples.<br>
                                    Implemented backpatching for jump statements.</h5>
                                    
                                    <h5><strong>Code Optimization:</strong><br>
                                    Basic blocks, DAG, CFG, Induction Variable elimination.<br>
                                    Applied optimization techniques including constant folding, copy propagation, dead code elimination, and peephole optimization.<br>
                                    Enhanced code efficiency through these optimizations.</h5>
                                    

                                </h5>
                            <br>
                            <br>
                            <h3> Report </h3>
                            <br>
                            <iframe src="img/projects/compiler/Group13_106121045_106121099_106121129.pdf" width="100%" height="600px"></iframe>
                            <br>
                            <h4> Lexical Analsysis </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Lexer_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> Parser </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Parser_Final_Report.pdf" width="100%" height="600px"></iframe>
                            <br>
                            </details>
                            <br>
                            <h4> Semantic Analsysis </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Semantic_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> ICG </h4>
                            <details>
                           
                            <iframe src="img/projects/compiler/ICG_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> Optimization </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Optimization_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/YA-Python-Compiler')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="octtree" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Oct tree based 3D OpenGL Renderer</h2>
   
                        <img src="img/projects/octtree.gif" class="img-responsive img-centered" alt="">
                       
                            <h5>
                                The Octree-Based 3D OpenGL Renderer, created with PyOpenGL, efficiently renders 3D scenes by using an octree data structure to manage and optimize rendering performance.
        
                                </h5>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/3D-Model-Renderer-Oct-Tree')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="sart" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Sartorius Cell Instance Segmentation</h2>
   
                        <img src="img/projects/sart.gif" class="img-responsive img-centered" alt="">
                       
                        <h5>
                            Trained R-CNN, U-Net and Detectron2 using Pytorch to detect and delineate
                                distinct objects of interest in biological images depicting neuronal cell types commonly used in studying neurological disorders
                        </h5>
                        <h5><b>Key findings include:</b></h5>
                        <ol>
                            <li><b>Model Comparison:</b> Detectron2 outperformed U-Net and Mask R-CNN, showcasing superior accuracy and robustness in segmenting complex neuronal cell structures.</li>
                            <li><b>U-Net:</b> Known for capturing fine details, U-Net was used with a ResNet34 backbone and a combination of focal and dice losses to handle class imbalance. Post-processing improved segmentation outcomes, though some blurriness persisted.</li>
                            <li><b>Mask R-CNN:</b> Leveraging ResNet50, this model generated object bounding boxes and masks, with loss functions tailored to boundary, mask, and classification losses. Overlapping segmentations were mitigated through pixel-wise intersection operations.</li>
                            <li><b>Detectron2:</b> As a flexible, high-performance framework, Detectron2 provided modular implementation and incorporated innovative techniques like deformable convolutions, leading to its superior performance in this study.</li>
                        </ol>
                        
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/Sartorius-Cell-Instance-Segmentation')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="others" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Other Projects</h2>
   
                        <img src="img/projects/others.gif" class="img-responsive img-centered" alt="">
                       
                        <h5>
                            Projects include: <br>

                            <a href="https://github.com/FrozenWolf-Cyber/Brain-Tumor-Classification"> Tumor classification </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/COVID19-Pneumonia-Chest-X-Ray-Detection"> Pneumonia (COVID-19) chest X-ray classification </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Neural-Style-Transfer"> Neural style transfer </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Tensorflow-Lite-Crack-Detection"> Crack detection with TF Lite </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Face-Recognition"> Face detection using a Siamese Neural Network model to develop a one-shot neural network with FaceNet and MTCNN as the backbone </a> 
                            <a href="https://github.com/FrozenWolf-Cyber/Keres"> Unity based C# game called "The Keres" </a> 
                        </h5>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>



    <!-- About Section -->
    <section class="success  grid-band2" id="presentation">
        <div class="container"  class="grid-band2">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Presentation & Surveys</h2>
                    <hr class="light">
                </div>
            </div>
            


            <div class="row"  class="grid-band2">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/iisc_mimic.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Multi-Modal Representation Learning for clinical data and Test-time adaptation for Optical Flow</strong></div>
                    <div class="pubv"><b>2022</b> </div>
                    These slides and report covers the results from MIMIC-IV multi-modal fusion and optical flow test-time adaptation for RAFT. Notion contains survey on multimodality and contrastive learning methods in MIMIC (III, IV) and similar dataset along with SOTA models available in each modalities such as CXR, EHR, ECG and Notes.
           

                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1CU_Gzy2BEQ5bYzLrQQq5W4BrMf7QgzsWftuQOCUiqvE/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                             <li style="margin-bottom: 10px;">
                                 <a href="https://oceanic-antler-eb0.notion.site/1c6ae01d802f44d4aa1c4bafe6f3d4a2?v=0198cff002b34f94ac6bce016294b0ad"  target="_blank" class="button notion">
                                     <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                     <b>Notion</b>
                                 </a>
                             </li>
                             <li>
                                 <a href="https://oceanic-antler-eb0.notion.site/SOTA-models-963b2664d0894f6bb0f09d6a12242704?pvs=4" target="_blank" class="button notion">
                                     <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                     <b>Notion (SOTA)</b>
                                 </a>
                             </li>
                                
                             <li style="margin-bottom: 10px;">
                                <a href="https://drive.google.com/file/d/17V-IovP9YHo6Lb0jlTgjNYpH9m7nXoAm/view?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Report</b>
                                </a>
                            </li>
                             
                         
                        </ul>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cl.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Continual Learning Survey</strong></div>
                    <div class="pubv"><b>2022</b> </div>
                    These slides and notion database partially covers the classical and SOTA methods in continual learning and its drawbacks till 2022.

                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1l_O9qThYFHkLAaoYyAB4Et7UxxGGsvF3D8gcSbEH6Ls/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://oceanic-antler-eb0.notion.site/113a771fb39c80f18251c3091f4f192a?v=113a771fb39c815782cb000c67af58ab" target="_blank" class="button notion">
                                    <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                    <b>Notion</b>
                                </a>
                            </li>
                        </ul>
                    </div>
                    


                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/LLM planner.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Embodiement Agent (TAMP with LLM) Mini Survey and proposed strategy</strong></div>
                    <div class="pubv"><b>2023</b> </div>
                    These slides and notion database partially covers the embodied agent and Evaluation metrics used in TAMP along with a proposed strategy consisting of Instruction Planner, Coder for debug analysis, and Skill Handler to optimize actions. It learns from execution feedback, continuously improving performance through reflection and memory management.


                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1ppk0cnIqiQhhx0jARil2j1jLPSAShffTaBfjwhznLb8/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://oceanic-antler-eb0.notion.site/f7e79139232c49c896540f515f38893e?v=c559a2753a7b40a8903a21ffb2a485f1" target="_blank" class="button notion">
                                    <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                    <b>Notion</b>
                                </a>
                            </li>
                        </ul>
                    </div>

                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cmu/comb.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-Interpreted Deep-Learning Approach for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="pubv"><b>2024</b> </div>
                    These slides cover contributions of SaSi
                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1-fbPDVgn0zuPuR4-GX3mox6vF11oglS6xcqOVqwfZtw/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                           
                        </ul>
                    </div>

                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/dl_teach.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Introduction to Deep Learning</strong></div>
                    <div class="pubv"><b>Spider R&D Workshop 2023</b> </div>
                    These slides cover entire deep learning from basic till generative models like GANs and Diffusion Based Models. The slides progress from basic neural networks to advanced models like GANs and Diffusion Based Models along with notebooks and visualizations.
                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1OTKlOIsqToT6K8lM0iTgMkguyzXt8DgZOraRozAy6Os/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://linktr.ee/dl_workshop?utm_source=linktree_profile_share&ltsid=84e5ac63-9c82-4d22-9425-5954a9ee3b03" target="_blank" class="button notion">
                                    <i class="fa fa-link"></i> <!-- Placeholder icon for Notion -->
                                    <b>LinkTree</b>
                                </a>
                            </li>
                        </ul>
                    </div>

                </div>
            </div>





    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <script src="js/timeline.js"></script>
    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/freelancer.js"></script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-92670333-1', 'auto');
        ga('require', 'ipMeta', {
            serviceProvider: 'dimension1',
            networkDomain: 'dimension2',
            networkType: 'dimension3',
        });
        ga('ipMeta:loadNetworkFields');
        ga('send', 'pageview');
    </script>
    <script async src="https://ipmeta.io/plugin.js"></script>
</body>

<script>
    document.addEventListener("DOMContentLoaded", function () {
      const url = new URL(window.location);
      const params = url.searchParams;
  
      console.log("myc", params);
      const targetId = params.get("trigger"); // Gets "cl-sam"
      console.log("myc", targetId);
  
      const el = document.getElementById(targetId);
      console.log("myc", el);
      console.log("myc before clicking");
      console.log("myc clicking it", el);
  
      if (el) el.click();
    });
  </script>
  
  
  
</html>
