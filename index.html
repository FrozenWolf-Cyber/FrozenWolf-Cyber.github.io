<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="./img/logos/terminal.png" />

    <title>Gokul Adethya: Personal Website</title>


    <!-- Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/timeline.css" rel="stylesheet">
    <link href="css/freelancer.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet"
        type="text/css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&display=swap" rel="stylesheet">
    <script
    src="https://kit.fontawesome.com/d06c559023.js"
    crossorigin="anonymous"
  ></script>
  <link
    href="https://fonts.googleapis.com/css2?family=Andika&family=Montserrat:wght@300;400;500;600&family=Roboto:wght@300&display=swap"
    rel="stylesheet"
  />
  <link rel="stylesheet" href="style.css" />
    <script src="js/modernizr.js"></script>
    <meta name="google-site-verification" content="of5d9ckLytWKCHg9c9NZ7oTGOIP9pyZIvhsY-E1l-kY" />
    <style>
       
        .button {
            display: inline-flex;
            align-items: center;
            padding: 10px 15px;
            border-radius: 5px;
            text-decoration: none;
            color: white;
            font-weight: bold;
            transition: background-color 0.3s;
            border: none;
            cursor: pointer;
        }
        .google-slides {
            background-color: #5796fb; /* Google Slides blue */
        }
        .notion {
            background-color: #fd6969; /* Notion red */
        }
        .button:hover {
            opacity: 0.9;
        }
        .icon {
            margin-right: 8px; /* Space between icon and text */
        }
    </style>
    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse"
                    data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-about-me-hidden navbar-brand" href="#page-top">GOKUL ADETHYA</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li class="page-scroll">
                            <a href="#timeline">Experience</a>
                        </li>
                    <li class="page-scroll">
                        <a href="#about">Publications</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#projects">Projects</a>
                    </li>

                    <li class="page-scroll">
                        <a href="#presentation">Presentation & Surveys</a>
                    </li>
                    

                    <li>
                        <a href="https://drive.google.com/file/d/1xgukjX0Roh4qaay9HiSJre7WK5HIBV05/view?usp=sharing" target="_blank">CV</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->

        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
        <div class="container">
            <div class="row">

                <div class="col-xs-4">
                    <img class="img-responsive img-circle" src="img/4.jpeg" alt="">
                </div>
                <div class="col-xs-8">
                    <div class="intro-text">
                        <span class="name">Gokul Adethya</span>
                        <br>

                        <span class="about_me"> 
                            <p style="text-align: justify">
                                I will be joining the <b>University of California, San Diego</b> as a Fall 2025 <b>Masters student at the Halıcıoğlu Data Science Institute</b>. I recently completed my <b>B.Tech in Computer Science from NIT Trichy (NITT)</b>.
                                
                                I’ve interned across multiple research labs and institutions. Most recently, I was a Research Intern at <a href="https://cistup.iisc.ac.in/" target="_blank">CiSTUP</a>, <a href="https://iisc.ac.in/" target="_blank">IISc</a>, where I worked on <b>multi-modal fusion</b>, <b>cross-modal alignment in medical datasets (MIMIC)</b>, and test-time adaptation of optical flow models.
                            
                                I previously interned at <a href="https://xulabs.github.io/" target="_blank">Xu Labs</a>, <a href="https://www.cmu.edu/" target="_blank">Carnegie Mellon University</a>, focusing on <b>self-supervised learning for Cryo-ET segmentation</b>. I also worked on <b>surgical task and motion planning</b> during an internship at the <a href="http://www.labren.org/mm/principal-investigator/" target="_blank">National University of Singapore</a>. Additional research internships include empathetic response generation and <b>LLM distillation</b> at <a href="https://www.samsungprism.com/" target="_blank">Samsung PRISM</a>, and <b>legal NLP</b> at NIT Trichy.
                            
                                I led the Machine Learning division at <a href="https://spider.nitt.edu/" target="_blank">Spider R&D</a>, NITT’s student-run research group.
                            </h5>
                            
                            <p style="text-align: justify">
                                <strong>Research Interests:</strong> My research interests are focused on two dimensions: 
                                (1) advancing core AI fields such as <b>Natural Language Processing (NLP)</b>, 
                                <b>Computer Vision (CV)</b>, <b>Multimodal Learning</b>, and complementary topics aimed at addressing real-world settings like 
                                <b>Continual Learning (CL)</b>, <b>Test-time Adaptation (TTA)</b>, <b>Self-supervised Learning</b> (for unlabeled/weakly labelled data), 
                                <b>Robustness</b>, <b>Efficiency</b>, and <b>Explainable AI (XAI)</b>; and 
                                (2) applying these advancements in <b>healthcare</b> and <b>biology</b>.
                                   </h5>
                            
                        
                            <h5>
                                For collaborations, reach out at <b><a href="mailto:gokul3112003.com@gmail.com">gokul3112003.com@gmail.com</a></b>, <b><a href="mailto:gthirumurugan@ucsd.edu">gthirumurugan@ucsd.edu</a></b>
                            </h5>
                        </span>
                        
                        <hr>
                        <span class="skills">
                            <ul class="list-inline">
                                <li>
                                    <a href="https://scholar.google.com/citations?hl=en&user=m8n5yo8AAAAJ"
                                        class="btn-social btn-outline" target="_blank"><i class="ai ai-google-scholar" ></i></a>
                                </li>
                                <li>
                                    <a href="https://www.linkedin.com/in/gokul-adethya/" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-linkedin"></i></a>
                                </li>
                                <li>
                                    <a href="https://github.com/FrozenWolf-Cyber" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-github"></i></a>
                                </li>
                                <li>
                                    <a href="mailto:gokul3112003.com@gmail.comr" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-envelope"></i></a>
                                </li>
                            </ul>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <section id="affilition">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Education</h2>
                    <hr class="light">
                </div>
            </div>
            <ul class="affiliation-list row">
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/ucsd.png" style="width: 196px; height: 110px;"/>
                    </a><br>
                    <b>Master of Science in Data Science<br>University of California San Diego<br><br>
                    (Sep 2025 - Present)</b>
                </li>
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/nitt.png" style="width: 115px; height: 110px;"/>
                    </a><br>
                <b>Bachelor of Technology in Computer Science and Engineering<br>NIT-Trichy<br>
                    (July 2021 - May 2025)</b>
                </li>
            </ul>
        </div>
    </section>

<section class="timeline-class" id="timeline">
    <!-- <header class="main-header"> -->
      <!-- <div class="down-arrow"></div> -->
      
    <div class="container">
      <div class="row">
        <div class="col-lg-12 text-center">
            <br>
            <h2>Experience</h2>
            <h4>Check the project reports for each experienes under Projects section! </h4>
            <hr class="light-white">
        </div>
       </div>

    </header>
    <section id="timeline">
        <ul>
            <h3><b class="larger-bold-text">Prof. Pengtao Xie Lab @ University of California San Diego</b><br><br>Research Intern | July 2025 – Present</h3>
            <li id="ucsd_logo">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">PRM in LLMs via Meta-Learning for Math Reasoning</h2>
                </div>
                <h5>
                    Fine-tuning <strong class="green-text">Process Reward Models (PRMs)</strong> using <strong class="green-text">Qwen2.5 7B</strong> with <strong class="green-text">LoRA</strong> for step-level reward assignment on math reasoning tasks. Leveraging solution trajectories from <strong class="green-text">o4-mini</strong> and <strong class="green-text">Gemini 2.5 Pro</strong>, and exploring <strong class="green-text">domain-weighted meta-learning</strong> strategies, benchmarked on the <strong class="green-text">AIME</strong>.
                </h5>
                  
              </div>
            </li>


            <h3><b class="larger-bold-text">VISTA @ Indian Institute of Science</b><br><br>Research Intern | Jun 2024 – Jan 2025</h3>
            <li id="iisc">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Multi-modality in Healthcare & Test-time Adaptation in Optical Flow</h2>
                </div>
                <h5>
                  Working on cross-modal alignment and <strong class="green-text">self-supervised learning</strong> for medical tasks (<strong class="green-text">MIMIC</strong>), including mortality prediction. Exploring <strong class="green-text">optical flow estimation</strong> and test-time adaptation using <strong class="green-text">Torchscale</strong> and <strong class="green-text">Fabric</strong> for multi-node training and performance optimization.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">Xu Labs @ Carnegie Mellon University</b><br><br>Research Intern | Dec 2023 – May 2024</h3>
            <li id="cmu">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Few-shot Weak Label Cryo-ET Segmentation</h2>
                </div>
                <h5>
                  First-author (submitted to <strong class="red-text">PLOS Computational Biology</strong>) on SaSi, a <strong class="green-text">self-augmented few-shot learning</strong> method for weakly supervised Cryo-ET segmentation. Worked on consistency loss, <strong class="green-text">SimCLR</strong>, AugMix, and pretraining <strong class="green-text">MAE</strong> for denoising/reconstruction. Adapted the <strong class="green-text">Segment Anything Model</strong>.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">Laboratory of Medical Mechatronics @ National University of Singapore</b><br><br>Research Intern | Feb 2023 – Feb 2024</h3>
            <li id="nus">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Surgical Task & Motion Planning </h2>
                </div>
                <h5>
                  Developed <span class="green-text">MASS</span>, an <span class="green-text">LLM</span> + <span class="green-text">RL</span> framework for interpretable <span class="green-text">robotic motion planning</span> in surgical scenarios using PyBullet and LapGym-SOFA (submitted to <strong class="red-text">RA-L</strong>). Trained <span class="green-text">RL</span> policies like HER with <span class="green-text">imitation learning</span> and integrated <strong class="green-text">Grounding DINO</strong> for enhanced planning.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">Spider R&D Club</b><br><br>Head of ML Research | July 2022 – May 2025</h3>
            <li id="spider">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Camouflage Video Segmentation</h2>
                </div>
                <h5>
                  Proposed SAM-PM for video camouflage object detection, improving SAM with minimal parameters (<strong class="red-text">CVPR 2024</strong>). Leading research in <span class="green-text">LLM-driven Task & Motion Planning</span>, <span class="green-text">Reinforcement Learning</span> with custom robotic arms, and <span class="green-text">Continual Learning</span>.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">Samsung PRISM</b><br><br>ML Research Intern | Aug 2022 – Mar 2023</h3>
            <li id="samsung">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Empathetic Response Generation</h2>
                </div>
                <h5>
                  Worked on <strong class="green-text">Empathetic Response Generation</strong> and emotion/intent classification using Flan-T5, BART, and RoBERTa with <strong class="green-text">Hugging Face</strong> for edge devices. Performed <strong class="green-text">Knowledge Distillation</strong> on T5 achieving a 77% size reduction while maintaining BLEU score.
                </h5>
              </div>
            </li>
            
            <h3><b class="larger-bold-text">NIT Trichy</b><br><br>ML Research Intern | Apr 2022 – Oct 2022</h3>
            <li id="nitt">
              <div>
                <div>
                  <h2 class="main-header-custom-h2">Natural Legal Language Processing</h2>
                </div>
                <h5>
                  Worked on <strong class="green-text">Natural Legal Language Processing</strong> using BERT, XLNET, and Hierarchical Transformers on judicial data. Published at <strong class="red-text">EMNLP NLLP 2022</strong> workshop with an 80x speedup for sentence boundary detection using a lightweight CNN model.
                </h5>
              </div>
            </li>
            
        </ul>
    </div>
      </section>
      
      
    <script src="main.js"></script>
</section>



    <section id="news-list" >
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>News</h2>
                    <hr class="light">
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">July 2025</div>
                    <div class="col-xs-8"><b>B.Tech thesis accepted in <span style="color: #E73916">MICCAI 2025 Workshop </span> on  Efficient Medical AI Wokrshop !</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Oct 2024</div>
                    <div class="col-xs-8"><b>Awarded the <span style="color: #E73916">IndiaAI Fellowship</span> and grant, given to only 80 across India, to research <span style="color: #e77b16">Speech & NLP multi-modal with Continual Learning !</b></div>
                </div>

                <div class="row">
                    <div class="col-xs-4 news-date">Sep 2024</div>
                    <div class="col-xs-8"><b>Achieved 12th rank out of 74,000 participants nationwide in the <span style="color: #E73916"> Amazon ML Challenge </span> !</b></div>
                </div>
                <!-- <div class="row">
                    <div class="col-xs-4 news-date">Sep 2024</div>
                    <div class="col-xs-8"><b>One first author paper submitted at <span style="color: #E73916">WACV 2025 Conference</span> (affliated with CMU) !</b></div>
                </div> -->
                <div class="row">
                    <div class="col-xs-4 news-date">Jun 2024</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Indian Institute of Science</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">April 2024</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">CVPR 2024 Workshop </span> on Pixel-level Video Understanding in the Wild Challenge !</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Dec 2023</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Carnegie Mellon University</span> as research intern!</b></div>
                </div>
                <!-- <div class="row">
                    <div class="col-xs-4 news-date">Sep 2023</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">International Journal of Biomedical Engineering and Technology </span> !</b></div>
                </div> -->
                <div class="row">
                    <div class="col-xs-4 news-date">July 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">National University of Singapore</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Feb 2023</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">EMNLP 2022 Workshop </span> on Natural Legal Language Processing !</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Nov 2022</div>
                    <div class="col-xs-8"><b>Top 30 teams in <span style="color: #E73916">NeurIPS 2022 CityLearn Challenge</span> in Multi-agent RL!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Aug 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Samsung PRISM</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">July 2022</div>
                    <div class="col-xs-8"><b>Winner of <span style="color: #E73916">Smart India Hackathon</span> among 160k+ students!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">April 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">NIT-Trichy</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Jan 2022</div>
                    <div class="col-xs-8"><b>Runner up of <span style="color: #E73916">MTX - HackOlympics 2.0 Shaastra</span> hackathon in my first year as a solo participant!</b></div>
                </div>
            </div>
        </div>
    </section>

    <!-- About Section -->
    <section class="success grid-band2" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Publications</h2>
                    <hr class="light">
                </div>
            </div>
            <!-- <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/cvpr.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha, Xingjian Li</div>
                    <div class="pubv"><b>WACV 2025</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="#cmu" target="_blank" style="color:red;" data-toggle="modal"><b>UNDER REVIEW - WanDB</b></a></li>
                        </ul>
                    </div>
                </div>
            </div> -->
            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cl-sam/4.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Continual Learning for Robust Video Segmentation of Robot-Assisted Surgical Tool</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Nitish N, Raghavan Balanathan, Sitara, K</div>
                    <div class="pubv"><b>MICCAI 2025 Workshop - B.Tech Thesis</b> </div>
                    <div class="publ">
                        <ul>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li> -->
                            <li><a href="https://frozenwolf-cyber.github.io/cl-sam.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/indiccl/1.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>A Study on Regularization-Based Continual Learning Methods for Indic ASR</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, S Nirmala</div>
                    <div class="pubv"><b>Funded by IndiaAI fellowship, arxiv pre-print</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://arxiv.org/abs/2508.06280" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://frozenwolf-cyber.github.io/indic-cl.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/cvpr.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SAM-PM: Enhancing Video Camouflaged Object Detection using Spatio-Temporal Attention</strong></div>
                    <div class="puba"> Muhammad Nawfal Meeran, <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha</div>
                    <div class="pubv"><b>CVPR 2024 Workshop</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://github.com/SpiderNitt/SAM-PM" target="_blank" style="color:red;"><b>Code</b></a></li>
                            <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/internal.jpg" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Be My ASSistant: Exploring LLM Empowered Interactive Surgical Assistant for Surgical Sub-Task Automation</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Lalithkumar Seenivasan, Ashwin Krishna Kumar, Mobarakol Islam, Hongliang Ren </div>

                    <div class="pubv"><b> (In progress) RA-L</b> </div>
                    <div class="publ">
                        <ul>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li> -->
                            <li><a href="https://frozenwolf-cyber.github.io/mass.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li> -->
                        </ul>
                    </div>
                </div>
            </div>


            


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cmu/comb.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-Interpreted Deep-Learning Approach for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha, Tianyang Wang, Xingjian Li, Min Xu </div>

                    <div class="pubv"><b>(In progress) PLOS Computational Biology</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://arxiv.org/abs/2505.19948" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://frozenwolf-cyber.github.io/sasi.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                            <li><a href="https://docs.google.com/presentation/d/1-fbPDVgn0zuPuR4-GX3mox6vF11oglS6xcqOVqwfZtw/edit?usp=sharing" target="_blank" style="color:blue;"><i class="fa fa-google icon"></i><b>Slides</b></a></li>
                       
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li> -->
                        </ul>
                    </div>

                 

                

                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/emnlp.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Efficient deep learning-based sentence boundary detection in legal text</strong></div>
                    <div class="puba"> Reshma Sheik, <b>T Gokul</b>, S Nirmala</div>
                    <div class="pubv"><b>EMNLP NLLP 2022 Workshop</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://aclanthology.org/2022.nllp-1.18.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://github.com/NLLP-ML/SBD" target="_blank" style="color:red;"><b>Code</b></a></li>
                            <li><a href="img/assests/emnlp.txt" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Talks</h2>
                    <hr class="light">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 text-center">
                <div class="pubt">Known unknowns: Learning novel concepts using reasoning-by-elimination (UAI Oral Talk)</div>
                <br>
                <iframe src="https://drive.google.com/file/d/1aqM2XWzicxCGEe6BbVHKozkmdvvUd2kp/preview"
                    width="480" height="320" allowfullscreen></iframe>
                    
                </div>

            </div>

        </div>
    </section> -->




    <!-- Portfolio Grid Section -->
    <section id="projects" class="" >
        <div class="">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Projects</h2>

                    <h5>
                        <strong class="click_custom">CLICK</strong> on the project to get the project report and 
                        <img src="https://raw.githubusercontent.com/wandb/assets/main/wandb-logo-yellow-dots-black-wb.svg" 
                             alt="W&B Logo" 
                             style="height: 3em; vertical-align: middle;" />
                        run reports it !!!
                      </h5>
                                <hr class="light">
                </div>
            </div>
            <div class="band">
                <div class="item-1" style="background-color: #e8eef7;">

                    <a id="onclick-indic" href="https://frozenwolf-cyber.github.io/indic-cl.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="width: 35vw; height: 20vw; background-image: url(img/projects/indiccl/indic.gif);"></div>
                        <article class="article">
                            <h1>A Study on Regularization-Based Continual Learning Methods for Indic ASR</h1>
                            <h5>
                                Sequentially adapting a Conformer-based ASR model to nine Indian languages using regularization based continual learning strategies, improving multilingual performance without forgetting.    </h5>
                            <span> Funded by IndiaAI fellowship</span>
                        </article>
                    </a>

                </div>

                <div class="item-2" style="background-color: #e8eef7;">


                    <a href="https://frozenwolf-cyber.github.io/cl-sam.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="width: 23vw; height: 20vw; background-image: url(img/projects/cl-sam/demo.gif);"></div>
                        <article class="article">
                            <h1>Continual Learning for Robust Video Segmentation of Robot-Assisted Surgical Tool </h1>
                            <h5>
                                A continual learning framework using SAM2 and LoRA for robust, privacy-preserving surgical tool segmentation across evolving, distorted endoscopic video domains.   </h5>
                            <span>MICCAI 2025 Workshop - Thesis@NITT</span>
                        </article>
                    </a>

                </div>

                <div class="item-3">


                    <a href="https://frozenwolf-cyber.github.io/mass.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb"
                            style="width: 25vw; background-image: url(img/projects/nus/15-ezgif.com-video-to-gif-converter.gif);">
                        </div>
                        <article class="article">
                            <h1>MASS (My ASSistant)</h1>
                            <h5>
MASS uses an LLM to assist surgeons in automating surgical robots by sequencing actions from simple instructions. The project develops robotic motion planning with LLMs and reinforcement/imitation learning for surgery.
                            </h5>
                            <span> (Submitted) IEEE Robotics and Automation Letters - Laboratory of Medical Mechatronics@National University of Singapore</span>
                        </article>
                    </a>

                </div>


                <div class="item-4" style="background-color: #e8eef7;">


                    <a id="onclick-mimic" href="#mimic" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="width: 23vw; height: 20vw; background-image: url(img/projects/iisc_mimic.gif);"></div>
                        <article class="article">
                            <h1>Medical Multi-Modal Fusion & Cross Modal Alignment </h1>
                            <h5>
                                This project focuses on multi-modal fusion and cross-modal representation alignment with self0supervised learning by integrating EHR, CXR, Notes, and ECG data.
                            </h5>
                            <span>VISTA@Indian Institute of Science</span>
                        </article>
                    </a>

                </div>





                <div class="item-5">

                    <a href="https://frozenwolf-cyber.github.io/sasi.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/cmu.gif);"></div>
                        <article class="article">
                            <h1>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</h1>
                            <h5>This project introduces a SaSi deep learning approach for weak label cryo-ET segmentation in few-shot learning, using Consistency Loss, SimCLR, and AugMix to improve data efficiency and outperform existing methods. </h5>
                            </h5>
                            <span>(Submitted) PLOS Computational Biology  Xu Labs@Carnegie Mellon University</span>
                        </article>
                    </a>

                </div>

                <div class="item-6">
        
                    <a id="onclick-optical" href="#optical" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/opticalflow.png);"></div>
                        <article class="article">
                            <h1>Test-time adaptation for Optical Flow</h1>
                            <h5>Test-time adaptation for optical flow method by using augmentation and pesudo predictions.</h5>
                            </h5>
                            <span>VISTA@Indian Institute of Science</span>
                        </article>
                    </a>


                </div>

                <div class="item-7">


                    <a id="onclick-SAM-PM" href="#SAM-PM" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/assests/cvpr.png);"></div>
                        <article class="article">
                            <h1>SAM-PM: Enhancing Video Camouflaged Object Detection</h1>
                            <h5>SAM-PM is a propagation Module to enhance the SAM’s performance in video camouflage object detec-
tion, achieving substantial improvements with minimal parameter addition (&lt 1M) while performing better than
previous SOTA by 37.9% on mIoU, which got accepted for CVPR 2024 workshops</h5>
                            </h5>
                            <span>CVPR PVUW Workshop</span>
                        </article>
                    </a>



                </div>

                <div class="item-8">


                    <a id="onclick-nllp" href="#nllp" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/assests/emnlp.png);"></div>
                        <article class="article">
                            <h1>Natural Legal Language Processing</h1>
                            <h5>
                                Worked on Natural Legal Language Processing by using LLMs like BERT, XLNET, and Hierarchical Transformers on judiciary datasets.
                                Developed a lightweight CNN model for sentence boundary detection that is 80x faster than traditional statistical models.
                            </h5>
                            <span>EMNLP NLLP Workshop</span>
                        </article>
                    </a>
       
                </div>




                <div class="item-9">
        
                    <a id="onclick-OCR" href="#OCR" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/mtx.jpeg);"></div>
                        <article class="article">
                            <h1>Intelligent OCR</h1>
                            <h5>
                                My OCR integrates CRAFT, Faster R-CNN, Tesseract, and a Siamese network to perform sentence classification and key-value pair detection, including bounding boxes and linked information using PyTorch which is hosted in Azure Cloud. 
                            </h5>
                            <span>NITT</span>
                        </article>
                    </a>
                  


                </div>

                <div class="item-10">
        
                    <a id="onclick-i-Pravesh" href="#i-Pravesh" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb"
                            style=" background-image: url(img/projects/sih.png);">
                        </div>
                        <article class="article">
                            <h1>i-Pravesh</h1>
                            <h5>I-Pravesh is a Smart Attendance Android App which uses a combination of edge face detection and recognition (MobileFaceNet + TensorFlow Lite) as the authentication biometric for recording attendance.</h5>
                            <span>NITT</span>
                        </article>
                    </a>

                  


                </div>

                <div class="item-11">
                 
                        
                    <a id="onclick-summarise" href="#summarise" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/summarise.png);"></div>
                        <article class="article">
                            <h1>SummarizeIQ: An Integrated Summarization and Content Analysis Engine</h1>
                            <h5>This project integrates T5 like LLM summarization models with Torch Serve, enhanced by extractive methods like LexRank and TextRank. It also includes keyword generation and implements query-based content filtering using cosine similarity, offering a comprehensive solution for article summarization and analysis. 
                            </h5>
                            <span>NITT</span>
                        </article>
                    </a>

                </div>


                <div class="item-12">
                 
                        
                    <a id="onclick-realestatevr" href="#realestatevr" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/vr.gif);"></div>
                        <article class="article">
                            <h1>Real Estate VR</h1>
                            <h5>This VR platform leverages Unity’s advanced capabilities to create an immersive and dynamic environment.
                                 The application features precise asset coordination and spatial intelligence for real-time building placement through an intuitive XR Device Simulator, enabling seamless navigation and interaction.
                                   
                                 
                            </h5>
                            <span>NITT</span>
                        </article>
                    </a>

                </div>


                <div class="item-13">
                 
                        
                    <a id="onclick-compiler" href="#compiler" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/compiler.gif);"></div>
                        <article class="article">
                            <h1>Yet Another Python Compiler</h1>
                            <h5>
                                
                                A Python Compiler, built with Bison and Flex, processes Python code by tokenizing it, parsing it into a syntax tree, and performing semantic checks. 
                                It generates intermediate code and applies optimizations like constant folding and dead code elimination. 
                                Key features include lexical analysis, syntax and semantic error detection, and code optimization.

                                 
                            </h5>
                            <span>Compiler Design - CSPC62 @ NITT</span>
                        </article>
                    </a>

                </div>


                <div id="onclick-octtree" class="item-14">
                 
                        
                    <a href="#octtree" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/octtree.gif);"></div>
                        <article class="article">
                            <h1>Oct tree based 3D OpenGL Renderer</h1>
                            <h5>
                                
The Octree-Based 3D OpenGL Renderer, created with PyOpenGL, efficiently renders 3D scenes by using an octree data structure to manage and optimize rendering performance.
                                 
                            </h5>
                            <span>Advanced Data Structures and Algorithms - CSPE43 @ NITT</span>
                        </article>
                    </a>

                </div>
                <div class="item-15">
                 
                        
                    <a id="onclick-sart" href="#sart" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/sart.gif);"></div>
                        <article class="article">
                            <h1>Sartorius Cell Instance Segmentation</h1>
                            <h5>Trained R-CNN, U-Net and Detectron2 using Pytorch to detect and delineate
                                distinct objects of interest in biological images depicting neuronal cell types commonly used in studying neurological disorders.
                            </h5>
                                <span>NITT</span>
                        </article>
                    </a>

                </div>

                <div class="item-16">
                 
                        
                    <a id="onclick-others" href="#others" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/others.gif);"></div>
                        <article class="article">
                            <h1>Other Projects</h1>
                            <h5>
                                Projects include: <br>

                                Brain tumor classification <br>
                                Pneumonia (COVID-19) chest X-ray classification <br>
                                Neural style transfer <br>
                                Crack detection with TF Lite <br>
                                Face detection using a Siamese Neural Network model to develop a one-shot neural network with FaceNet and MTCNN as the backbone
                                Unity based C# game called "The Keres" 
                            </h5>
                                <span>NITT</span>
                        </article>
                    </a>

                </div>


          

            </div>
        </div>
    </section>



    <div class="portfolio-modal modal fade" id="cl-sam" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Continual Learning for Robust Video Segmentation of Robot-Assisted Surgical Tool</h2>
                            <br>
                            <div class="row">
                                <div class="col-md-8 col-md-offset-2">
                              
                                  <!-- Optional video block -->
                                  <!-- 
                                  <p style="text-align:center;">
                                    <video id="v0" width="100%" playsinline autoplay muted loop controls>
                                      <source src="img/demo_sequence_compressed.mp4" type="video/mp4">
                                    </video>
                                  </h5>
                                  -->
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/2.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h3>Abstract</h3>
                              
                                  <p class="text-justify">
                                    Robust identification and segmentation of surgical tools in robot-assisted procedures are critical for safeguarding patient safety and driving progress toward fully automated surgeries. However, deep learning models often fail to handle challenging visual conditions of real-world surgical scenes, including distortions like over-bleeding, smoke, and low illumination. Conventional training on limited, high-quality data makes adaptation to diverse surgical conditions difficult, with standard approaches often causing catastrophic forgetting and violating privacy due to the need for sensitive historical data.
                                  </h5>
                                  <p class="text-justify">
                                    We propose a new framework that unifies Domain-Incremental Continual Learning (CL) to achieve robust surgical tool segmentation while preserving data privacy across evolving domains. Our approach utilizes the Segment Anything Model 2 (SAM2) as a baseline, employing parameter-efficient Low-Rank Adaptation (LoRA) to facilitate adaptation within the CL framework. This enables sequential, privacy-preserving learning across domains and mitigates forgetting.
                                  </h5>
                                  <p class="text-justify">
                                    We evaluate our methodology on challenging endoscopic video datasets containing various distortions, measuring performance using standard segmentation metrics (mIoU, DSC) and assessing knowledge retention via forgetting analysis. Our results demonstrate that the K-Means based CL method achieves high performance across several learned domains, presenting a significant advancement towards reliable, adaptable, and privacy-conscious computer vision systems for real-world surgical applications.
                                  </h5>
                              
                                </div>
                              </div>
                              
                              <p style="text-align:center;">
                                <img src="img/projects/cl-sam/7.png" class="img-responsive" style="max-width: 50%; transform: scale(1); transform-origin: center;">
                              </h5>
                              
                              <p style="text-align:center;">
                                <img src="img/projects/cl-sam/3.png" class="img-responsive" style="max-width: 50%; transform: scale(1); transform-origin: center;">
                              </h5>
                              
                              <div class="row">
                                <div class="col-md-8 col-md-offset-2">
                                  <br>
                                  <h3>Methodology</h3>
                              
                                  <h4>Domain Identification and Adaptive LoRA Loading (K-Means + CLIP)</h4>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/4.png" class="img-responsive" style="max-width: 70%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h5>
                                    Instead of adapting a single set of parameters over time, this method trains <strong>individual LoRA adapters</strong> for each domain encountered during the learning phase. During inference, the <strong>domain of the input video</strong> is first identified using <strong>CLIP embeddings and K-Means clustering</strong>, and the corresponding adapter is loaded into the base segmentation model for inference.
                                  </h5>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/8.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h5>Training Phase:</h5>
                                  <ul>
                                    <li>Each domain gets a dedicated LoRA adapter trained independently.</li>
                                    <li>Initialization can either be random or derived from the previous domain's adapter.</li>
                                    <li>The segmentation loss is optimized per domain.</li>
                                    <li>Optional <strong>knowledge distillation</strong> uses the previous domain’s adapter as a teacher.</li>
                                    <li>Each trained adapter is stored separately, preserving domain-specific expertise.</li>
                                  </ul>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/9.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h5>Inference Phase:</h5>
                                  <ul>
                                    <li>Frame-level embeddings are extracted using a <strong>pre-trained CLIP model</strong>.</li>
                                    <li>Embeddings are averaged per video and clustered to compute <strong>domain anchor embeddings</strong>.</li>
                                    <li>A test sequence’s embedding is matched to the closest anchor.</li>
                                    <li>The <strong>corresponding LoRA adapter</strong> is dynamically loaded into the model.</li>
                                  </ul>
                              
                                  <h5>
                                    This approach transforms the challenge of continual learning into a <strong>domain recognition and parameter selection problem</strong>, offering robustness to domain shifts and avoiding typical drawbacks of shared-parameter models.
                                  </h5>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/5.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h3>Observations</h3>
                              
                                  <h4>Continual Learning Strategy Evaluation and Forgetting Analysis</h4>
                              
                                  <h5>
                                    The core challenge in continual learning is <strong>catastrophic forgetting</strong>, where the model degrades on earlier domains after learning new ones. To assess this, performance metrics like <strong>mean Dice Similarity Coefficient (DSC)</strong> and <strong>IoU</strong> were tracked across domains as new ones were added in sequence:
                                  </h5>
                                  <h5><em>Smoke → Blood → Low Brightness → Background Change → Regular</em></h5>
                              
                                  <p style="text-align:center;">
                                    <img src="img/projects/cl-sam/6.png" class="img-responsive" style="max-width: 100%; transform: scale(1); transform-origin: center;">
                                  </h5>
                              
                                  <h5>Key Findings:</h5>
                              
                                  <h6>1. Naive Sequential Fine-tuning</h6>
                                  <ul>
                                    <li>Exhibited <strong>significant forgetting</strong>, especially on early domains.</li>
                                    <li>Example: Performance on Smoke dropped by <strong>-0.27</strong> after Background Change training.</li>
                                    <li>Adapts well to recent domains but often <strong>overfits</strong>, hurting generalization.</li>
                                    <li>Shows minimal forward knowledge transfer to new tasks.</li>
                                  </ul>
                              
                                  <h6>2. Learning without Forgetting (LwF)</h6>
                                  <ul>
                                    <li>Demonstrated <strong>better retention</strong> than the Naive baseline.</li>
                                    <li>Example: Forgetting on Smoke was reduced to <strong>-0.106</strong>.</li>
                                    <li>Uses distillation to maintain old knowledge but may slow new domain adaptation.</li>
                                  </ul>
                              
                                  <h6>3. K-Means + CLIP Adapter Selection (KM)</h6>
                                  <ul>
                                    <li>Showed <strong>the least forgetting</strong>, with scores often near zero or slightly positive.</li>
                                    <li>Example: <strong>+0.021 on Smoke</strong> after Low Brightness training.</li>
                                    <li>Uses <strong>separate adapters</strong> for each domain, preventing knowledge overwriting.</li>
                                    <li>Exhibits better <strong>forward generalization</strong> due to CLIP-guided adapter selection.</li>
                                  </ul>
                              
                                  <h5>
                                    This strategy uses <strong>domain-aware inference</strong> to maintain high performance across tasks and avoids the instability of sequential fine-tuning approaches.
                                  </h5>
                              
                                </div>
                              </div>
                              
                              <br>
                              
                              <h3>WanDB Report</h3>
                              <br>
                              <iframe src="https://wandb.ai/frozenwolf/CL-SAM2/reports/CL-for-Robust-Video-Segmentation-of-Surgical-Tool--VmlldzoxMjkwMTg5OQ" style="border:none;height:1024px;width:100%"></iframe>
                              

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>




    <div class="portfolio-modal modal fade" id="cl-asr" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>A Study on Regularization-Based Continual Learning Methods for Indic ASR</h2>
                            <br>
                            
                            <div class="row">
                                <div class="col-md-8 col-md-offset-2">
                                    <h3>Abstract</h3>
                                    <p class="text-justify">
                                        India's linguistic diversity challenges inclusive Automatic Speech Recognition (ASR) system development. Traditional multilingual models, requiring simultaneous access to all language data, are impractical due to sequential data arrival and privacy constraints.
                                    </h5>
                                    <p class="text-justify">
                                        Continual Learning (CL) enables models to learn new languages sequentially without catastrophically forgetting prior knowledge. This paper investigates CL for ASR on Indian languages using the subset of the <em>indicSUPERB</em> benchmark.
                                    </h5>
                                    <p class="text-justify">
                                        We employ a Conformer-based hybrid RNN-T/CTC model, initially pretrained on Hindi, which is subsequently trained incrementally on eight additional Indian languages, for a sequence of nine languages in total.
                                    </h5>
                                    <p class="text-justify">
                                        We evaluate three prominent regularization and distillation-based CL strategies: Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without Forgetting (LwF), chosen for their suitability in no-replay, privacy-conscious scenarios.
                                    </h5>
                                    <p class="text-justify">
                                        Performance is analyzed using Word Error Rate (WER) for both RNN-T and CTC paths on clean/noisy data, and knowledge retention via Backward Transfer. We explore varying training epochs (1, 2, 5 and 10) per task.
                                    </h5>
                                    <p class="text-justify">
                                        Results, compared against naive fine-tuning, demonstrate CL's efficacy in mitigating forgetting for scalable ASR in diverse Indian languages under realistic constraints.
                                    </h5>
                                </div>
                            </div>
                            
                            <div class="row">
                                <div class="col-md-8 col-md-offset-2">
                                    <br>
                                    <h3>Observations</h3>
                            
                                    <h4>CTC Benchmarking</h4>
                                    <img src="img/projects/indiccl/1.png" class="img-responsive" style="max-width: 100%;">
                                    <p class="text-justify">
                                        As shown in Figure 1, the average WER across tasks reveals a clear ranking among methods. <strong>LwF</strong> achieves the best overall performance, followed by <strong>EWC</strong>, then <strong>MAS</strong>, with naive fine-tuning performing the worst. This ranking is particularly evident in short and medium task horizons. For longer sequences, however, the performance gap between methods narrows considerably. Naive fine-tuning, in particular, produces the highest WER maxima across tasks. When analyzing backward transfer (BWT), <strong>MAS</strong> performs best in short sequences, while <strong>LwF</strong> excels in medium-length tasks. For longer sequences, both <strong>MAS</strong> and <strong>LwF</strong> converge to similar average BWT values, whereas <strong>EWC</strong> and naive fine-tuning fall behind.
                                    </h5>
                            
                                    <h4>RNN-T Benchmarking</h4>
                                    <img src="img/projects/indiccl/9.png" class="img-responsive" style="max-width: 100%;">
                                    <p class="text-justify">
                                        Figure 9 shows that RNN-T consistently outperforms CTC in WER across all continual learning strategies. Among these, <strong>EWC</strong> achieves the lowest WER across task lengths, demonstrating strong performance retention on the current task. However, this benefit comes at a cost: <strong>EWC</strong> exhibits the worst BWT of all methods, even lower than that of naive fine-tuning, indicating substantial forgetting. <strong>MAS</strong> shows some improvement in BWT for medium-length sequences, but for longer horizons, BWT scores deteriorate across all methods except <strong>EWC</strong>, eventually becoming nearly indistinguishable.
                                    </h5>
                            
                                    <h4>General Comparison of CL Methods under Noisy Settings</h4>
                                    <img src="img/projects/indiccl/2.png" class="img-responsive" style="max-width: 100%;">
                                    <p class="text-justify">
                                        In noisy conditions (Figure 2), both <strong>LwF</strong> and <strong>MAS</strong> outperform <strong>EWC</strong> and the naive baseline in BWT, suggesting better retention of prior knowledge. Interestingly, noise appears to improve backward transfer, likely due to regularization effects. However, this improvement comes with a trade-off: WER increases, and models perform better on clean audio in absolute terms. This contrast indicates that noise can enhance stability, by reducing forgetting, while simultaneously impairing plasticity, by diminishing learning precision, which is reflected in the higher WER.
                                    </h5>
                            
                                    <img src="img/projects/indiccl/4.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/3.png" class="img-responsive" style="max-width: 100%;">
                            
                                    <h4>WER Performance Analysis</h4>
                                    <p class="text-justify">
                                        Figures 3 and 4 present WER trends over increasing task lengths. Evaluations are averaged over the last two and current tasks, categorized as short (1–3), medium (1–6), and long (1–9). In general, models perform better with clean data. Among the methods, <strong>LwF</strong> consistently maintains WER below 1.0, with high stability indicated by narrow shaded variance regions.
                                    </h5>
                                    <p class="text-justify">
                                        Interestingly, the upper bounds of noisy WER for <strong>LwF</strong> are comparable to the maxima seen under clean conditions. This can be attributed to its distillation-based loss, which prevents overfitting to noisy inputs by anchoring the model to previous predictions. <strong>MAS</strong> follows a similar pattern, though with slightly lower stability. <strong>EWC</strong> occasionally achieves better minimum WERs, particularly for short tasks, but continues to show poor BWT. The naive method performs surprisingly well in short sequences but fails to retain knowledge over longer horizons. Overall, <strong>LwF</strong> demonstrates the effectiveness of knowledge distillation in maintaining a balance between acquiring new knowledge and retaining previous learning. For longer sequences, average WER tends to decline, possibly due to simpler language characteristics in later tasks.
                                    </h5>
                            
                                    <h4>EWC Ablation Studies</h4>
                                    <img src="img/projects/indiccl/8.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/14.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/11.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/5.png" class="img-responsive" style="max-width: 100%;">
                                    <p class="text-justify">
                                        In Figure 5, we examine the impact of different regularization strengths in <strong>EWC</strong> by testing λ<sub>EWC</sub> ∈ {5, 10}. While both values yield similar outcomes, λ<sub>EWC</sub> = 10 leads to slightly better WER in medium and long tasks, though the benefit is minimal in short tasks. BWT trends (Figure 8) for both values remain close to those of the naive baseline, suggesting limited ability to retain performance on earlier tasks. Additionally, results from epoch-wise ablation (Figure 11) show that increasing training epochs reduces WER, with the best results achieved at epoch 10. However, BWT steadily declines with more epochs (Figure 14), confirming the stability-plasticity trade-off: improved learning on new tasks often leads to increased forgetting of previous ones.
                                    </h5>
                            
                                    <img src="img/projects/indiccl/6.png" class="img-responsive" style="max-width: 100%;">
                            
                                    <h4>LwF Ablation Studies</h4>
                                    <p class="text-justify">
                                        As shown in Figure 6, adjusting the distillation weight α<sub>KD</sub> significantly impacts <strong>LwF</strong>’s performance. A higher value of 0.5 severely limits the model’s ability to learn new tasks, resulting in WERs close to 1.0 across all horizons—worse than naive fine-tuning for short sequences. In contrast, α<sub>KD</sub> = 0.1 strikes a better balance, achieving WER comparable to or better than naive fine-tuning while maintaining much stronger BWT. As shown in Figure 8, the 0.5 configuration yields the highest BWT, primarily because the model barely updates and effectively freezes previous knowledge. The 0.1 setting enables more meaningful learning while controlling forgetting.
                                    </h5>
                                    <p class="text-justify">
                                        Epoch-wise trends (Figures 10 and 14) are consistent with those observed in <strong>EWC</strong>. Increasing the epochs improves WER but worsens BWT.
                                    </h5>
                            
                                    <img src="img/projects/indiccl/7.png" class="img-responsive" style="max-width: 100%;">
                            
                                    <h4>MAS Ablation Studies</h4>
                                    <p class="text-justify">
                                        In Figure 7, we compare <strong>MAS</strong> with regularization weights α<sub>ctx</sub> of 0.3 and 1.0. The stronger setting of 1.0 consistently achieves better WER and shows more stable variance across tasks. Its shaded performance region closely overlaps with that of naive fine-tuning, though with lower dispersion. When examining BWT (Figure 8), the 0.3 configuration performs better, matching <strong>LwF</strong> in retaining knowledge.
                                    </h5>
                                    <p class="text-justify">
                                        As with the other methods, <strong>MAS</strong> exhibits the stability-plasticity trade-off: increasing epochs (Figure 12) lowers WER but leads to worsening BWT (Figure 14). This consistent trend across methods emphasizes the fundamental challenge in continual learning of effectively balancing the acquisition of new information with the retention of existing knowledge.
                                    </h5>
                            
                                    <img src="img/projects/indiccl/12.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/13.png" class="img-responsive" style="max-width: 100%;">
                                    <img src="img/projects/indiccl/10.png" class="img-responsive" style="max-width: 100%;">
                                </div>
                            </div>
                            
                            <br>
                            <h3>WanDB Report</h3>
                            <br>
                            <iframe src="https://wandb.ai/frozenwolf/CL-ASR/reports/Regularization-Based-CL-Methods-for-Indic-ASR--VmlldzoxMjg5OTAwNw"
                                style="border:none;height:1024px;width:100%"></iframe>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>






    <div class="portfolio-modal modal fade" id="mimic" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Medical Multi-Modal Fusion & Cross Modal Alignment</h2>
                            <br>
                            
                                <img src="img/projects/iisc_mimic.gif" class="img-responsive img-centered" alt="">
                            
                                <div class="publ_">
                                    <ul style="list-style-type: none; padding: 0;">
                                        <li style="margin-bottom: 10px;">
                                            <a href="https://docs.google.com/presentation/d/1CU_Gzy2BEQ5bYzLrQQq5W4BrMf7QgzsWftuQOCUiqvE/edit?usp=sharing" target="_blank" class="button google-slides">
                                                <i class="fa fa-google icon"></i>
                                                <b>Slides</b>
                                            </a>
                                        </li>
                                         <li style="margin-bottom: 10px;">
                                             <a href="https://oceanic-antler-eb0.notion.site/1c6ae01d802f44d4aa1c4bafe6f3d4a2?v=0198cff002b34f94ac6bce016294b0ad"  target="_blank" class="button notion">
                                                 <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                                 <b>Notion</b>
                                             </a>
                                         </li>
                                         <li>
                                             <a href="https://oceanic-antler-eb0.notion.site/SOTA-models-963b2664d0894f6bb0f09d6a12242704?pvs=4" target="_blank" class="button notion">
                                                 <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                                 <b>Notion (SOTA)</b>
                                             </a>
                                         </li>
                                            
                                         <li style="margin-bottom: 10px;">
                                            <a href="https://drive.google.com/file/d/17V-IovP9YHo6Lb0jlTgjNYpH9m7nXoAm/view?usp=sharing" target="_blank" class="button google-slides">
                                                <i class="fa fa-google icon"></i>
                                                <b>Report</b>
                                            </a>
                                        </li>
                                         
                                    </div>
                                    <h4> Summer internship report </h4>
                                    <details open>
                                    
                                    <iframe src="img/projects/mimic.pdf" width="100%" height="600px"></iframe>
                                    <br>
                                    </details>
                                <strong>Dataset Analysis and Performance Report</strong><br><br>

                                <h6>This project focuses on analyzing and improving the performance of models using a multimodal dataset comprising Electronic Health Records (EHR) data, including demographic information, ICU vitals, Chest X-rays (CXR), ECG signals, and clinical notes. The primary tasks include predicting readmission, mortality, and length of stay (LOS) for patients based on data available within 24 to 48 hours after admission.<br><br>
                                </h6>
                                <h5><strong>Modalities and Tasks</strong></h5>
                                <ul>
                                    <li><strong>EHR Demographics:</strong> <h6>Includes categorical data like race and age, with age discretized into groups due to deidentification.</h6></li>
                                    <li><strong>ICU Vitals:</strong> <h6>A selection of 39 vital signs, time series data, and categorical events like procedures.</h6></li>
                                    <li><strong>CXR:</strong> <h6>Time series of chest X-ray images.</h6></li>
                                    <li><strong>ECG:</strong> <h6>12-lead ECG signals represented as time series.</h6></li>
                                    <li><strong>Clinical Notes:</strong> <h6>Series of notes, such as discharge and radiology notes.</h6></li>
                                </ul>
                                
                                <h6>The tasks involve:</h6>
                                <ol>
                                    <li><strong>Readmission Prediction:</strong> <h6>Whether a patient will be readmitted within a month after discharge.</h6></li>
                                    <li><strong>Mortality Prediction:</strong> <h6>Predicting if the patient will survive or die.</h6></li>
                                    <li><strong>Length of Stay Prediction:</strong> <h6>Whether the stay will exceed 3 or 7 days.</h6></li>
                                </ol>
                                
                                <h5><strong>Dataset and Class Imbalance</strong></h5>
                                <ul>
                                    <li><h6>The dataset shows heavy class imbalance across all tasks and modalities. This imbalance persists even when considering the intersection of multiple modalities, leading to a significant portion of the data being unused.</h6></li>
                                    <li><h6>The dataset was split into training, validation, and test sets with an approximately 80%-10%-10% split, maintaining the proportions of each modality combination and balancing the classes.</h6></li>
                                </ul>
                                
                                <h5><strong>Experimentation Setup</strong></h5>
                                <ul>
                                    <li><strong>Data Pipeline:</strong> <h6>A heavily modified HADM pipeline handles data loading and interlinked modality fusion. PyTorch Lightning with Distributed Data Parallel (DDP) is used for training, with configurations managed via YAML files.</h6></li>
                                    <li><strong>Metrics:</strong> <h6>Evaluation metrics include binary classification metrics monitored across train, validation, and test sets. Performance is also assessed at the modality and modality combination levels.</h6></li>
                                    <li><strong>Model Structure:</strong> <h6>The model pipeline includes a modality encoder, a time series encoder, and a binary classification head. Class imbalance is addressed through oversampling and undersampling strategies, depending on the skewness of the class distribution.</h6></li>
                                </ul>
                                
                                <h5><strong>Observations from Literature</strong></h5>
                                <ul>
                                    <li><h6>Cross-modal alignment is rarely performed or analyzed extensively in current research, and ECG data is often excluded or underutilized in multimodal tasks. Reported F1 scores are typically below 0.7, highlighting the challenge in improving model performance.</h6></li>
                                </ul>
                                
                                <h5><strong>Experimentation Results</strong><br></h5>
                                <h5>4.2. Multi-Modal Learning Framework</h5>
                                
                                <h5>1. EHR:</h5>
                                
                                <h5>a. GRU-D vs RNN:</h5>
                                <h5>Table 1: GRU-D vs RNN F1-score comparison.</h5>
                                <table>
                                <tr><th>MODEL</th><th>TEST F1 SCORE</th></tr>
                                <tr><td>GRU-D</td><td>0.6625</td></tr>
                                <tr><td>RNN</td><td>0.71844</td></tr>
                                </table>
                                <h5>RNN demonstrates superior performance compared to GRU-D, both in terms of F1
                                    score and training time. RNN achieved a higher F1 score (0.71844 vs. 0.6625) and
                                    was more efficient, taking almost 40% less time to train. This indicates that the RNN
                                    model is both faster and more effective for EHR data</h5>
                                
                                <h5>b. RNN vs RNN-TS:</h5>
                                <h5>Table 2: RNN vs RNN-TS F1-score comparison.</h5>
                                <table>
                                <tr><th>MODEL</th><th>TEST F1 SCORE</th></tr>
                                <tr><td>RNN</td><td>0.71844</td></tr>
                                <tr><td>RNN-TS</td><td>0.71549</td></tr>
                                </table>
                                <h5>RNN-TS (with time series data) performed almost similarly to the standard RNN (F1
                                    score of 0.71549 vs. 0.71844). This suggests that incorporating time series data as
                                    positional embeddings does not significantly enhance the model's performance and
                                    may even slightly degrade it.</h5>
                                
                                <h5>c. RNN Class Balance vs. Without Class Balance:</h5>
                                <h5>Table 3: RNN Class Balance comparison.</h5>
                                <table>
                                <tr><th>MODEL</th><th>TEST F1 SCORE</th></tr>
                                <tr><td>RNN (NO CLASS BALANCE)</td><td>0.71288</td></tr>
                                <tr><td>RNN (WITH CLASS BALANCE)</td><td>0.71844</td></tr>
                                </table>
                                <h5>
                                    Class balancing provided a marginal improvement in performance, boosting the F1 score from 0.71288 to 0.71844. This suggests that while class balancing helps, its
impact is not substantial for the EHR dataset.
                                </h5>
                                <h5>d. Training Time:</h5>
                                <h5>Table 4: GRU-D vs RNN-TS training time.</h5>
                                <table>
                                <tr><th>MODEL</th><th>TRAINING TIME (SECONDS)</th></tr>
                                <tr><td>GRU-D</td><td>135,230</td></tr>
                                <tr><td>RNN-TS</td><td>82,968</td></tr>
                                </table>
                                <h5>RNN-TS is significantly faster to train compared to GRU-D, indicating that the
                                    simpler RNN architecture is more efficient in this context, potentially due to the
                                    reduced complexity in handling time-series data compared to GRU-D.</h5>
                                
                                <h5>2. CXR:</h5>
                                <h5>Table 5: Class balance and positional embedding comparison.</h5>
                                <table>
                                <tr><th>CONFIGURATION</th><th>TEST F1 SCORE</th></tr>
                                <tr><td>No class balance and positional embedding</td><td>0.56609</td></tr>
                                <tr><td>With class balance and positional embedding</td><td>0.66634</td></tr>
                                <tr><td>With class balance but no positional embedding</td><td>0.62633</td></tr>
                                </table>
                                <h5>Class balancing improved the model’s performance significantly, increasing the F1
                                    score from 0.56609 to 0.66634. Additionally, adding positional embeddings further
                                    enhanced the performance (F1 score: 0.66634 vs. 0.62633), suggesting that positional
                                    embeddings in CXR data play a role in improving the model's capacity to capture
                                    temporal information.</h5>
                                
                                <h5>3. Notes:</h5>
                                <h5>Table 6: Frozen vs unfrozen configurations.</h5>
                                <table>
                                <tr><th>CONFIGURATION</th><th>TEST F1 SCORE</th></tr>
                                <tr><td>No class balance (Frozen)</td><td>0.12199</td></tr>
                                <tr><td>Frozen (With class balance)</td><td>0.5004</td></tr>
                                <tr><td>Frozen (No positional embedding)</td><td>0.24544</td></tr>
                                <tr><td>Unfrozen (No class balance)</td><td>0.66085</td></tr>
                                <tr><td>Unfrozen (With class balance)</td><td>0.63192</td></tr>
                                </table>
                                <h5>Unfreezing the notes encoder and allowing the model to fine-tune the notes
                                    embeddings significantly improved performance, with the F1 score rising to 0.66085
                                    (without class balance). Class balancing marginally reduced performance in the
                                    unfrozen model (0.63192). Frozen models performed poorly, highlighting the
                                    importance of fine-tuning. Additionally, positional embeddings play a significant role,
                                    as removing them drastically dropped the F1 score from 0.5004 to 0.24544.</h5>
                                
                                <h5>4. ECG:</h5>
                                <h5>Table 7: Class balance impact on ECG performance.</h5>
                                <table>
                                <tr><th>CONFIGURATION</th><th>TEST F1 SCORE</th><th>AUROC</th></tr>
                                <tr><td>Without class balance</td><td>0.0</td><td>0.5197</td></tr>
                                <tr><td>With class balance</td><td>0.6236</td><td>0.52089</td></tr>
                                </table>
                                <h5>Without class balancing, the ECG model failed to predict more than one class,
                                    resulting in an F1 score of 0. However, class balancing greatly improved the
                                    performance, yielding an F1 score of 0.6236. This demonstrates that class imbalance
                                    severely impacts the ECG modality, and balancing is crucial for achieving meaningful
                                    predictions. Additionally, this highlights that AUROC is not a reliable metric in
                                    skewed predictions, as the AUROC for the unbalanced ECG model was still
                                    comparable despite the F1 score being 0.</h5>
                                
                                <h5>5. Total Comparison:</h5>
                                <h5>Table 8: Summary across modalities.</h5>
                                <table>
                                <tr><th>MODALITY</th><th>TEST F1 SCORE</th><th>AUROC SCORE</th></tr>
                                <tr><td>Notes</td><td>0.66085</td><td>0.62601</td></tr>
                                <tr><td>ECG</td><td>0.6236</td><td>0.52089</td></tr>
                                <tr><td>RNN-TS (EHR)</td><td>0.71549</td><td>0.77536</td></tr>
                                <tr><td>CXR</td><td>0.66634</td><td>0.52081</td></tr>
                                </table>
                                <h5>Among the single modalities, the EHR data using RNN-TS performed the best, with
                                    an F1 score of 0.71549 and AUROC of 0.77536. CXR also performed well, with an
                                    F1 score of 0.66634. Notes and ECG followed with slightly lower scores. The
                                    AUROC scores indicate that while F1 remains the primary measure of interest,
                                    AUROC is not as reliable in imbalanced datasets like ECG and CXR.</h5>
                                <h5>6. Multimodality:</h5>
                                <h5>Table 9: Fusion method comparison.</h5>
                                <table>
                                <tr><th>FUSION METHOD</th><th>F1 SCORE</th></tr>
                                <tr><td>Sum Fusion of CXR + EHR</td><td>0.69789</td></tr>
                                <tr><td>Transformer Fusion of CXR + EHR</td><td>0.70182</td></tr>
                                <tr><td>Sum Fusion of CXR + EHR + Notes</td><td>0.45058</td></tr>
                                <tr><td>Transformer Fusion of CXR + EHR + Notes</td><td>0.66817</td></tr>
                                <tr><td>Sum Fusion of CXR + ECG + EHR + Notes</td><td>0.51967</td></tr>
                                <tr><td>Transformer Fusion of CXR + ECG + EHR + Notes</td><td>0.61993</td></tr>
                                </table>
                                <h5>The transformer-based fusion consistently outperforms the sum fusion method across
                                    all modality combinations. For instance, Transformer CXR + EHR (F1 score:
                                    0.70182) slightly outperforms Sum CXR + EHR (F1 score: 0.69789). Similarly,
                                    Transformer CXR + ECG + EHR + Notes (F1 score: 0.61993) shows significant
                                    improvement over the sum fusion of the same modalities (F1 score: 0.51967). This
                                    indicates that transformer-based fusion is more effective in learning from the complex interactions between modalities.</h5>
                                <h5>Table 10: Fusion vs. single modality baselines.</h5>
                                <table>
                                <tr><th>FUSION/MODALITY</th><th>F1 SCORE</th></tr>
                                <tr><td>Transformer CXR + ECG + EHR + Notes</td><td>0.61993</td></tr>
                                <tr><td>Transformer CXR + Notes + ECG</td><td>0.55538</td></tr>
                                <tr><td>Transformer CXR + EHR + ECG</td><td>0.45439</td></tr>
                                <tr><td>Transformer CXR + EHR + Notes</td><td>0.66817</td></tr>
                                <tr><td>Transformer CXR + Notes</td><td>0.64509</td></tr>
                                <tr><td>Transformer Notes + ECG</td><td>0.58547</td></tr>
                                <tr><td><strong>SINGLE MODALITY BASELINE</strong></td><td></td></tr>
                                <tr><td>EHR</td><td>0.71549</td></tr>
                                <tr><td>CXR</td><td>0.66634</td></tr>
                                <tr><td>Notes</td><td>0.66085</td></tr>
                                <tr><td>ECG</td><td>0.62360</td></tr>
                                </table>
                                
                                <h6>Although multimodal fusion using transformers improves performance in some cases,
                                    adding more modalities does not always result in better outcomes. For example,
                                    Transformer CXR + ECG + EHR + Notes (F1 score: 0.61993) performed worse
                                    than EHR alone (F1 score: 0.71549). This suggests that poorly aligned modality
                                    combinations can degrade performance. The best-performing multimodal models
                                    should ideally surpass the highest-performing single-modality model. However, this is
                                    not consistently observed, indicating that proper modality alignment and fusion are
                                    critical for achieving optimal performance.
                                    Transformer-based fusion provides better results compared to sum fusion in
                                    multimodal learning, especially when integrating complex and diverse modalities such
                                    as CXR, ECG, EHR, and Notes. However, simply combining modalities does not
                                    guarantee improved performance; well-aligned models must be carefully designed to
                                    achieve results that exceed the best-performing individual modalities. Ideally, the
                                    performance of a multimodal model should be greater than the maximum F1 score of
                                    the individual modalities (e.g., EHR or CXR alone).</h6>
                                
                      
                                
              

                            </h6>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://api.wandb.ai/links/tirthajitb-indian-institute-of-science/s7ncw5hy" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="nus" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Surgical Motion Planning</h2>
                            <br>
                            <h5>
                                <img src="img/projects/nus/surgicalcommand2motion-Page-1.drawio.png" class="img-responsive img-centered" alt="">
                            
                                <h5><strong>Project Summary:</strong></h5>

                                <h5>The project focuses on the development of an embodied robotic agent designed to execute high-level natural language surgical instructions. This system leverages a combination of Reinforcement Learning (RL), Imitation Learning, and a pre-trained Large Language Model (LLM) to function as a "planner." The agent can perform short-horizon skills across multiple surgical scenarios using a library of pre-trained policies. These policies are selected based on the scenario, with skills that are common across scenarios mapped under common primitives with textual labels.</h5>
                                
                                <h5><strong>Methodology:</strong></h5>
                                
                                <h5><strong>1. Planning and Filtering:</strong> The LLM is responsible for planning by determining the sequence of actions required to execute a given surgical instruction. A filtering process similar to SayCAN is used, which ranks skills based on both logical reasoning and geometric feasibility. The LLM generates possible action permutations, which are then filtered to a manageable number using a Chain-of-Thought (CoT) process, reducing the likelihood of errors and improving interpretability.</h5>
                                
                                <h5><strong>2. Action Scoring:</strong> Each action is scored based on two criteria: the LLM's planning score and the RL policy’s affordance score, which reflects the geometric feasibility of the action. The best action is selected based on these combined scores, with a fallback to the highest LLM score if all actions are deemed infeasible.</h5>
                                
                                <h5><strong>3. Execution:</strong> Once an action is selected, a human-in-the-loop is given the option to approve or modify the action before execution. The agent updates the scene and action status after each step, which informs subsequent actions.</h5>
                                
                                <h5><strong>4. Experimental Setups:</strong> The system is tested in two main environments: Surrol and Lapgym, each with specific tasks like retracting tissue, reaching a tumor, and picking and placing gauze. The experiments explore various strategies, including closed-loop, open-loop, and inner monologue setups, with scene and action updates. The DINO model is utilized for detecting objects in the surgical environment, such as gauze and blood, by generating bounding boxes around these targets based on textual inputs provided by the LLM. This integration allows for accurate, real-time detection of key elements, which is critical for the success of surgical tasks.</h5>
                                
                                <h5><strong>Key Features:</strong></h5>
                                
                                <h5><strong>Human-in-the-loop Feedback:</strong> Ensures control over the actions performed, allowing for adjustments based on the requirements.</h5>
                                
                                <h5><strong>Chain-of-Thought (CoT):</strong> Improves planning efficiency and makes the process more interpretable for surgical tasks.</h5>
                                
                                <h5><strong>Scene Description Generation:</strong> Simplifies the scene context for the LLM, making the planning process more effective and relatable to real-world applications.</h5>
                                
                                <h5>The project demonstrates the feasibility of integrating LLMs with RL, Imitation Learning, and DINO for complex surgical tasks, providing a robust foundation for further development in autonomous surgical systems.</h5>
                                

                            
                            </h5>
                            <br>
                            <h3> Demo </h3>
                            <img src="img/projects/nus/15-ezgif.com-video-to-gif-converter.gif" class="img-responsive img-centered" alt="">
                            <img src="img/projects/nus/ezgif-2-584f48650a.gif" class="img-responsive img-centered" alt="">
                            

                            <br>
                         
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="cmu" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</h2>
                            <br>
                            <h5>
                                <img src="img/projects/cmu.png" class="img-responsive img-centered" alt="">
                            
                                <h5><strong>Description:</strong></h5>
                                <h5>This project tackles the critical task of particle localization and classification in cryo-electron tomography (cryo-ET), focusing on real-world challenges such as limited and weakly annotated data. The study is conducted on both private datasets and the SHREC 2021 simulated dataset, aiming to establish robust baselines and explore advanced techniques to improve model performance. The goal is to enhance the accuracy and robustness of particle detection and classification, making it more effective for structural biology applications.</h5>
                                
                                <h5><strong>Methodology:</strong></h5>
                                
                                <h5><strong>1. Supervised Learning:</strong> The approach begins by generating <strong>pseudo-strong labels</strong> from weak labels, converting them into segmentation masks by drawing spheres of minimum radius around the particle centroids. These masks serve as ground truth for training. The tomograms are divided into smaller subtomograms, which are fed into the model to generate voxel-level probabilities for each particle type. Key techniques include:</h5>
                                <ul>
                                    <li><strong>Balanced Sampling and Window Method:</strong> Ensuring that training data is evenly sampled and that particle centroids are accurately localized within smaller windows.</li>
                                    <li><strong>AugMix Modification:</strong> A tailored version of AugMix is applied to improve data augmentation, focusing on enhancing the diversity and robustness of the training samples.</li>
                                </ul>
                                
                                <h5><strong>2. Self-Supervised Learning:</strong> To further improve the model's performance, <strong>SimCLR</strong> is employed. This technique leverages contrastive learning to learn representations that are invariant to data augmentation, improving the model’s ability to generalize from few-shot data.</h5>
                                
                                <h5><strong>3. Post-Processing:</strong> After initial predictions, advanced post-processing techniques are applied to refine the results:</h5>
                                <ul>
                                    <li><strong>Connected Components Analysis:</strong> Used to identify and isolate individual particles from the segmentation masks, improving the accuracy of particle localization.</li>
                                    <li><strong>MeanShift Clustering:</strong> Applied to group detected particles based on their spatial coordinates, helping to better classify and localize particles within the tomogram.</li>
                                </ul>
                       
                            </h5>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://wandb.ai/frozenwolf/semi-self%20supervised%20fewshot/reports/Enhancing-Few-Shot-Detection-with-Weak-Labels-A-Self-Supervised-Learning-and-Augmix-Approach-in-Cryo-ET--Vmlldzo5MTE3MDk4" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="SAM-PM" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SAM-PM: Enhancing Video Camouflaged Object Detection</h2>
                            <br>
                            <h5>
                                <img src="img/assests/cvpr.png" class="img-responsive img-centered" alt="">


                                <div class="publ">
                                    <ul>
                                        <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                                        <li><a href="https://github.com/SpiderNitt/SAM-PM" target="_blank" style="color:red;"><b>Code</b></a></li>
                                        <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li>
                                    </ul>
                                </div>

                                <h5><strong>Abstract:</strong></h5>
                                
                                <h5>
                                In the domain of large foundation models the Segment Anything Model (SAM) has gained notable recognition for its exceptional performance in image segmentation. However tackling the video camouflage object detection (VCOD) task presents a unique challenge. Camouflaged objects typically blend into the background making them difficult to distinguish in still images. Additionally ensuring temporal consistency in this context is a challenging problem. As a result SAM encounters limitations and falls short when applied to the VCOD task. To overcome these challenges we propose a new method called the SAM Propagation Module (SAM-PM). Our propagation module enforces temporal consistency within SAM by employing spatio-temporal cross-attention mechanisms. Moreover we exclusively train the propagation module while keeping the SAM network weights frozen allowing us to integrate task-specific insights with the vast knowledge accumulated by the large model. Our method effectively incorporates temporal consistency and domain-specific expertise into the segmentation network with an addition of less than 1% of SAM's parameters. Extensive experimentation reveals a substantial performance improvement in the VCOD benchmark when compared to the most recent state-of-the-art techniques. Code and pre-trained weights are open-sourced at https://github.com/SpiderNitt/SAM-PM
                                </h5>

                                <h5>
                                    <strong>Methodology</strong><br>
                                    The proposed <strong>SAM-PM</strong> framework adapts the Segment Anything Model (SAM) for the <strong>Video Camouflaged Object Detection (VCOD)</strong> task. SAM-PM consists of two main components: the <strong>Temporal Fusion Mask Module (TFMM)</strong> and the <strong>Memory Prior Affinity Module (MPAM)</strong>.
                                    <ol>
                                    <li><strong>Temporal Fusion Mask Module (TFMM)</strong>: This module enhances temporal information by integrating mask embeddings from multiple frames. It uses a <strong>spatio-temporal cross-attention</strong> mechanism to create temporally enriched mask embeddings.
                                    </li>
                                    <li><strong>Memory Prior Affinity Module (MPAM)</strong>: MPAM utilizes the temporally infused mask embeddings from TFMM along with image embeddings from the current and previous frames. It applies <strong>affinity</strong> to strengthen temporal consistency in mask predictions.
                                </li>
                                    SAM-PM operates in a <strong>semi-supervised</strong> manner, where only the first frame's ground truth mask is used for training. The framework keeps SAM's weights frozen and trains only the SAM-PM components, ensuring <strong>parameter efficiency</strong> with less than 1 million parameters.
                                </ol>
                                    During training and inference, SAM-PM updates its memory with new frames and their predicted masks while discarding outdated data to maintain temporal coherence.
                                    </h5>
                                    
                            </h5>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://api.wandb.ai/links/spider-r-d/7izxfp8j" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SAM-PM')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                             
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="optical" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Test-time adaptation for Optical Flow </h2>
                            <br>
                            <h5>
                                <img src="img/projects/opticalflow.png" class="img-responsive img-centered" alt="">
                                <img src="img/projects/opticalflowex.png" class="img-responsive img-centered" alt="">
                            
                                <h5>
                                    <h5>
                                        <strong>Optical Flow Estimation</strong><br><br>
                                      
                                        In this research, we focused on optimizing the existing RAFT (Recurrent All-Pairs Field Transforms) model for optical flow estimation using a <strong>multi-GPU</strong> and <strong>multi-node</strong> setup. The model was deployed in a distributed computing environment using <strong>Distributed Data Parallel (DDP)</strong> processing, facilitated by <strong>Lightning-Fabric</strong>. The primary aim of this setup was to efficiently scale the model's training to accommodate larger batch sizes without running into out-of-memory (OOM) errors, a common challenge in deep learning tasks with high computational demands like optical flow estimation.
                                      
                                        The RAFT model's architecture was not modified; instead, we concentrated on improving the scalability and memory handling to enhance computational efficiency.<br><br>
                                      
                                        To achieve these optimizations, a cluster of multiple GPUs and nodes was configured to maximize parallel processing capabilities. The use of <strong>DDP</strong> allowed us to distribute the workload across multiple GPUs and nodes, synchronizing their operations to ensure smooth and efficient model training. This setup provided the necessary computational resources to handle large-scale data while maintaining memory efficiency across devices.<br><br>
                                      
                                        Several key optimization techniques were employed to maximize the efficiency and scalability of the RAFT model in this distributed environment. First, <strong>Fairscale’s CPU offloading</strong> was integrated to shift some memory loads from the GPU to the CPU, thereby freeing up GPU resources for more critical computations. This technique helped mitigate memory bottlenecks, allowing for the handling of larger batch sizes.<br><br>
                                      
                                        Next, we applied <strong>mixed-precision training</strong>, which combines 16-bit and 32-bit floating point operations. This significantly reduced memory consumption without compromising the accuracy of the optical flow estimations.<br><br>
                                      
                                        Finally, we implemented <strong>activation checkpointing</strong> to further minimize memory usage. By only storing key intermediate results and recomputing others when necessary, we reduced the overall memory footprint during training, allowing for a higher batch size without OOM errors.<br><br>
                                      
                                        These optimizations, applied in tandem with a <strong>distributed multi-GPU, multi-node setup</strong>, enabled us to expand the computational capabilities of the RAFT model, ensuring more efficient training while preserving memory integrity.
                                      </h5>

                                      <!-- Load MathJax (Put this in the <head> or before closing </body> tag) -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h5>
  We begin with an input image \( x \), from which an initial optical flow map is generated: <br><br>
  \[
    \text{opt} = \text{model}(x) \tag{9.1}
  \]
  
  Following this, the optical flow map is iteratively refined through a series of augmentation steps. Each step involves generating an augmented input \( x' \) as follows: <br><br>
  \[
    x' = \text{aug}(\text{opt}, x) \tag{9.2}
  \]

  The model then processes the augmented input \( x' \) to produce an updated optical flow map: <br><br>
  \[
    \text{opt}' = \text{model}(x') \tag{9.3}
  \]

  The loss function is calculated as the variance between the original optical flow map \( \text{opt} \) and the augmented optical flow map \( \text{opt}' \): <br><br>
  \[
    \text{Loss} = \text{Variance}(\text{opt}', \text{opt}) \tag{9.4}
  \]

  The primary goal of this algorithm is to reduce the variance between the optical flow map generated for the original input and the augmented images. The augmentation process generates random patches with pixel values constrained by the minimum and maximum values of the input image. These patches are then placed in random positions in both the original image \( x \) and its augmented counterpart \( x' \). The optical flow map produced by the model during test-time adaptation guides the patch placement, and minimizing the variance between the original and augmented optical flow maps helps the model improve its predictions on unseen data.
</h5>


<!-- Include MathJax (add once in your <head> or before </body>) -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <h5>
        <img src="img/projects/optical-flow/fig 9.png" class="img-responsive img-centered" alt="">
        
      <strong>Figure 9:</strong> Percentage of absolute EPE score changes <br>
      <img src="img/projects/optical-flow/fig 10.png" class="img-responsive img-centered" alt="">
      <strong>Figure 10:</strong> EPE scores for different configurations <br>
      <strong>Table 11:</strong> EPE scores for different configurations
    
      <br><br>
    
      <table style="border-collapse: collapse; width: 100%; text-align: center;">
        <thead>
          <tr>
            <th style="border: 1px solid black;">Batch Size</th>
            <th style="border: 1px solid black;">Number of Augmentations</th>
            <th style="border: 1px solid black;">EPE Value</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border: 1px solid black;">8</td>
            <td style="border: 1px solid black;">2</td>
            <td style="border: 1px solid black;">2.75277</td>
          </tr>
          <tr>
            <td style="border: 1px solid black;">2</td>
            <td style="border: 1px solid black;">11</td>
            <td style="border: 1px solid black;">2.75071</td>
          </tr>
          <tr>
            <td style="border: 1px solid black;">6</td>
            <td style="border: 1px solid black;">3</td>
            <td style="border: 1px solid black;">2.78732</td>
          </tr>
          <tr>
            <td style="border: 1px solid black;">2</td>
            <td style="border: 1px solid black;">3</td>
            <td style="border: 1px solid black;">2.74143</td>
          </tr>
          <tr>
            <td style="border: 1px solid black;">6</td>
            <td style="border: 1px solid black;">2</td>
            <td style="border: 1px solid black;">2.80244</td>
          </tr>
          <tr>
            <td style="border: 1px solid black;">2</td>
            <td style="border: 1px solid black;">2</td>
            <td style="border: 1px solid black;">2.76932</td>
          </tr>
        </tbody>
      </table>
    
      <br><br>
    
      Figure 9 illustrates percentage of absolute EPE score decrease for different combinations of number of augmentations, number of epochs, and batch size, while Figure 10 illustrates different combinations of those parameters and their corresponding EPE scores. Table 11 presents EPE scores for different batch sizes and number of augmentations using a system equipped with six GPUs. 
    
      <br><br>
    
      Across these configurations, a reduction in EPE is observed, although the trend is not entirely consistent. This suggests that while augmentations and optimizations contribute to minimizing EPE, further tuning of batch size, GPU count, and augmentation strategies is needed to achieve definitive improvements.
    
      <br><br>
    
      For example, the configuration with a batch size of 2 and 11 augmentations resulted in the lowest EPE value of 2.75071. This configuration illustrates how batch size is calculated in distributed training, where the net batch size is determined using:
    
      \[
        \text{Net Batch Size} = \text{Batch Size} \times (\text{Num of Augmentations} + 1) \times \text{Number of GPUs}
      \]
    
      In this case:
    
      \[
        2 \times (11 + 1) \times 6 = 144
      \]
    </h5>
    
                                      
                                    <br>
                                    <h5><strong>Tech Stack</strong><br>
                                    <ul>
                                        <li><strong>Framework:</strong> Torch Scale</li>
                                    <li><strong>Framework:</strong> PyTorch Lightning Fabric</li>
                               
                                </h5>  
                            </h5>
                            <br>
                            <h3> WanDB Report </h3>
                            Currently in progress and kept private.
                            
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="nllp" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Natural Legal Language Processing</h2>
                            <br>
                            <h5>
                                <img src="img/assests/emnlp.png" class="img-responsive img-centered" alt="">
                                
                                <h5>
                                    Abstract
A key component of the Natural Language Processing (NLP) pipeline is Sentence Boundary Detection (SBD). Erroneous SBD could affect other processing steps and reduce performance. A few criteria based on punctuation and capitalization are necessary to identify sentence borders in well-defined corpora. However, due to several grammatical ambiguities, the complex structure of legal data poses difficulties for SBD. In this paper, we have trained a neural network framework for identifying the end of the sentence in legal text. We used several state-of-the-art deep learning models, analyzed their performance, and identified that Convolutional Neural Network(CNN) outperformed other deep learning frameworks. We compared the results with rule-based, statistical, and transformer-based frameworks. The best neural network model outscored the popular rule-based framework with an improvement of 8% in the F1 score. Although domain-specific statistical models have slightly improved performance, the trained CNN is 80 times faster in run-time and doesn’t require much feature engineering. Furthermore, after extensive pretraining, the transformer models fall short in overall performance compared to the best deep learning model.
                                </h5>  
                            </h5>
                            <br>
                         
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/NLLP-ML/SBD')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>



    
    <div class="portfolio-modal modal fade" id="OCR" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Intelligent OCR </h2>
                            <img src="img/projects/ocr.gif" class="img-responsive img-centered" alt="">
                            <img src="img/projects/ocr.png" class="img-responsive img-centered" alt="">
                             
                            <h5>
                                This project involves the development of an advanced Optical Character Recognition (OCR) software, leveraging a combination of cutting-edge neural network models: CRAFT, Faster R-CNN, Tesseract, and a Siamese network model. The software is hosted on Azure Cloud, utilizing a virtual machine (VM) to run the server and process documents. The entire system is designed to convert scanned documents into editable text, extract bounding boxes for words and sentences, classify sentences into categories, and provide a comprehensive annotation interface for user modifications.
                                </h5>
                                <h5>
                                <b>Key Features:</b>
                                <ul>
                                <li><b>Model Integration:</b> The OCR software integrates multiple state-of-the-art models trained using PyTorch on the FUND dataset. This setup ensures high accuracy and reliability in text extraction and classification.</li>
                                <li><b>Document Processing:</b> Users can upload scanned documents in various formats (.png, .jpg, .jpeg, and .pdf) to the frontend website. For PDFs, only the first page is processed. The software extracts editable text, provides bounding boxes for each word and sentence, and classifies sentences into categories such as 'other', 'question', 'answer', and 'header'.</li>
                                <li><b>Annotation Interface:</b> The frontend features a user-friendly annotation interface built with annotorious.js. Users can modify model predictions or annotate documents from scratch. Annotations can be saved and downloaded in a simple .txt format.</li>
                                <li><b>Offline Processing:</b> The software supports offline batch processing, allowing users to process multiple images at once. Users can choose between output formats like MTX or FUND dataset formats.</li>
                                <li><b>Server Details:</b> The models run on an Azure VM with Linux (Ubuntu 18.04), specifically a Standard B2s instance (2 vCPUs, 4 GiB memory). Due to server connection constraints, there might be occasional interruptions in access, but comprehensive setup instructions are provided for replicating the environment.</li>
                                <li><b>Training and Metrics:</b> Model training is conducted using PyTorch, with detailed explanations of training steps and performance metrics. All trained models and predictions on public test datasets are available for download.</li>
                                </ul>
                                </h5>
                                
                            <br>
                            <img src="img/projects/ocr2.png" class="img-responsive img-centered" alt="">
                            <img src="img/projects/ocr_demo.png" class="img-responsive img-centered" alt="">
                               
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/OCR')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                        <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://youtu.be/8Cci5rw-KyY')"><i
                                            class="fa fa-fw fa-youtube"></i> Demo</button>
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="i-Pravesh" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>i-Pravesh </h2>
   
                        <img src="img/projects/sih.png" class="img-responsive img-centered" alt="">
                        <img src="img/projects/sih_.png" class="img-responsive img-centered" alt="">
                            <h5>
                                <h5>I-Pravesh is a sophisticated Smart Attendance Android application designed to simplify and enhance the attendance recording process. It uses advanced face detection and recognition technologies, specifically MobileFaceNet combined with TensorFlow Lite, for accurate biometric authentication. The app not only performs fast and reliable face recognition but also ensures that the user is within a designated location through geofencing. It maintains face embeddings and encrypts credentials for secure data handling. Additionally, I-Pravesh features a comprehensive dashboard for both users and administrators, providing a user-friendly interface for managing and monitoring attendance records efficiently.</h5>
                            <br>
                        
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SIH')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>

                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="summarise" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SummarizeIQ: An Integrated Summarization and Content Analysis Engine </h2>
   
                        <img src="img/projects/summarise.png" class="img-responsive img-centered" alt="">
                              
                            <h5>
                                <h5>
                                    SummarizeIQ combines T5 summarization with Torch Serve, incorporating advanced extractive methods like LexRank and TextRank for enhanced text processing. The system features keyword generation and employs query-based content filtering using cosine similarity to deliver precise and relevant summaries. It also boasts a user-friendly website interface, allowing seamless interaction with the summarization tools. Additionally, SummarizeIQ supports multi-threaded web scraping, enabling efficient and concurrent data extraction from various sources for comprehensive article analysis and summarization.
                                </h5>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SummarizeIQ')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>

                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="realestatevr" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Real Estate VR </h2>
   
                        <img src="img/projects/vr.gif" class="img-responsive img-centered" alt="">
                       
                            <h5>
                                <h5>
                                    <h5><strong>Problem Statement</strong><br>
                                        Traditional real estate methods rely on physical visits and static images, limiting immersion and convenience. To enhance the real estate experience, we aim to integrate virtual reality (VR) technology, creating a user-friendly, cross-platform VR application. This solution will offer immersive property exploration and real-time updates, modernizing how users view and interact with properties.</h5>
                                        
                                        <h5><strong>Design</strong><br>
                                        1. <strong>Asset Coordination and Environment Generation:</strong> Utilizing Unity’s assets and spatial mapping, we create a realistic, adaptable virtual environment for immersive exploration.<br>
                                        2. <strong>Advanced User Interaction with XR Device Simulator:</strong> An intuitive interface with Unity’s XR Device Simulator allows seamless navigation and interaction through a 'digital wand,' enhancing user experience with C# scripting.<br>
                                        3. <strong>Dynamic Building Placement with Spatial Intelligence:</strong> Real-time building placement is facilitated by Unity’s spatial analysis, offering precise options based on spatial context.<br>
                                        4. <strong>Futuristic Navigation Paradigms:</strong> Advanced navigation includes teleportation and aerial views, allowing instant spatial traversal and comprehensive property inspection.<br>
                                        5. <strong>Precision-Engineered C# Scripting:</strong> C# scripts ensure smooth user controls, dynamic building placement, and effective interaction management.<br>
                                        Our VR platform allows users to add plots, select and place buildings, and view properties with detailed information. It supports flythrough and teleportation, with eagle-eye views for broader city and plot planning.</h5>
                                        
                                        <h5><strong>Key Features</strong><br>
                                        - Add and visualize new plots<br>
                                        - Place houses with size and price details<br>
                                        - Immersive VR interface<br>
                                        - Flythrough and teleportation travel<br>
                                        - Multi-plot monitoring with eagle-eye views</h5>
                                        
                                        <h5><strong>Tools Used</strong><br>
                                        - Unity<br>
                                        - C#<br>
                                        <img src="img/projects/vr.png" class="img-responsive img-centered" alt="">
                               
                                </h5>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/Real-Estate-VR')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                        <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://youtu.be/galwI6_rIiw')"><i
                                            class="fa fa-fw fa-youtube"></i> Demo</button>
    
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="compiler" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Yet Another Python Compiler</h2>
   
                        <img src="img/projects/compiler.gif" class="img-responsive img-centered" alt="">
                       
                            <h5>
                                A Python Compiler, built with Bison and Flex, processes Python code by tokenizing it, parsing it into a syntax tree, and performing semantic checks. 
                                It generates intermediate code and applies optimizations like constant folding and dead code elimination. 
                                Key features include lexical analysis, syntax and semantic error detection, and code optimization.

                                <h5><strong>Overview</strong><br>
                                    The project focuses on developing a compiler for a custom programming language, implementing various stages of compilation including lexical analysis, parsing, semantic analysis, intermediate code generation, and code optimization.</h5>
                                    
                                    <h5><strong>Tech Stack</strong><br>
                                    <strong>Programming Language:</strong> Python<br>
                                    <strong>Tools:</strong> YACC (Yet Another Compiler Compiler), Lex (for lexical analysis)<br>
                                    <strong>Data Structures:</strong> Symbol tables, parse trees, abstract syntax trees (ASTs)</h5>
                                    
                                    <h5><strong>Features and Contributions</strong></h5>
                                    
                                    <h5><strong>Lexical Analysis:</strong><br>
                                    Token identification and lexical error detection.<br>
                                    Developed token identification and error detection mechanisms.<br>
                                    Created a symbol table to manage identifiers and constants.</h5>
                                    
                                    <h5><strong>Parser:</strong><br>
                                    Syntax declaration, indentation and syntactic error detection.<br>
                                    Implemented syntax rules using YACC for expressions and control flow.<br>
                                    Generated parse trees and abstract syntax trees (ASTs).</h5>
                                    
                                    <h5><strong>Semantic Analysis:</strong><br>
                                    SDD + SDT, Annotated Parse Tree, and Semantic Error detection.<br>
                                    Designed syntax-directed translations to ensure semantic correctness.<br>
                                    Detected semantic errors such as undeclared variables and misuse of reserved identifiers.</h5>
                                    
                                    <h5><strong>Intermediate Code Generation (ICG):</strong><br>
                                    Generated three-address code and quadruples.<br>
                                    Implemented backpatching for jump statements.</h5>
                                    
                                    <h5><strong>Code Optimization:</strong><br>
                                    Basic blocks, DAG, CFG, Induction Variable elimination.<br>
                                    Applied optimization techniques including constant folding, copy propagation, dead code elimination, and peephole optimization.<br>
                                    Enhanced code efficiency through these optimizations.</h5>
                                    

                                </h5>
                            <br>
                            <br>
                            <h3> Report </h3>
                            <br>
                            <iframe src="img/projects/compiler/Group13_106121045_106121099_106121129.pdf" width="100%" height="600px"></iframe>
                            <br>
                            <h4> Lexical Analsysis </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Lexer_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> Parser </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Parser_Final_Report.pdf" width="100%" height="600px"></iframe>
                            <br>
                            </details>
                            <br>
                            <h4> Semantic Analsysis </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Semantic_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> ICG </h4>
                            <details>
                           
                            <iframe src="img/projects/compiler/ICG_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> Optimization </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Optimization_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/YA-Python-Compiler')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="octtree" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Oct tree based 3D OpenGL Renderer</h2>
   
                        <img src="img/projects/octtree.gif" class="img-responsive img-centered" alt="">
                       
                            <h5>
                                The Octree-Based 3D OpenGL Renderer, created with PyOpenGL, efficiently renders 3D scenes by using an octree data structure to manage and optimize rendering performance.
        
                                </h5>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/3D-Model-Renderer-Oct-Tree')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="sart" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Sartorius Cell Instance Segmentation</h2>
   
                        <img src="img/projects/sart.gif" class="img-responsive img-centered" alt="">
                       
                        <h5>
                            Trained R-CNN, U-Net and Detectron2 using Pytorch to detect and delineate
                                distinct objects of interest in biological images depicting neuronal cell types commonly used in studying neurological disorders
                        </h5>
                        <h5><b>Key findings include:</b></h5>
                        <ol>
                            <li><b>Model Comparison:</b> Detectron2 outperformed U-Net and Mask R-CNN, showcasing superior accuracy and robustness in segmenting complex neuronal cell structures.</li>
                            <li><b>U-Net:</b> Known for capturing fine details, U-Net was used with a ResNet34 backbone and a combination of focal and dice losses to handle class imbalance. Post-processing improved segmentation outcomes, though some blurriness persisted.</li>
                            <li><b>Mask R-CNN:</b> Leveraging ResNet50, this model generated object bounding boxes and masks, with loss functions tailored to boundary, mask, and classification losses. Overlapping segmentations were mitigated through pixel-wise intersection operations.</li>
                            <li><b>Detectron2:</b> As a flexible, high-performance framework, Detectron2 provided modular implementation and incorporated innovative techniques like deformable convolutions, leading to its superior performance in this study.</li>
                        </ol>
                        
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/Sartorius-Cell-Instance-Segmentation')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="others" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Other Projects</h2>
   
                        <img src="img/projects/others.gif" class="img-responsive img-centered" alt="">
                       
                        <h5>
                            Projects include: <br>

                            <a href="https://github.com/FrozenWolf-Cyber/Brain-Tumor-Classification"> Tumor classification </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/COVID19-Pneumonia-Chest-X-Ray-Detection"> Pneumonia (COVID-19) chest X-ray classification </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Neural-Style-Transfer"> Neural style transfer </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Tensorflow-Lite-Crack-Detection"> Crack detection with TF Lite </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Face-Recognition"> Face detection using a Siamese Neural Network model to develop a one-shot neural network with FaceNet and MTCNN as the backbone </a> 
                            <a href="https://github.com/FrozenWolf-Cyber/Keres"> Unity based C# game called "The Keres" </a> 
                        </h5>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>



    <!-- About Section -->
    <section class="success  grid-band2" id="presentation">
        <div class="container"  class="grid-band2">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Presentation & Surveys</h2>
                    <hr class="light">
                </div>
            </div>
            


            <div class="row"  class="grid-band2">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/iisc_mimic.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Multi-Modal Representation Learning for clinical data and Test-time adaptation for Optical Flow</strong></div>
                    <div class="pubv"><b>2022</b> </div>
                    These slides and report covers the results from MIMIC-IV multi-modal fusion and optical flow test-time adaptation for RAFT. Notion contains survey on multimodality and contrastive learning methods in MIMIC (III, IV) and similar dataset along with SOTA models available in each modalities such as CXR, EHR, ECG and Notes.
           

                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1CU_Gzy2BEQ5bYzLrQQq5W4BrMf7QgzsWftuQOCUiqvE/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                             <li style="margin-bottom: 10px;">
                                 <a href="https://oceanic-antler-eb0.notion.site/1c6ae01d802f44d4aa1c4bafe6f3d4a2?v=0198cff002b34f94ac6bce016294b0ad"  target="_blank" class="button notion">
                                     <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                     <b>Notion</b>
                                 </a>
                             </li>
                             <li>
                                 <a href="https://oceanic-antler-eb0.notion.site/SOTA-models-963b2664d0894f6bb0f09d6a12242704?pvs=4" target="_blank" class="button notion">
                                     <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                     <b>Notion (SOTA)</b>
                                 </a>
                             </li>
                                
                             <li style="margin-bottom: 10px;">
                                <a href="https://drive.google.com/file/d/17V-IovP9YHo6Lb0jlTgjNYpH9m7nXoAm/view?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Report</b>
                                </a>
                            </li>
                             
                         
                        </ul>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cl.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Continual Learning Survey</strong></div>
                    <div class="pubv"><b>2022</b> </div>
                    These slides and notion database partially covers the classical and SOTA methods in continual learning and its drawbacks till 2022.

                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1l_O9qThYFHkLAaoYyAB4Et7UxxGGsvF3D8gcSbEH6Ls/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://oceanic-antler-eb0.notion.site/113a771fb39c80f18251c3091f4f192a?v=113a771fb39c815782cb000c67af58ab" target="_blank" class="button notion">
                                    <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                    <b>Notion</b>
                                </a>
                            </li>
                        </ul>
                    </div>
                    


                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/LLM planner.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Embodiement Agent (TAMP with LLM) Mini Survey and proposed strategy</strong></div>
                    <div class="pubv"><b>2023</b> </div>
                    These slides and notion database partially covers the embodied agent and Evaluation metrics used in TAMP along with a proposed strategy consisting of Instruction Planner, Coder for debug analysis, and Skill Handler to optimize actions. It learns from execution feedback, continuously improving performance through reflection and memory management.


                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1ppk0cnIqiQhhx0jARil2j1jLPSAShffTaBfjwhznLb8/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://oceanic-antler-eb0.notion.site/f7e79139232c49c896540f515f38893e?v=c559a2753a7b40a8903a21ffb2a485f1" target="_blank" class="button notion">
                                    <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                    <b>Notion</b>
                                </a>
                            </li>
                        </ul>
                    </div>

                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cmu/comb.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-Interpreted Deep-Learning Approach for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="pubv"><b>2024</b> </div>
                    These slides cover contributions of SaSi
                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1-fbPDVgn0zuPuR4-GX3mox6vF11oglS6xcqOVqwfZtw/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                           
                        </ul>
                    </div>

                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/dl_teach.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Introduction to Deep Learning</strong></div>
                    <div class="pubv"><b>Spider R&D Workshop 2023</b> </div>
                    These slides cover entire deep learning from basic till generative models like GANs and Diffusion Based Models. The slides progress from basic neural networks to advanced models like GANs and Diffusion Based Models along with notebooks and visualizations.
                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1OTKlOIsqToT6K8lM0iTgMkguyzXt8DgZOraRozAy6Os/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://linktr.ee/dl_workshop?utm_source=linktree_profile_share&ltsid=84e5ac63-9c82-4d22-9425-5954a9ee3b03" target="_blank" class="button notion">
                                    <i class="fa fa-link"></i> <!-- Placeholder icon for Notion -->
                                    <b>LinkTree</b>
                                </a>
                            </li>
                        </ul>
                    </div>

                </div>
            </div>





    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <script src="js/timeline.js"></script>
    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/freelancer.js"></script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-92670333-1', 'auto');
        ga('require', 'ipMeta', {
            serviceProvider: 'dimension1',
            networkDomain: 'dimension2',
            networkType: 'dimension3',
        });
        ga('ipMeta:loadNetworkFields');
        ga('send', 'pageview');
    </script>
    <script async src="https://ipmeta.io/plugin.js"></script>
</body>

<script>
    document.addEventListener("DOMContentLoaded", function () {
      const url = new URL(window.location);
      const params = url.searchParams;
  
      console.log("myc", params);
      const targetId = params.get("trigger"); // Gets "cl-sam"
      console.log("myc", targetId);
  
      const el = document.getElementById(targetId);
      console.log("myc", el);
      console.log("myc before clicking");
      console.log("myc clicking it", el);
  
      if (el) el.click();
    });
  </script>
  
  
  
</html>
