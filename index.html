<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="./img/logos/terminal.png" />

    <title>Gokul Adethya: Personal Website</title>


    <!-- Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/timeline.css" rel="stylesheet">
    <link href="css/freelancer.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet"
        type="text/css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&display=swap" rel="stylesheet">
    <script src="js/modernizr.js"></script>
    <meta name="google-site-verification" content="of5d9ckLytWKCHg9c9NZ7oTGOIP9pyZIvhsY-E1l-kY" />
    <style>
       
        .button {
            display: inline-flex;
            align-items: center;
            padding: 10px 15px;
            border-radius: 5px;
            text-decoration: none;
            color: white;
            font-weight: bold;
            transition: background-color 0.3s;
            border: none;
            cursor: pointer;
        }
        .google-slides {
            background-color: #5796fb; /* Google Slides blue */
        }
        .notion {
            background-color: #fd6969; /* Notion red */
        }
        .button:hover {
            opacity: 0.9;
        }
        .icon {
            margin-right: 8px; /* Space between icon and text */
        }
    </style>
    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse"
                    data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-about-me-hidden navbar-brand" href="#page-top">GOKUL ADETHYA</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <!-- <li class="page-scroll">
                            <a href="#timeline">Timeline</a>
                        </li> -->
                    <li class="page-scroll">
                        <a href="#about">Publications</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#projects">Projects</a>
                    </li>

                    <li class="page-scroll">
                        <a href="#presentation">Presentation & Surveys</a>
                    </li>
                    

                    <li>
                        <a href="https://drive.google.com/file/d/1xgukjX0Roh4qaay9HiSJre7WK5HIBV05/view?usp=sharing" target="_blank">CV</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->

        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
        <div class="container">
            <div class="row">

                <div class="col-xs-4">
                    <img class="img-responsive img-circle" src="img/gokul.jpeg" alt="">
                </div>
                <div class="col-xs-8">
                    <div class="intro-text">
                        <span class="name">Gokul Adethya</span>
                        <br>

                        <span class="about_me"> 
                            <p style="text-align: justify">
                                I am a final year student from National Institute of Technology Trichy in the Computer Science Department. I am currently working as a Research Intern at the <a href="https://cistup.iisc.ac.in/" target="_blank"> CiSTUP</a> at <a href="https://iisc.ac.in/" target="_blank"> The Indian Institute of Science </a>, under the guidance of
                                <a href="https://www.punitrathore.com/" >Prof. Punith Rathore</a>, where my research focuses on Multi-Modal Fusion with cross modal alignment using contrastive learning in medical dataset. I am also working on test time adaptation of Optical Flow models. Prior to this, I interned at CMU, <a href="https://xulabs.github.io/">Xu Labs</a> where I worked under the supervision of <a href="https://xulabs.github.io/" target="_blank"> Prof. Min Xu </a> on 
                                Self-suprvised learning for Cryo-ET for few-shot setup. I also interned at <a href="https://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>/<a href="https://nus.edu.sg/">National University of Singapore</a>, under the guidance of <a href="http://www.labren.org/mm/"> Dr. Ren Hongliang </a> focusing on Robotic Motion Planning for surgical scenarios. I have also worked on Empathetic Response Generation and model distillation using LLM during my intern at Samsung PRISM and on Natural Legal Language Processing under the guidance <a href="https://scholar.google.co.in/citations?user=wHHgs8UAAAAJ&hl=en">Dr. S. Jaya Nirmala</a>.  I am currently the head of Machine Learning domain of the <a href="https://spider.nitt.edu/" target="_blank">Spider R&D</a> club, an independent research and development group at NITT run by students.
                            </p>
                            <p style="text-align: justify">
                                My research focuses on Computer Vision, Reinforcement Learning, Multi-Modality, Self-supervised Learning, Continual learning with a specific interest in Healthcare
                            </p>
                            <p>
                                For any collaboration, feel free to reach me at <b><a href="mailto:gokul3112003.com@gmail.com" >gokul3112003.com@gmail.com</a></b>
                            </p>
                        </span>
                        <hr>
                        <span class="skills">
                            <ul class="list-inline">
                                <li>
                                    <a href="https://scholar.google.com/citations?hl=en&user=m8n5yo8AAAAJ"
                                        class="btn-social btn-outline" target="_blank"><i class="ai ai-google-scholar" ></i></a>
                                </li>
                                <li>
                                    <a href="https://www.linkedin.com/in/gokul-adethya/" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-linkedin"></i></a>
                                </li>
                                <li>
                                    <a href="https://github.com/FrozenWolf-Cyber" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-github"></i></a>
                                </li>
                                <li>
                                    <a href="mailto:gokul3112003.com@gmail.comr" class="btn-social btn-outline" target="_blank"><i
                                            class="fa fa-fw fa-envelope"></i></a>
                                </li>
                            </ul>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <section id="affilition">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Affiliations</h2>
                    <hr class="light">
                </div>
            </div>
            <ul class="affiliation-list row">
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/iisc.jpg" style="width: 110px; height: 110px;"/>
                    </a><br>
                    IISc<br>
                    (Jun 2024 - Present)
                </li>
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/cmu.png" style="width: 110px; height: 110px;"/>
                    </a><br>
                    CMU<br>
                    (Dec 2023 - May 2024)
                </li>
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/cuhk.png" style="width: 185px; height: 110px;"/>
                    </a><br>
                    NUS/CUHK  <br>
                    (Feb 2023 - Feb 2024)<br>
                </li>
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/nus.jpg" style="width: 185px; height: 110px;"/>
                    </a><br>
                    NUS/CUHK  <br>
                    (Feb 2023 - Feb 2024)<br>
                </li>
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/samsung.png" style="width: 110px; height: 110px;"/>
                    </a><br>
                    Samsung PRISM<br>
                    (Aug 2022 - March 2023)
                </li>
                <li class="col-sm-3 col-xs-6">
                    <a class="btn-affl">
                        <img class="affilition logo" src="img/logos/nitt.png" style="width: 115px; height: 110px;"/>
                    </a><br>
                    NIT-Trichy<br>
                    (2021 - Present)
                </li>
            </ul>
        </div>
    </section>
    <section id="news-list">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>News</h2>
                    <hr class="light">
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Oct 2024</div>
                    <div class="col-xs-8"><b>Awarded the <span style="color: #E73916">IndiaAI Fellowship</span> and grant, given to only 50 across India, to research <span style="color: #e77b16">Speech & NLP multi-modal with Continual Learning !</b></div>
                </div>

                <div class="row">
                    <div class="col-xs-4 news-date">Sep 2024</div>
                    <div class="col-xs-8"><b>Achieved 12th rank out of 74,000 participants nationwide in the <span style="color: #E73916"> Amazon ML Challenge </span> !</b></div>
                </div>
                <!-- <div class="row">
                    <div class="col-xs-4 news-date">Sep 2024</div>
                    <div class="col-xs-8"><b>One first author paper submitted at <span style="color: #E73916">WACV 2025 Conference</span> (affliated with CMU) !</b></div>
                </div> -->
                <div class="row">
                    <div class="col-xs-4 news-date">Jun 2024</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Indian Institute of Science</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">April 2024</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">CVPR 2024 Workshop </span> on Pixel-level Video Understanding in the Wild Challenge !</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Dec 2023</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Carnegie Mellon University</span> as research intern!</b></div>
                </div>
                <!-- <div class="row">
                    <div class="col-xs-4 news-date">Sep 2023</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">International Journal of Biomedical Engineering and Technology </span> !</b></div>
                </div> -->
                <div class="row">
                    <div class="col-xs-4 news-date">July 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">National Institute of Singapore</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Feb 2023</div>
                    <div class="col-xs-8"><b>One paper accepted in <span style="color: #E73916">EMNLP 2022 Workshop </span> on Natural Legal Language Processing !</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Nov 2022</div>
                    <div class="col-xs-8"><b>Top 30 teams in <span style="color: #E73916">NeurIPS 2022 CityLearn Challenge</span> in Multi-agent RL!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Aug 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">Samsung PRISM</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">July 2022</div>
                    <div class="col-xs-8"><b>Winner of <span style="color: #E73916">Smart India Hackathon</span> among 160k+ students!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">April 2022</div>
                    <div class="col-xs-8"><b>Joined <span style="color: #E73916">NIT-Trichy</span> as research intern!</b></div>
                </div>
                <div class="row">
                    <div class="col-xs-4 news-date">Jan 2022</div>
                    <div class="col-xs-8"><b>Runner up of <span style="color: #E73916">MTX - HackOlympics 2.0 Shaastra</span> hackathon in my first year as a solo participant!</b></div>
                </div>
            </div>
        </div>
    </section>

    <!-- About Section -->
    <section class="success" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Publications</h2>
                    <hr class="light">
                </div>
            </div>
            <!-- <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/cvpr.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha, Xingjian Li</div>
                    <div class="pubv"><b>WACV 2025</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="#cmu" target="_blank" style="color:red;" data-toggle="modal"><b>UNDER REVIEW - WanDB</b></a></li>
                        </ul>
                    </div>
                </div>
            </div> -->

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/cvpr.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SAM-PM: Enhancing Video Camouflaged Object Detection using Spatio-Temporal Attention</strong></div>
                    <div class="puba"> Muhammad Nawfal Meeran, <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha</div>
                    <div class="pubv"><b>CVPR 2024 Workshop</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://github.com/SpiderNitt/SAM-PM" target="_blank" style="color:red;"><b>Code</b></a></li>
                            <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/internal.jpg" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Be My ASSistant: Exploring LLM Empowered Interactive Surgical Assistant for Surgical Sub-Task Automation</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Lalithkumar Seenivasan, Ashwin Krishna Kumar, Mobarakol Islam, Hongliang Ren </div>

                    <div class="pubv"><b>Submitted to RA-L</b> </div>
                    <div class="publ">
                        <ul>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li> -->
                            <li><a href="https://frozenwolf-cyber.github.io/sasi.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li> -->
                        </ul>
                    </div>
                </div>
            </div>


            


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cmu/comb.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-Interpreted Deep-Learning Approach for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="puba"> <b>Gokul Adethya T</b>, Bhanu Pratyush Mantha, Tianyang Wang, Xingjian Li, Min Xu </div>

                    <div class="pubv"><b>Submitted to PLOS Computational Biology</b> </div>
                    <div class="publ">
                        <ul>
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/papers/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li> -->
                            <li><a href="https://frozenwolf-cyber.github.io/sasi.html" target="_blank" style="color:red;"><b>Project Page</b></a></li>
                            <li><a href="https://docs.google.com/presentation/d/1CU_Gzy2BEQ5bYzLrQQq5W4BrMf7QgzsWftuQOCUiqvE/edit?usp=sharing" target="_blank" style="color:blue;"><i class="fa fa-google icon"></i><b>Slides</b></a></li>
                       
                            <!-- <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/PVUW/html/Meeran_SAM-PM_Enhancing_Video_Camouflaged_Object_Detection_using_Spatio-Temporal_Attention_CVPRW_2024_paper.html" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li> -->
                        </ul>
                    </div>

                 

                

                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/assests/emnlp.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Efficient deep learning-based sentence boundary detection in legal text</strong></div>
                    <div class="puba"> Reshma Sheik, <b>T Gokul</b>, S Nirmala</div>
                    <div class="pubv"><b>EMNLP NLLP 2022 Workshop</b> </div>
                    <div class="publ">
                        <ul>
                            <li><a href="https://aclanthology.org/2022.nllp-1.18.pdf" target="_blank" style="color:blue;"><b>PDF</b></a></li>
                            <li><a href="https://github.com/NLLP-ML/SBD" target="_blank" style="color:red;"><b>Code</b></a></li>
                            <li><a href="img/assests/emnlp.txt" target="_blank" style="color:#B87333;"><b>bibtex</b></a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Talks</h2>
                    <hr class="light">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 text-center">
                <div class="pubt">Known unknowns: Learning novel concepts using reasoning-by-elimination (UAI Oral Talk)</div>
                <br>
                <iframe src="https://drive.google.com/file/d/1aqM2XWzicxCGEe6BbVHKozkmdvvUd2kp/preview"
                    width="480" height="320" allowfullscreen></iframe>
                    
                </div>

            </div>

        </div>
    </section> -->




    <!-- Portfolio Grid Section -->
    <section id="projects">
        <div class="">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Projects</h2>

            <h5>Click on the project to know more about it !!!</h5>
                    <hr class="light">
                </div>
            </div>
            <div class="band">



                <div class="item-1">


                    <a href="#mimic" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="width: 23vw; height: 20vw; background-image: url(img/projects/iisc_mimic.gif);"></div>
                        <article class="article">
                            <h1>Medical Multi-Modal Fusion & Cross Modal Alignment </h1>
                            <p>
                                This project focuses on multi-modal fusion and cross-modal representation alignment with self0supervised learning by integrating EHR, CXR, Notes, and ECG data.
                            </p>
                            <span>IISc</span>
                        </article>
                    </a>

                </div>



                <div class="item-2">


                    <a href="https://frozenwolf-cyber.github.io/mass.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb"
                            style="width: 25vw; background-image: url(img/projects/nus/15-ezgif.com-video-to-gif-converter.gif);">
                        </div>
                        <article class="article">
                            <h1>MASS (My ASSistant)</h1>
                            <p>
MASS uses an LLM to assist surgeons in automating surgical robots by sequencing actions from simple instructions. The project develops robotic motion planning with LLMs and reinforcement/imitation learning for surgery.
                            </p>
                            <span>NUS/CUHK</span>
                        </article>
                    </a>

                </div>

                <div class="item-3">

                    <a href="https://frozenwolf-cyber.github.io/sasi.html" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/cmu.gif);"></div>
                        <article class="article">
                            <h1>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</h1>
                            <p>This project introduces a SaSi deep learning approach for weak label cryo-ET segmentation in few-shot learning, using Consistency Loss, SimCLR, and AugMix to improve data efficiency and outperform existing methods. </p>
                            </p>
                            <span>CMU</span>
                        </article>
                    </a>

                </div>

                <div class="item-4">
        
                    <a href="#optical" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/opticalflow.png);"></div>
                        <article class="article">
                            <h1>Test-time adaptation for Optical Flow</h1>
                            <p>Test-time adaptation for optical flow method by using augmentation and pesudo predictions.</p>
                            </p>
                            <span>IISc</span>
                        </article>
                    </a>


                </div>

                <div class="item-5">


                    <a href="#SAM-PM" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/assests/cvpr.png);"></div>
                        <article class="article">
                            <h1>SAM-PM: Enhancing Video Camouflaged Object Detection</h1>
                            <p>SAM-PM is a propagation Module to enhance the SAM’s performance in video camouflage object detec-
tion, achieving substantial improvements with minimal parameter addition (&lt 1M) while performing better than
previous SOTA by 37.9% on mIoU, which got accepted for CVPR 2024 workshops</p>
                            </p>
                            <span>NITT</span>
                        </article>
                    </a>



                </div>

                <div class="item-6">


                    <a href="#nllp" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/assests/emnlp.png);"></div>
                        <article class="article">
                            <h1>Natural Legal Language Processing</h1>
                            <p>
                                Worked on Natural Legal Language Processing by using LLMs like BERT, XLNET, and Hierarchical Transformers on judiciary datasets.
                                Developed a lightweight CNN model for sentence boundary detection that is 80x faster than traditional statistical models.
                            </p>
                            <span>NITT</span>
                        </article>
                    </a>
       
                </div>




                <div class="item-7">
        
                    <a href="#OCR" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style=" background-image: url(img/projects/mtx.jpeg);"></div>
                        <article class="article">
                            <h1>Intelligent OCR</h1>
                            <p>
                                My OCR integrates CRAFT, Faster R-CNN, Tesseract, and a Siamese network to perform sentence classification and key-value pair detection, including bounding boxes and linked information using PyTorch which is hosted in Azure Cloud. 
                            </p>
                            <span>NITT</span>
                        </article>
                    </a>
                  


                </div>

                <div class="item-8">
        
                    <a href="#i-Pravesh" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb"
                            style=" background-image: url(img/projects/sih.png);">
                        </div>
                        <article class="article">
                            <h1>i-Pravesh</h1>
                            <p>I-Pravesh is a Smart Attendance Android App which uses a combination of edge face detection and recognition (MobileFaceNet + TensorFlow Lite) as the authentication biometric for recording attendance.</p>
                            <span>NITT</span>
                        </article>
                    </a>

                  


                </div>

                <div class="item-9">
                 
                        
                    <a href="#summarise" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/summarise.png);"></div>
                        <article class="article">
                            <h1>SummarizeIQ: An Integrated Summarization and Content Analysis Engine</h1>
                            <p>This project integrates T5 like LLM summarization models with Torch Serve, enhanced by extractive methods like LexRank and TextRank. It also includes keyword generation and implements query-based content filtering using cosine similarity, offering a comprehensive solution for article summarization and analysis. 
                            </p>
                            <span>NITT</span>
                        </article>
                    </a>

                </div>


                <div class="item-10">
                 
                        
                    <a href="#realestatevr" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/vr.gif);"></div>
                        <article class="article">
                            <h1>Real Estate VR</h1>
                            <p>This VR platform leverages Unity’s advanced capabilities to create an immersive and dynamic environment.
                                 The application features precise asset coordination and spatial intelligence for real-time building placement through an intuitive XR Device Simulator, enabling seamless navigation and interaction.
                                   
                                 
                            </p>
                            <span>NITT</span>
                        </article>
                    </a>

                </div>


                <div class="item-11">
                 
                        
                    <a href="#compiler" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/compiler.gif);"></div>
                        <article class="article">
                            <h1>Yet Another Python Compiler</h1>
                            <p>
                                
                                A Python Compiler, built with Bison and Flex, processes Python code by tokenizing it, parsing it into a syntax tree, and performing semantic checks. 
                                It generates intermediate code and applies optimizations like constant folding and dead code elimination. 
                                Key features include lexical analysis, syntax and semantic error detection, and code optimization.

                                 
                            </p>
                            <span>NITT</span>
                        </article>
                    </a>

                </div>


                <div class="item-12">
                 
                        
                    <a href="#octtree" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/octtree.gif);"></div>
                        <article class="article">
                            <h1>Oct tree based 3D OpenGL Renderer</h1>
                            <p>
                                
The Octree-Based 3D OpenGL Renderer, created with PyOpenGL, efficiently renders 3D scenes by using an octree data structure to manage and optimize rendering performance.
                                 
                            </p>
                            <span>NITT</span>
                        </article>
                    </a>

                </div>
                <div class="item-13">
                 
                        
                    <a href="#sart" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/sart.gif);"></div>
                        <article class="article">
                            <h1>Sartorius Cell Instance Segmentation</h1>
                            <p>Trained R-CNN, U-Net and Detectron2 using Pytorch to detect and delineate
                                distinct objects of interest in biological images depicting neuronal cell types commonly used in studying neurological disorders.
                            </p>
                                <span>NITT</span>
                        </article>
                    </a>

                </div>

                <div class="item-13">
                 
                        
                    <a href="#others" class="card portfolio-link" data-toggle="modal">
                        <div class="thumb" style="background-image: url(img/projects/others.gif);"></div>
                        <article class="article">
                            <h1>Other Projects</h1>
                            <p>
                                Projects include: <br>

                                Brain tumor classification <br>
                                Pneumonia (COVID-19) chest X-ray classification <br>
                                Neural style transfer <br>
                                Crack detection with TF Lite <br>
                                Face detection using a Siamese Neural Network model to develop a one-shot neural network with FaceNet and MTCNN as the backbone
                                Unity based C# game called "The Keres" 
                            </p>
                                <span>NITT</span>
                        </article>
                    </a>

                </div>


          

            </div>
        </div>
    </section>


    <!-- Footer 
        <footer class="text-center">
            <div class="footer-below">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-12">
                            Copyright &copy; Harsh Agrawal 2016
                        </div>
                    </div>
                </div>
            </div>
        </footer>
    -->

    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes)
    <div class="scroll-top page-scroll visible-xs visible-sm">
        <a class="btn btn-primary" href="#page-top">
            <i class="fa fa-chevron-up"></i>
        </a>
    </div> -->

    <!-- Portfolio Modals -->
    <div class="portfolio-modal modal fade" id="mimic" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Medical Multi-Modal Fusion & Cross Modal Alignment</h2>
                            <br>
                            <p>
                                <img src="img/projects/iisc_mimic.gif" class="img-responsive img-centered" alt="">
                            
                                <strong>Dataset Analysis and Performance Report</strong><br><br>

                                This project focuses on analyzing and improving the performance of models using a multimodal dataset comprising Electronic Health Records (EHR) data, including demographic information, ICU vitals, Chest X-rays (CXR), ECG signals, and clinical notes. The primary tasks include predicting readmission, mortality, and length of stay (LOS) for patients based on data available within 24 to 48 hours after admission.<br><br>
                                
                                <strong>Modalities and Tasks</strong><br>
                                <ul>
                                    <li><strong>EHR Demographics:</strong> Includes categorical data like race and age, with age discretized into groups due to deidentification.</li>
                                    <li><strong>ICU Vitals:</strong> A selection of 39 vital signs, time series data, and categorical events like procedures.</li>
                                    <li><strong>CXR:</strong> Time series of chest X-ray images.</li>
                                    <li><strong>ECG:</strong> 12-lead ECG signals represented as time series.</li>
                                    <li><strong>Clinical Notes:</strong> Series of notes, such as discharge and radiology notes.</li>
                                </ul>
                                The tasks involve:
                                <ol>
                                    <li><strong>Readmission Prediction:</strong> Whether a patient will be readmitted within a month after discharge.</li>
                                    <li><strong>Mortality Prediction:</strong> Predicting if the patient will survive or die.</li>
                                    <li><strong>Length of Stay Prediction:</strong> Whether the stay will exceed 3 or 7 days.</li>
                                </ol><br>
                                
                                <strong>Dataset and Class Imbalance</strong><br>
                                <ul>
                                    <li>The dataset shows heavy class imbalance across all tasks and modalities. This imbalance persists even when considering the intersection of multiple modalities, leading to a significant portion of the data being unused.</li>
                                    <li>The dataset was split into training, validation, and test sets with an approximately 80%-10%-10% split, maintaining the proportions of each modality combination and balancing the classes.</li>
                                </ul><br>
                                
                                <strong>Experimentation Setup</strong><br>
                                <ul>
                                    <li><strong>Data Pipeline:</strong> A heavily modified HADM pipeline handles data loading and interlinked modality fusion. PyTorch Lightning with Distributed Data Parallel (DDP) is used for training, with configurations managed via YAML files.</li>
                                    <li><strong>Metrics:</strong> Evaluation metrics include binary classification metrics monitored across train, validation, and test sets. Performance is also assessed at the modality and modality combination levels.</li>
                                    <li><strong>Model Structure:</strong> The model pipeline includes a modality encoder, a time series encoder, and a binary classification head. Class imbalance is addressed through oversampling and undersampling strategies, depending on the skewness of the class distribution.</li>
                                </ul><br>
                                
                                <strong>Observations from Literature</strong><br>
                                <ul>
                                    <li>Cross-modal alignment is rarely performed or analyzed extensively in current research, and ECG data is often excluded or underutilized in multimodal tasks. Reported F1 scores are typically below 0.7, highlighting the challenge in improving model performance.</li>
                                </ul><br>
                                
                                <strong>Experimentation Results</strong><br>
                                <ul>
                                    <li><strong>Validation Embeddings:</strong> Representations of different training strategies and modality combinations are visualized to observe variation in embeddings.</li>
                                    <li><strong>Task Metrics:</strong> Metrics for LOS_3, LOS_7, and mortality tasks are carefully monitored.</li>
                                </ul><br>
                                
                      
                                
                                <div class="publ_">
                                    <ul style="list-style-type: none; padding: 0;">
                                        <li style="margin-bottom: 10px;">
                                            <a href="https://docs.google.com/presentation/d/1CU_Gzy2BEQ5bYzLrQQq5W4BrMf7QgzsWftuQOCUiqvE/edit?usp=sharing" target="_blank" class="button google-slides">
                                                <i class="fa fa-google icon"></i>
                                                <b>Slides</b>
                                            </a>
                                        </li>
                                         <li style="margin-bottom: 10px;">
                                             <a href="https://oceanic-antler-eb0.notion.site/1c6ae01d802f44d4aa1c4bafe6f3d4a2?v=0198cff002b34f94ac6bce016294b0ad"  target="_blank" class="button notion">
                                                 <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                                 <b>Notion</b>
                                             </a>
                                         </li>
                                         <li>
                                             <a href="https://oceanic-antler-eb0.notion.site/SOTA-models-963b2664d0894f6bb0f09d6a12242704?pvs=4" target="_blank" class="button notion">
                                                 <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                                 <b>Notion (SOTA)</b>
                                             </a>
                                         </li>
                                            
                                         <li style="margin-bottom: 10px;">
                                            <a href="https://drive.google.com/file/d/17V-IovP9YHo6Lb0jlTgjNYpH9m7nXoAm/view?usp=sharing" target="_blank" class="button google-slides">
                                                <i class="fa fa-google icon"></i>
                                                <b>Report</b>
                                            </a>
                                        </li>
                                         
                                    </div>

                            </p>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://api.wandb.ai/links/tirthajitb-indian-institute-of-science/s7ncw5hy" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="nus" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Surgical Motion Planning</h2>
                            <br>
                            <p>
                                <img src="img/projects/nus/surgicalcommand2motion-Page-1.drawio.png" class="img-responsive img-centered" alt="">
                            
                                <p><strong>Project Summary:</strong></p>

                                <p>The project focuses on the development of an embodied robotic agent designed to execute high-level natural language surgical instructions. This system leverages a combination of Reinforcement Learning (RL), Imitation Learning, and a pre-trained Large Language Model (LLM) to function as a "planner." The agent can perform short-horizon skills across multiple surgical scenarios using a library of pre-trained policies. These policies are selected based on the scenario, with skills that are common across scenarios mapped under common primitives with textual labels.</p>
                                
                                <p><strong>Methodology:</strong></p>
                                
                                <p><strong>1. Planning and Filtering:</strong> The LLM is responsible for planning by determining the sequence of actions required to execute a given surgical instruction. A filtering process similar to SayCAN is used, which ranks skills based on both logical reasoning and geometric feasibility. The LLM generates possible action permutations, which are then filtered to a manageable number using a Chain-of-Thought (CoT) process, reducing the likelihood of errors and improving interpretability.</p>
                                
                                <p><strong>2. Action Scoring:</strong> Each action is scored based on two criteria: the LLM's planning score and the RL policy’s affordance score, which reflects the geometric feasibility of the action. The best action is selected based on these combined scores, with a fallback to the highest LLM score if all actions are deemed infeasible.</p>
                                
                                <p><strong>3. Execution:</strong> Once an action is selected, a human-in-the-loop is given the option to approve or modify the action before execution. The agent updates the scene and action status after each step, which informs subsequent actions.</p>
                                
                                <p><strong>4. Experimental Setups:</strong> The system is tested in two main environments: Surrol and Lapgym, each with specific tasks like retracting tissue, reaching a tumor, and picking and placing gauze. The experiments explore various strategies, including closed-loop, open-loop, and inner monologue setups, with scene and action updates. The DINO model is utilized for detecting objects in the surgical environment, such as gauze and blood, by generating bounding boxes around these targets based on textual inputs provided by the LLM. This integration allows for accurate, real-time detection of key elements, which is critical for the success of surgical tasks.</p>
                                
                                <p><strong>Key Features:</strong></p>
                                
                                <p><strong>Human-in-the-loop Feedback:</strong> Ensures control over the actions performed, allowing for adjustments based on the requirements.</p>
                                
                                <p><strong>Chain-of-Thought (CoT):</strong> Improves planning efficiency and makes the process more interpretable for surgical tasks.</p>
                                
                                <p><strong>Scene Description Generation:</strong> Simplifies the scene context for the LLM, making the planning process more effective and relatable to real-world applications.</p>
                                
                                <p>The project demonstrates the feasibility of integrating LLMs with RL, Imitation Learning, and DINO for complex surgical tasks, providing a robust foundation for further development in autonomous surgical systems.</p>
                                

                            
                            </p>
                            <br>
                            <h3> Demo </h3>
                            <img src="img/projects/nus/15-ezgif.com-video-to-gif-converter.gif" class="img-responsive img-centered" alt="">
                            <img src="img/projects/nus/ezgif-2-584f48650a.gif" class="img-responsive img-centered" alt="">
                            

                            <br>
                         
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="cmu" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SaSi: A Self-augmented and Self-interpreted for Few-shot Cryo-ET Particle Detection</h2>
                            <br>
                            <p>
                                <img src="img/projects/cmu.png" class="img-responsive img-centered" alt="">
                            
                                <p><strong>Description:</strong></p>
                                <p>This project tackles the critical task of particle localization and classification in cryo-electron tomography (cryo-ET), focusing on real-world challenges such as limited and weakly annotated data. The study is conducted on both private datasets and the SHREC 2021 simulated dataset, aiming to establish robust baselines and explore advanced techniques to improve model performance. The goal is to enhance the accuracy and robustness of particle detection and classification, making it more effective for structural biology applications.</p>
                                
                                <p><strong>Methodology:</strong></p>
                                
                                <p><strong>1. Supervised Learning:</strong> The approach begins by generating <strong>pseudo-strong labels</strong> from weak labels, converting them into segmentation masks by drawing spheres of minimum radius around the particle centroids. These masks serve as ground truth for training. The tomograms are divided into smaller subtomograms, which are fed into the model to generate voxel-level probabilities for each particle type. Key techniques include:</p>
                                <ul>
                                    <li><strong>Balanced Sampling and Window Method:</strong> Ensuring that training data is evenly sampled and that particle centroids are accurately localized within smaller windows.</li>
                                    <li><strong>AugMix Modification:</strong> A tailored version of AugMix is applied to improve data augmentation, focusing on enhancing the diversity and robustness of the training samples.</li>
                                </ul>
                                
                                <p><strong>2. Self-Supervised Learning:</strong> To further improve the model's performance, <strong>SimCLR</strong> is employed. This technique leverages contrastive learning to learn representations that are invariant to data augmentation, improving the model’s ability to generalize from few-shot data.</p>
                                
                                <p><strong>3. Post-Processing:</strong> After initial predictions, advanced post-processing techniques are applied to refine the results:</p>
                                <ul>
                                    <li><strong>Connected Components Analysis:</strong> Used to identify and isolate individual particles from the segmentation masks, improving the accuracy of particle localization.</li>
                                    <li><strong>MeanShift Clustering:</strong> Applied to group detected particles based on their spatial coordinates, helping to better classify and localize particles within the tomogram.</li>
                                </ul>
                       
                            </p>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://wandb.ai/frozenwolf/semi-self%20supervised%20fewshot/reports/Enhancing-Few-Shot-Detection-with-Weak-Labels-A-Self-Supervised-Learning-and-Augmix-Approach-in-Cryo-ET--Vmlldzo5MTE3MDk4" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="SAM-PM" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SAM-PM: Enhancing Video Camouflaged Object Detection</h2>
                            <br>
                            <p>
                                <img src="img/assests/cvpr.png" class="img-responsive img-centered" alt="">
                            
                                <p>
                                    <strong>Methodology</strong><br>
                                    The proposed <strong>SAM-PM</strong> framework adapts the Segment Anything Model (SAM) for the <strong>Video Camouflaged Object Detection (VCOD)</strong> task. SAM-PM consists of two main components: the <strong>Temporal Fusion Mask Module (TFMM)</strong> and the <strong>Memory Prior Affinity Module (MPAM)</strong>.
                                    <ol>
                                    <li><strong>Temporal Fusion Mask Module (TFMM)</strong>: This module enhances temporal information by integrating mask embeddings from multiple frames. It uses a <strong>spatio-temporal cross-attention</strong> mechanism to create temporally enriched mask embeddings.
                                    </li>
                                    <li><strong>Memory Prior Affinity Module (MPAM)</strong>: MPAM utilizes the temporally infused mask embeddings from TFMM along with image embeddings from the current and previous frames. It applies <strong>affinity</strong> to strengthen temporal consistency in mask predictions.
                                </li>
                                    SAM-PM operates in a <strong>semi-supervised</strong> manner, where only the first frame's ground truth mask is used for training. The framework keeps SAM's weights frozen and trains only the SAM-PM components, ensuring <strong>parameter efficiency</strong> with less than 1 million parameters.
                                </ol>
                                    During training and inference, SAM-PM updates its memory with new frames and their predicted masks while discarding outdated data to maintain temporal coherence.
                                    </p>
                                    
                            </p>
                            <br>
                            <h3> WanDB Report </h3>
                            <br>
                            <iframe src="https://api.wandb.ai/links/spider-r-d/7izxfp8j" style="border:none;height:1024px;width:100%"></iframe>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SAM-PM')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                             
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="optical" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Test-time adaptation for Optical Flow </h2>
                            <br>
                            <p>
                                <img src="img/projects/opticalflow.png" class="img-responsive img-centered" alt="">
                                <img src="img/projects/opticalflowex.png" class="img-responsive img-centered" alt="">
                            
                                <p>
                                    We proposed a test-time adaptation method for optical flow estimation using RAFT-like optical flow models to assesses and improve EPE scores on the Sintel and KITTI 2015 datasets. The implementation is built with Lightning Fabric, allowing for efficient management of training processes. Key features include advanced offloading, checkpointing, and multi-node Distributed Data Parallel (DDP) strategies to optimize batch sizes and assess their impact on model performance. This approach is designed to improve the precision of optical flow predictions and adapt to various real-world conditions.
                                    
                                    <br>
                                    <p><strong>Tech Stack</strong><br>
                                    <ul>
                                        <li><strong>Framework:</strong> Torch Scale</li>
                                    <li><strong>Framework:</strong> PyTorch Lightning Fabric</li>
                               
                                </p>  
                            </p>
                            <br>
                            <h3> WanDB Report </h3>
                            Currently in progress and kept private.
                            
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="nllp" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Natural Legal Language Processing</h2>
                            <br>
                            <p>
                                <img src="img/assests/emnlp.png" class="img-responsive img-centered" alt="">
                                
                                <p>
                                    Abstract
A key component of the Natural Language Processing (NLP) pipeline is Sentence Boundary Detection (SBD). Erroneous SBD could affect other processing steps and reduce performance. A few criteria based on punctuation and capitalization are necessary to identify sentence borders in well-defined corpora. However, due to several grammatical ambiguities, the complex structure of legal data poses difficulties for SBD. In this paper, we have trained a neural network framework for identifying the end of the sentence in legal text. We used several state-of-the-art deep learning models, analyzed their performance, and identified that Convolutional Neural Network(CNN) outperformed other deep learning frameworks. We compared the results with rule-based, statistical, and transformer-based frameworks. The best neural network model outscored the popular rule-based framework with an improvement of 8% in the F1 score. Although domain-specific statistical models have slightly improved performance, the trained CNN is 80 times faster in run-time and doesn’t require much feature engineering. Furthermore, after extensive pretraining, the transformer models fall short in overall performance compared to the best deep learning model.
                                </p>  
                            </p>
                            <br>
                         
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/NLLP-ML/SBD')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>



    
    <div class="portfolio-modal modal fade" id="OCR" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Intelligent OCR </h2>
                            <img src="img/projects/ocr.gif" class="img-responsive img-centered" alt="">
                            <img src="img/projects/ocr.png" class="img-responsive img-centered" alt="">
                             
                            <p>
                                This project involves the development of an advanced Optical Character Recognition (OCR) software, leveraging a combination of cutting-edge neural network models: CRAFT, Faster R-CNN, Tesseract, and a Siamese network model. The software is hosted on Azure Cloud, utilizing a virtual machine (VM) to run the server and process documents. The entire system is designed to convert scanned documents into editable text, extract bounding boxes for words and sentences, classify sentences into categories, and provide a comprehensive annotation interface for user modifications.
                                </p>
                                <p>
                                <b>Key Features:</b>
                                <ul>
                                <li><b>Model Integration:</b> The OCR software integrates multiple state-of-the-art models trained using PyTorch on the FUND dataset. This setup ensures high accuracy and reliability in text extraction and classification.</li>
                                <li><b>Document Processing:</b> Users can upload scanned documents in various formats (.png, .jpg, .jpeg, and .pdf) to the frontend website. For PDFs, only the first page is processed. The software extracts editable text, provides bounding boxes for each word and sentence, and classifies sentences into categories such as 'other', 'question', 'answer', and 'header'.</li>
                                <li><b>Annotation Interface:</b> The frontend features a user-friendly annotation interface built with annotorious.js. Users can modify model predictions or annotate documents from scratch. Annotations can be saved and downloaded in a simple .txt format.</li>
                                <li><b>Offline Processing:</b> The software supports offline batch processing, allowing users to process multiple images at once. Users can choose between output formats like MTX or FUND dataset formats.</li>
                                <li><b>Server Details:</b> The models run on an Azure VM with Linux (Ubuntu 18.04), specifically a Standard B2s instance (2 vCPUs, 4 GiB memory). Due to server connection constraints, there might be occasional interruptions in access, but comprehensive setup instructions are provided for replicating the environment.</li>
                                <li><b>Training and Metrics:</b> Model training is conducted using PyTorch, with detailed explanations of training steps and performance metrics. All trained models and predictions on public test datasets are available for download.</li>
                                </ul>
                                </p>
                                
                            <br>
                            <img src="img/projects/ocr2.png" class="img-responsive img-centered" alt="">
                            <img src="img/projects/ocr_demo.png" class="img-responsive img-centered" alt="">
                               
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/OCR')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                        <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://youtu.be/8Cci5rw-KyY')"><i
                                            class="fa fa-fw fa-youtube"></i> Demo</button>
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="i-Pravesh" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>i-Pravesh </h2>
   
                        <img src="img/projects/sih.png" class="img-responsive img-centered" alt="">
                        <img src="img/projects/sih_.png" class="img-responsive img-centered" alt="">
                            <p>
                                <p>I-Pravesh is a sophisticated Smart Attendance Android application designed to simplify and enhance the attendance recording process. It uses advanced face detection and recognition technologies, specifically MobileFaceNet combined with TensorFlow Lite, for accurate biometric authentication. The app not only performs fast and reliable face recognition but also ensures that the user is within a designated location through geofencing. It maintains face embeddings and encrypts credentials for secure data handling. Additionally, I-Pravesh features a comprehensive dashboard for both users and administrators, providing a user-friendly interface for managing and monitoring attendance records efficiently.</p>
                            <br>
                        
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SIH')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>

                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="summarise" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>SummarizeIQ: An Integrated Summarization and Content Analysis Engine </h2>
   
                        <img src="img/projects/summarise.png" class="img-responsive img-centered" alt="">
                              
                            <p>
                                <p>
                                    SummarizeIQ combines T5 summarization with Torch Serve, incorporating advanced extractive methods like LexRank and TextRank for enhanced text processing. The system features keyword generation and employs query-based content filtering using cosine similarity to deliver precise and relevant summaries. It also boasts a user-friendly website interface, allowing seamless interaction with the summarization tools. Additionally, SummarizeIQ supports multi-threaded web scraping, enabling efficient and concurrent data extraction from various sources for comprehensive article analysis and summarization.
                                </p>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/SummarizeIQ')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>

                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="realestatevr" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Real Estate VR </h2>
   
                        <img src="img/projects/vr.gif" class="img-responsive img-centered" alt="">
                       
                            <p>
                                <p>
                                    <p><strong>Problem Statement</strong><br>
                                        Traditional real estate methods rely on physical visits and static images, limiting immersion and convenience. To enhance the real estate experience, we aim to integrate virtual reality (VR) technology, creating a user-friendly, cross-platform VR application. This solution will offer immersive property exploration and real-time updates, modernizing how users view and interact with properties.</p>
                                        
                                        <p><strong>Design</strong><br>
                                        1. <strong>Asset Coordination and Environment Generation:</strong> Utilizing Unity’s assets and spatial mapping, we create a realistic, adaptable virtual environment for immersive exploration.<br>
                                        2. <strong>Advanced User Interaction with XR Device Simulator:</strong> An intuitive interface with Unity’s XR Device Simulator allows seamless navigation and interaction through a 'digital wand,' enhancing user experience with C# scripting.<br>
                                        3. <strong>Dynamic Building Placement with Spatial Intelligence:</strong> Real-time building placement is facilitated by Unity’s spatial analysis, offering precise options based on spatial context.<br>
                                        4. <strong>Futuristic Navigation Paradigms:</strong> Advanced navigation includes teleportation and aerial views, allowing instant spatial traversal and comprehensive property inspection.<br>
                                        5. <strong>Precision-Engineered C# Scripting:</strong> C# scripts ensure smooth user controls, dynamic building placement, and effective interaction management.<br>
                                        Our VR platform allows users to add plots, select and place buildings, and view properties with detailed information. It supports flythrough and teleportation, with eagle-eye views for broader city and plot planning.</p>
                                        
                                        <p><strong>Key Features</strong><br>
                                        - Add and visualize new plots<br>
                                        - Place houses with size and price details<br>
                                        - Immersive VR interface<br>
                                        - Flythrough and teleportation travel<br>
                                        - Multi-plot monitoring with eagle-eye views</p>
                                        
                                        <p><strong>Tools Used</strong><br>
                                        - Unity<br>
                                        - C#<br>
                                        <img src="img/projects/vr.png" class="img-responsive img-centered" alt="">
                               
                                </p>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/Real-Estate-VR')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                        <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://youtu.be/galwI6_rIiw')"><i
                                            class="fa fa-fw fa-youtube"></i> Demo</button>
    
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="compiler" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Yet Another Python Compiler</h2>
   
                        <img src="img/projects/compiler.gif" class="img-responsive img-centered" alt="">
                       
                            <p>
                                A Python Compiler, built with Bison and Flex, processes Python code by tokenizing it, parsing it into a syntax tree, and performing semantic checks. 
                                It generates intermediate code and applies optimizations like constant folding and dead code elimination. 
                                Key features include lexical analysis, syntax and semantic error detection, and code optimization.

                                <p><strong>Overview</strong><br>
                                    The project focuses on developing a compiler for a custom programming language, implementing various stages of compilation including lexical analysis, parsing, semantic analysis, intermediate code generation, and code optimization.</p>
                                    
                                    <p><strong>Tech Stack</strong><br>
                                    <strong>Programming Language:</strong> Python<br>
                                    <strong>Tools:</strong> YACC (Yet Another Compiler Compiler), Lex (for lexical analysis)<br>
                                    <strong>Data Structures:</strong> Symbol tables, parse trees, abstract syntax trees (ASTs)</p>
                                    
                                    <p><strong>Features and Contributions</strong></p>
                                    
                                    <p><strong>Lexical Analysis:</strong><br>
                                    Token identification and lexical error detection.<br>
                                    Developed token identification and error detection mechanisms.<br>
                                    Created a symbol table to manage identifiers and constants.</p>
                                    
                                    <p><strong>Parser:</strong><br>
                                    Syntax declaration, indentation and syntactic error detection.<br>
                                    Implemented syntax rules using YACC for expressions and control flow.<br>
                                    Generated parse trees and abstract syntax trees (ASTs).</p>
                                    
                                    <p><strong>Semantic Analysis:</strong><br>
                                    SDD + SDT, Annotated Parse Tree, and Semantic Error detection.<br>
                                    Designed syntax-directed translations to ensure semantic correctness.<br>
                                    Detected semantic errors such as undeclared variables and misuse of reserved identifiers.</p>
                                    
                                    <p><strong>Intermediate Code Generation (ICG):</strong><br>
                                    Generated three-address code and quadruples.<br>
                                    Implemented backpatching for jump statements.</p>
                                    
                                    <p><strong>Code Optimization:</strong><br>
                                    Basic blocks, DAG, CFG, Induction Variable elimination.<br>
                                    Applied optimization techniques including constant folding, copy propagation, dead code elimination, and peephole optimization.<br>
                                    Enhanced code efficiency through these optimizations.</p>
                                    

                                </p>
                            <br>
                            <br>
                            <h3> Report </h3>
                            <br>
                            <iframe src="img/projects/compiler/Group13_106121045_106121099_106121129.pdf" width="100%" height="600px"></iframe>
                            <br>
                            <h4> Lexical Analsysis </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Lexer_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> Parser </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Parser_Final_Report.pdf" width="100%" height="600px"></iframe>
                            <br>
                            </details>
                            <br>
                            <h4> Semantic Analsysis </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Semantic_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> ICG </h4>
                            <details>
                           
                            <iframe src="img/projects/compiler/ICG_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <h4> Optimization </h4>
                            <details>
                            
                            <iframe src="img/projects/compiler/Optimization_Report.pdf" width="100%" height="600px"></iframe>
                            </details>
                            <br>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/YA-Python-Compiler')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="octtree" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Oct tree based 3D OpenGL Renderer</h2>
   
                        <img src="img/projects/octtree.gif" class="img-responsive img-centered" alt="">
                       
                            <p>
                                The Octree-Based 3D OpenGL Renderer, created with PyOpenGL, efficiently renders 3D scenes by using an octree data structure to manage and optimize rendering performance.
        
                                </p>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/3D-Model-Renderer-Oct-Tree')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="sart" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Sartorius Cell Instance Segmentation</h2>
   
                        <img src="img/projects/sart.gif" class="img-responsive img-centered" alt="">
                       
                        <p>
                            Trained R-CNN, U-Net and Detectron2 using Pytorch to detect and delineate
                                distinct objects of interest in biological images depicting neuronal cell types commonly used in studying neurological disorders
                        </p>
                        <p><b>Key findings include:</b></p>
                        <ol>
                            <li><b>Model Comparison:</b> Detectron2 outperformed U-Net and Mask R-CNN, showcasing superior accuracy and robustness in segmenting complex neuronal cell structures.</li>
                            <li><b>U-Net:</b> Known for capturing fine details, U-Net was used with a ResNet34 backbone and a combination of focal and dice losses to handle class imbalance. Post-processing improved segmentation outcomes, though some blurriness persisted.</li>
                            <li><b>Mask R-CNN:</b> Leveraging ResNet50, this model generated object bounding boxes and masks, with loss functions tailored to boundary, mask, and classification losses. Overlapping segmentations were mitigated through pixel-wise intersection operations.</li>
                            <li><b>Detectron2:</b> As a flexible, high-performance framework, Detectron2 provided modular implementation and incorporated innovative techniques like deformable convolutions, leading to its superior performance in this study.</li>
                        </ol>
                        
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    <button type="button" class="btn btn-default" data-dismiss="modal" onclick="window.open('https://github.com/FrozenWolf-Cyber/Sartorius-Cell-Instance-Segmentation')"><i
                                        class="fa fa-fw fa-github"></i> Github</button>
                                       
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="portfolio-modal modal fade" id="others" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <div class="modal-body">
                            <h2>Other Projects</h2>
   
                        <img src="img/projects/others.gif" class="img-responsive img-centered" alt="">
                       
                        <p>
                            Projects include: <br>

                            <a href="https://github.com/FrozenWolf-Cyber/Brain-Tumor-Classification"> Tumor classification </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/COVID19-Pneumonia-Chest-X-Ray-Detection"> Pneumonia (COVID-19) chest X-ray classification </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Neural-Style-Transfer"> Neural style transfer </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Tensorflow-Lite-Crack-Detection"> Crack detection with TF Lite </a> <br>
                            <a href="https://github.com/FrozenWolf-Cyber/Face-Recognition"> Face detection using a Siamese Neural Network model to develop a one-shot neural network with FaceNet and MTCNN as the backbone </a> 
                            <a href="https://github.com/FrozenWolf-Cyber/Keres"> Unity based C# game called "The Keres" </a> 
                        </p>
                            <br>
                       
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i
                                    class="fa fa-times"></i> Close</button>
                                    
                                        
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>



    <!-- About Section -->
    <section class="success" id="presentation">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Presentation & Surveys</h2>
                    <hr class="light">
                </div>
            </div>
            


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/iisc_mimic.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Multi-Modal Representation Learning for clinical data and Test-time adaptation for Optical Flow</strong></div>
                    <div class="pubv"><b>2022</b> </div>
                    These slides and report covers the results from MIMIC-IV multi-modal fusion and optical flow test-time adaptation for RAFT. Notion contains survey on multimodality and contrastive learning methods in MIMIC (III, IV) and similar dataset along with SOTA models available in each modalities such as CXR, EHR, ECG and Notes.
           

                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1CU_Gzy2BEQ5bYzLrQQq5W4BrMf7QgzsWftuQOCUiqvE/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                             <li style="margin-bottom: 10px;">
                                 <a href="https://oceanic-antler-eb0.notion.site/1c6ae01d802f44d4aa1c4bafe6f3d4a2?v=0198cff002b34f94ac6bce016294b0ad"  target="_blank" class="button notion">
                                     <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                     <b>Notion</b>
                                 </a>
                             </li>
                             <li>
                                 <a href="https://oceanic-antler-eb0.notion.site/SOTA-models-963b2664d0894f6bb0f09d6a12242704?pvs=4" target="_blank" class="button notion">
                                     <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                     <b>Notion (SOTA)</b>
                                 </a>
                             </li>
                                
                             <li style="margin-bottom: 10px;">
                                <a href="https://drive.google.com/file/d/17V-IovP9YHo6Lb0jlTgjNYpH9m7nXoAm/view?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Report</b>
                                </a>
                            </li>
                             
                         
                        </ul>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cl.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Continual Learning Survey</strong></div>
                    <div class="pubv"><b>2022</b> </div>
                    These slides and notion database partially covers the classical and SOTA methods in continual learning and its drawbacks till 2022.

                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1l_O9qThYFHkLAaoYyAB4Et7UxxGGsvF3D8gcSbEH6Ls/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://oceanic-antler-eb0.notion.site/113a771fb39c80f18251c3091f4f192a?v=113a771fb39c815782cb000c67af58ab" target="_blank" class="button notion">
                                    <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                    <b>Notion</b>
                                </a>
                            </li>
                        </ul>
                    </div>
                    


                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/LLM planner.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Embodiement Agent (TAMP with LLM) Mini Survey and proposed strategy</strong></div>
                    <div class="pubv"><b>2023</b> </div>
                    These slides and notion database partially covers the embodied agent and Evaluation metrics used in TAMP along with a proposed strategy consisting of Instruction Planner, Coder for debug analysis, and Skill Handler to optimize actions. It learns from execution feedback, continuously improving performance through reflection and memory management.


                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1ppk0cnIqiQhhx0jARil2j1jLPSAShffTaBfjwhznLb8/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://oceanic-antler-eb0.notion.site/f7e79139232c49c896540f515f38893e?v=c559a2753a7b40a8903a21ffb2a485f1" target="_blank" class="button notion">
                                    <i class="fa fa-sticky-note icon"></i> <!-- Placeholder icon for Notion -->
                                    <b>Notion</b>
                                </a>
                            </li>
                        </ul>
                    </div>

                </div>
            </div>

            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/cmu/comb.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>SaSi: A Self-augmented and Self-Interpreted Deep-Learning Approach for Few-shot Cryo-ET Particle Detection</strong></div>
                    <div class="pubv"><b>2024</b> </div>
                    These slides cover contributions of SaSi
                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1CU_Gzy2BEQ5bYzLrQQq5W4BrMf7QgzsWftuQOCUiqvE/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                           
                        </ul>
                    </div>

                </div>
            </div>


            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <img class="img-responsive" src="img/projects/dl_teach.png" style="width: 300px; height: 140px;" alt="">
                </div>
                <div class="col-sm-8 col-xs-12">
                    <div class="pubt"><strong>Introduction to Deep Learning</strong></div>
                    <div class="pubv"><b>Spider R&D Workshop 2023</b> </div>
                    These slides cover entire deep learning from basic till generative models like GANs and Diffusion Based Models. The slides progress from basic neural networks to advanced models like GANs and Diffusion Based Models along with notebooks and visualizations.
                    <div class="publ_">
                        <ul style="list-style-type: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <a href="https://docs.google.com/presentation/d/1OTKlOIsqToT6K8lM0iTgMkguyzXt8DgZOraRozAy6Os/edit?usp=sharing" target="_blank" class="button google-slides">
                                    <i class="fa fa-google icon"></i>
                                    <b>Slides</b>
                                </a>
                            </li>
                            <li>
                                <a href="https://linktr.ee/dl_workshop?utm_source=linktree_profile_share&ltsid=84e5ac63-9c82-4d22-9425-5954a9ee3b03" target="_blank" class="button notion">
                                    <i class="fa fa-link"></i> <!-- Placeholder icon for Notion -->
                                    <b>LinkTree</b>
                                </a>
                            </li>
                        </ul>
                    </div>

                </div>
            </div>





    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <script src="js/timeline.js"></script>
    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/freelancer.js"></script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-92670333-1', 'auto');
        ga('require', 'ipMeta', {
            serviceProvider: 'dimension1',
            networkDomain: 'dimension2',
            networkType: 'dimension3',
        });
        ga('ipMeta:loadNetworkFields');
        ga('send', 'pageview');
    </script>
    <script async src="https://ipmeta.io/plugin.js"></script>
</body>

</html>
